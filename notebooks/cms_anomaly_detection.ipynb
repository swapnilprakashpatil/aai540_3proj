{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abfe8a62",
   "metadata": {},
   "source": [
    "# CMS Open Payments Anomaly Detection\n",
    "\n",
    "**Project:** AAI-540 Machine Learning Operations - Final Team Project  \n",
    "**Dataset:** CMS Open Payments Program Year 2024 General Payments  \n",
    "**Objective:** End-to-end pipeline for detecting anomalous healthcare payment patterns\n",
    "\n",
    "## Team Members - Group 3\n",
    "1. Swapnil Patil\n",
    "2. Jamshed Nabizada\n",
    "3. Tej Singh Bahadur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611be53",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "### Setup & Configuration\n",
    "- [1. Environment Setup](#1-environment-setup)\n",
    "- [2. AWS Configuration & Datalake Setup](#2-aws-configuration--datalake-setup)\n",
    "\n",
    "### Data Preparation\n",
    "- [3. Data Loading & Exploration](#3-data-loading--exploration)\n",
    "- [4. Data Cleaning & Preprocessing](#4-data-cleaning--preprocessing)\n",
    "\n",
    "### Analysis & Features\n",
    "- [5. Exploratory Data Analysis](#5-exploratory-data-analysis)\n",
    "  - Payment Statistics\n",
    "  - Recipient Type Analysis\n",
    "  - State-Level Analysis\n",
    "  - Temporal Trend Analysis\n",
    "- [6. Feature Engineering](#6-feature-engineering)\n",
    "  - Data Splitting\n",
    "  - Historical Features\n",
    "  - Risk Indicators\n",
    "\n",
    "### Model Training\n",
    "- [7. Isolation Forest Model Training](#7-isolation-forest-model-training)\n",
    "  - Data Preparation\n",
    "  - Model Training\n",
    "  - Anomaly Detection\n",
    "  - Performance Evaluation\n",
    "- [8. Autoencoder Model Training](#8-autoencoder-model-training)\n",
    "  - Architecture Design\n",
    "  - Training with Early Stopping\n",
    "  - Reconstruction Error Analysis\n",
    "  - Visualization\n",
    "- [9. XGBoost Model Training](#9-xgboost-model-training)\n",
    "  - Pseudo-Label Generation\n",
    "  - Model Configuration\n",
    "  - Training & Evaluation\n",
    "  - Feature Importance\n",
    "\n",
    "### Optimization & Deployment\n",
    "- [10. Hyperparameter Tuning](#10-hyperparameter-tuning)\n",
    "  - Grid Search\n",
    "  - Randomized Search\n",
    "  - Model Comparison\n",
    "- [11. Model Deployment to SageMaker](#11-model-deployment-to-sagemaker)\n",
    "  - Model Packaging\n",
    "  - Inference Script\n",
    "  - Endpoint Deployment\n",
    "  - Testing & Validation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c85a03f",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc58467",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8085fd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import json\n",
    "import joblib\n",
    "import tarfile\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "import awswrangler as wr\n",
    "from pyathena import connect\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import xgboost as xgb\n",
    "\n",
    "parent_dir = Path.cwd().parent\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "from utils.visualizations import PaymentVisualizer, ModelVisualizer\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "visualizer = PaymentVisualizer()\n",
    "model_viz = ModelVisualizer()\n",
    "\n",
    "print(\"Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796de5e6",
   "metadata": {},
   "source": [
    "## 2. AWS Configuration & Datalake Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed2debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity().get('Account')\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "dataset_file_name = \"lightdataset.csv\"\n",
    "bucket = f\"cmsopenpaymentsystemslight\"\n",
    "cms_data_prefix = \"cms-open-payments-light\"\n",
    "database_name = \"cms_open_payments_light\"\n",
    "\n",
    "raw_data_prefix = f\"{cms_data_prefix}/raw\"\n",
    "parquet_data_prefix = f\"{cms_data_prefix}/parquet\"\n",
    "\n",
    "s3_raw_path = f\"s3://{bucket}/{raw_data_prefix}\"\n",
    "s3_parquet_path = f\"s3://{bucket}/{parquet_data_prefix}\"\n",
    "s3_athena_staging = f\"s3://{bucket}/athena/staging\"\n",
    "\n",
    "table_name_csv = \"general_payments_csv\"\n",
    "table_name_parquet = \"general_payments_parquet\"\n",
    "\n",
    "print(f\"Region: {region} | Bucket: {bucket} | Database: {database_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44522ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bucket_exists(bucket_name):\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def ensure_bucket_exists(bucket_name, region):\n",
    "    try:\n",
    "        if check_bucket_exists(bucket_name):\n",
    "            print(f\"Bucket exists: {bucket_name}\")\n",
    "            return True\n",
    "        if region == 'us-east-1':\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "        print(f\"Created bucket: {bucket_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error with bucket: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "ensure_bucket_exists(bucket, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33023741",
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_conn = connect(region_name=region, s3_staging_dir=s3_athena_staging)\n",
    "\n",
    "create_db_query = f\"CREATE DATABASE IF NOT EXISTS {database_name}\"\n",
    "pd.read_sql(create_db_query, athena_conn)\n",
    "print(f\"Database created: {database_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13293ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_dir = Path(\"../data\")\n",
    "local_csv_file = local_data_dir / dataset_file_name\n",
    "\n",
    "if not local_csv_file.exists():\n",
    "    print(f\"Data file not found: {local_csv_file}\")\n",
    "else:\n",
    "    print(f\"Data file ready: {local_csv_file.stat().st_size / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43714d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_raw_file_path = f\"{s3_raw_path}/{local_csv_file.name}\"\n",
    "\n",
    "if local_csv_file.exists():\n",
    "    try:\n",
    "        s3_client.upload_file(\n",
    "            str(local_csv_file),\n",
    "            bucket,\n",
    "            f\"{raw_data_prefix}/{local_csv_file.name}\"\n",
    "        )\n",
    "        print(f\"Uploaded to S3: {s3_raw_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Upload error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac90906",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schema = pd.read_csv(local_csv_file, nrows=1000)\n",
    "\n",
    "TEXT_COLUMN_PATTERNS = [\n",
    "    '_name', '_address', '_city', '_state', '_country', '_province',\n",
    "    '_zip', '_postal', '_code', '_ccn', '_npi', '_id', \n",
    "    'first_name', 'last_name', 'middle_name', 'suffix',\n",
    "    'street', 'covered_recipient_type', 'nature_of', 'form_of',\n",
    "    'physician_specialty', 'recipient_primary_', 'business_street',\n",
    "    'teaching_hospital', 'applicable_manufacturer', 'making_payment'\n",
    "]\n",
    "\n",
    "def get_athena_type(col_name, dtype):\n",
    "    col_lower = col_name.lower()\n",
    "    if any(pattern in col_lower for pattern in TEXT_COLUMN_PATTERNS):\n",
    "        return 'STRING'\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return 'BIGINT'\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return 'DOUBLE'\n",
    "    elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "        return 'TIMESTAMP'\n",
    "    else:\n",
    "        return 'STRING'\n",
    "\n",
    "columns_def = []\n",
    "for col in df_schema.columns:\n",
    "    athena_type = get_athena_type(col, df_schema[col].dtype)\n",
    "    columns_def.append(f\"`{col}` {athena_type}\")\n",
    "\n",
    "columns_str = ',\\n    '.join(columns_def)\n",
    "\n",
    "create_table_query = f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {database_name}.{table_name_csv} (\n",
    "    {columns_str}\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\\\n'\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '{s3_raw_path}/'\n",
    "TBLPROPERTIES (\n",
    "    'skip.header.line.count'='1',\n",
    "    'serialization.null.format'=''\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(create_table_query, athena_conn)\n",
    "print(f\"Table created: {table_name_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bce4795",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning up old Parquet data...\")\n",
    "try:\n",
    "    wr.s3.delete_objects(f\"s3://{bucket}/{parquet_data_prefix}\")\n",
    "    cursor = athena_conn.cursor()\n",
    "    cursor.execute(f\"DROP TABLE IF EXISTS {database_name}.{table_name_parquet}\")\n",
    "    print(\"Cleanup complete\")\n",
    "except Exception as e:\n",
    "    print(f\"Cleanup note: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0cbec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_query = f\"SELECT * FROM {database_name}.{table_name_csv} LIMIT 1\"\n",
    "\n",
    "force_string_cols = [\n",
    "    'teaching_hospital_ccn', 'teaching_hospital_id',\n",
    "    'applicable_manufacturer_or_applicable_gpo_making_payment_id',\n",
    "    'covered_recipient_npi', 'covered_recipient_profile_id',\n",
    "    'physician_profile_id', 'recipient_zip_code', 'recipient_postal_code'\n",
    "]\n",
    "\n",
    "columns_df = pd.read_sql(columns_query, athena_conn)\n",
    "columns = columns_df.columns.tolist()\n",
    "\n",
    "if 'program_year' in columns:\n",
    "    columns.remove('program_year')\n",
    "\n",
    "formatted_columns = []\n",
    "for col in columns:\n",
    "    if col in force_string_cols or any(pattern in col.lower() for pattern in \n",
    "        ['_name', '_address', '_city', '_id', '_npi', '_ccn', 'zip', 'postal']):\n",
    "        formatted_columns.append(f'CAST(\"{col}\" AS VARCHAR) AS \"{col}\"')\n",
    "    else:\n",
    "        formatted_columns.append(f'\"{col}\"')\n",
    "\n",
    "columns_str = ',\\n '.join(formatted_columns)\n",
    "\n",
    "create_parquet_query = f\"\"\"\n",
    "CREATE TABLE {database_name}.{table_name_parquet}\n",
    "WITH (\n",
    "    format = 'PARQUET',\n",
    "    parquet_compression = 'SNAPPY',\n",
    "    external_location = '{s3_parquet_path}/',\n",
    "    partitioned_by = ARRAY['program_year']\n",
    ")\n",
    "AS\n",
    "SELECT \n",
    "    {columns_str},\n",
    "    '2024' as program_year\n",
    "FROM {database_name}.{table_name_csv}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Converting to Parquet...\")\n",
    "pd.read_sql(create_parquet_query, athena_conn)\n",
    "print(f\"Parquet table created: {table_name_parquet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaafe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_sql(\n",
    "    f\"SELECT COUNT(*) as row_count FROM {database_name}.{table_name_parquet}\",\n",
    "    athena_conn\n",
    ")\n",
    "print(f\"Data verified: {result['row_count'][0]:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dc9ff9",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c9c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1_000_000\n",
    "\n",
    "sample_query = f\"\"\"\n",
    "SELECT *\n",
    "FROM {database_name}.{table_name_parquet}\n",
    "LIMIT {sample_size}\n",
    "\"\"\"\n",
    "\n",
    "df = wr.athena.read_sql_query(\n",
    "    sql=sample_query,\n",
    "    database=database_name,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "print(f\"Data loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998527d2",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b395b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_PAYMENT_FEATURES = [\n",
    "    'total_amount_of_payment_usdollars',\n",
    "    'number_of_payments_included_in_total_amount',\n",
    "    'date_of_payment'\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'covered_recipient_type',\n",
    "    'nature_of_payment_or_transfer_of_value',\n",
    "    'form_of_payment_or_transfer_of_value',\n",
    "    'physician_specialty',\n",
    "    'recipient_state'\n",
    "]\n",
    "\n",
    "IDENTIFIER_FEATURES = [\n",
    "    'covered_recipient_profile_id',\n",
    "    'covered_recipient_npi',\n",
    "    'applicable_manufacturer_or_applicable_gpo_making_payment_name'\n",
    "]\n",
    "\n",
    "RISK_INDICATOR_FEATURES = [\n",
    "    'physician_ownership_indicator',\n",
    "    'third_party_payment_recipient_indicator',\n",
    "    'product_indicator'\n",
    "]\n",
    "\n",
    "all_selected_features = (CORE_PAYMENT_FEATURES + CATEGORICAL_FEATURES + \n",
    "                         IDENTIFIER_FEATURES + RISK_INDICATOR_FEATURES)\n",
    "\n",
    "available_features = [f for f in all_selected_features if f in df.columns]\n",
    "df_selected = df[available_features].copy()\n",
    "\n",
    "print(f\"Features selected: {df_selected.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a1e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'total_amount_of_payment_usdollars' in df_selected.columns:\n",
    "    df_selected['total_amount_of_payment_usdollars'] = pd.to_numeric(\n",
    "        df_selected['total_amount_of_payment_usdollars'], errors='coerce'\n",
    "    ).abs()\n",
    "\n",
    "if 'date_of_payment' in df_selected.columns:\n",
    "    df_selected['date_of_payment'] = pd.to_datetime(df_selected['date_of_payment'], errors='coerce')\n",
    "\n",
    "if 'number_of_payments_included_in_total_amount' in df_selected.columns:\n",
    "    df_selected['number_of_payments_included_in_total_amount'] = pd.to_numeric(\n",
    "        df_selected['number_of_payments_included_in_total_amount'], errors='coerce'\n",
    "    ).fillna(1).astype('int64')\n",
    "\n",
    "indicator_mapping = {'Yes': 1, 'Y': 1, 'No': 0, 'N': 0, 'Unknown': 0}\n",
    "for col in RISK_INDICATOR_FEATURES:\n",
    "    if col in df_selected.columns and df_selected[col].dtype == 'object':\n",
    "        df_selected[col] = df_selected[col].map(indicator_mapping).fillna(0).astype('int64')\n",
    "\n",
    "if 'recipient_state' in df_selected.columns:\n",
    "    df_selected['recipient_state'] = df_selected['recipient_state'].str.upper().str.strip()\n",
    "\n",
    "print(\"Data types converted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc85c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'total_amount_of_payment_usdollars' in df_selected.columns:\n",
    "    payment_median = df_selected['total_amount_of_payment_usdollars'].median()\n",
    "    df_selected['total_amount_of_payment_usdollars'].fillna(payment_median, inplace=True)\n",
    "\n",
    "if 'date_of_payment' in df_selected.columns:\n",
    "    df_selected['date_of_payment'].fillna(method='ffill', inplace=True)\n",
    "    df_selected['date_of_payment'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "for col in df_selected.select_dtypes(include=['object']).columns:\n",
    "    if df_selected[col].isnull().sum() > 0:\n",
    "        df_selected[col].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "for col in df_selected.select_dtypes(include=[np.number]).columns:\n",
    "    if df_selected[col].isnull().sum() > 0:\n",
    "        df_selected[col].fillna(df_selected[col].median(), inplace=True)\n",
    "\n",
    "print(\"Missing values handled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effc4b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_before = len(df_selected)\n",
    "\n",
    "df_selected = df_selected.drop_duplicates()\n",
    "\n",
    "if 'total_amount_of_payment_usdollars' in df_selected.columns:\n",
    "    df_selected = df_selected[df_selected['total_amount_of_payment_usdollars'] > 0]\n",
    "\n",
    "if 'date_of_payment' in df_selected.columns:\n",
    "    df_selected = df_selected[df_selected['date_of_payment'].notnull()]\n",
    "\n",
    "if 'covered_recipient_profile_id' in df_selected.columns:\n",
    "    df_selected = df_selected[df_selected['covered_recipient_profile_id'].notnull()]\n",
    "\n",
    "df_selected = df_selected.reset_index(drop=True)\n",
    "records_after = len(df_selected)\n",
    "\n",
    "print(f\"Records: {records_after:,} | Removed: {records_before - records_after:,} | Retention: {records_after/records_before*100:.2f}%\")\n",
    "\n",
    "df = df_selected.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f63af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_date_col = 'date_of_payment'\n",
    "if payment_date_col in df.columns:\n",
    "    df['payment_year'] = df[payment_date_col].dt.year\n",
    "    df['payment_month'] = df[payment_date_col].dt.month\n",
    "    df['payment_quarter'] = df[payment_date_col].dt.quarter\n",
    "    df['payment_dayofweek'] = df[payment_date_col].dt.dayofweek\n",
    "    df['is_weekend'] = (df[payment_date_col].dt.dayofweek >= 5).astype('int64')\n",
    "\n",
    "print(\"Temporal features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a869397",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb63642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_col = 'total_amount_of_payment_usdollars'\n",
    "\n",
    "if payment_col in df.columns:\n",
    "    payment_stats = df[payment_col].describe()\n",
    "    \n",
    "    print(f\"Payment Statistics:\")\n",
    "    print(f\"Count: {payment_stats['count']:,.0f} | Mean: ${payment_stats['mean']:,.2f} | Median: ${payment_stats['50%']:,.2f}\")\n",
    "    print(f\"Min: ${payment_stats['min']:,.2f} | Max: ${payment_stats['max']:,.2f}\")\n",
    "    print(f\"Skewness: {df[payment_col].skew():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bdc654",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.plot_payment_distribution_detailed(df, payment_col=payment_col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2b944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipient_type_col = 'covered_recipient_type'\n",
    "\n",
    "if recipient_type_col in df.columns and payment_col in df.columns:\n",
    "    type_stats = df.groupby(recipient_type_col)[payment_col].agg([\n",
    "        'count', 'sum', 'mean', 'median'\n",
    "    ]).round(2)\n",
    "    type_stats.columns = ['Count', 'Total ($)', 'Mean ($)', 'Median ($)']\n",
    "    type_stats = type_stats.sort_values('Total ($)', ascending=False)\n",
    "    display(type_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def25107",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_col = 'recipient_state'\n",
    "\n",
    "if state_col in df.columns and payment_col in df.columns:\n",
    "    state_stats = df.groupby(state_col)[payment_col].agg([\n",
    "        'count', 'sum', 'mean'\n",
    "    ]).round(2)\n",
    "    state_stats.columns = ['Count', 'Total ($)', 'Mean ($)']\n",
    "    state_stats = state_stats.sort_values('Total ($)', ascending=False).head(10)\n",
    "    display(state_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa58790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if recipient_type_col in df.columns and payment_col in df.columns:\n",
    "    visualizer.plot_bivariate_comparison(df, group_col=recipient_type_col, amount_col=payment_col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24f0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "nature_col = 'nature_of_payment_or_transfer_of_value'\n",
    "\n",
    "if nature_col in df.columns and payment_col in df.columns:\n",
    "    visualizer.plot_payment_nature_by_total(\n",
    "        df,\n",
    "        nature_col=nature_col,\n",
    "        amount_col=payment_col,\n",
    "        top_n=15\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d052b5e",
   "metadata": {},
   "source": [
    "### Temporal Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb7c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'payment_month' in df.columns and payment_col in df.columns:\n",
    "    visualizer.plot_monthly_trends(df, payment_col=payment_col, month_col='payment_month')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03483fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'date_of_payment' in df.columns and payment_col in df.columns:\n",
    "    visualizer.plot_temporal_trends(df, date_col='date_of_payment', amount_col=payment_col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac9c67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'payment_quarter' in df.columns and payment_col in df.columns:\n",
    "    quarterly_stats = df.groupby('payment_quarter')[payment_col].agg(['count', 'sum', 'mean', 'median']).round(2)\n",
    "    quarterly_stats.columns = ['Count', 'Total ($)', 'Mean ($)', 'Median ($)']\n",
    "    quarterly_stats.index.name = 'Quarter'\n",
    "    \n",
    "    print(\"Quarterly Payment Statistics:\")\n",
    "    display(quarterly_stats)\n",
    "    \n",
    "    visualizer.plot_quarterly_trends(df, quarter_col='payment_quarter', amount_col=payment_col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3395dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features_eda = [payment_col, 'number_of_payments_included_in_total_amount']\n",
    "if 'payment_month' in df.columns:\n",
    "    numeric_features_eda.append('payment_month')\n",
    "if 'payment_quarter' in df.columns:\n",
    "    numeric_features_eda.append('payment_quarter')\n",
    "\n",
    "available_numeric = [col for col in numeric_features_eda if col in df.columns]\n",
    "\n",
    "if len(available_numeric) >= 2:\n",
    "    visualizer.plot_correlation_heatmap(df, available_numeric)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b221e33",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e81bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "perms = np.random.rand(len(df))\n",
    "\n",
    "df['dataset_usage'] = pd.cut(\n",
    "    perms, \n",
    "    bins=[0, 0.4, 0.5, 0.6, 1.0], \n",
    "    labels=['train', 'test', 'validation', 'production']\n",
    ")\n",
    "\n",
    "split_summary = df['dataset_usage'].value_counts().sort_index()\n",
    "print(\"Data Split (40/10/10/40):\")\n",
    "for label in split_summary.index:\n",
    "    pct = split_summary[label] / len(df) * 100\n",
    "    print(f\"  {label.capitalize()}: {split_summary[label]:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813a302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id = 'covered_recipient_profile_id' \n",
    "payment_col = 'total_amount_of_payment_usdollars'\n",
    "\n",
    "df_train = df[df['dataset_usage'] == 'train']\n",
    "global_median = df_train[payment_col].median()\n",
    "\n",
    "recipient_features = df_train.groupby(target_id).agg({\n",
    "    payment_col: ['count', 'sum', 'mean', 'std', 'max']\n",
    "}).reset_index()\n",
    "\n",
    "recipient_features.columns = [\n",
    "    target_id, 'hist_pay_count', 'hist_pay_total', 'hist_pay_avg', 'hist_pay_std', 'hist_pay_max'\n",
    "]\n",
    "\n",
    "df = df.merge(recipient_features, on=target_id, how='left')\n",
    "\n",
    "df['amt_to_avg_ratio'] = df[payment_col] / df['hist_pay_avg']\n",
    "df['amt_to_max_ratio'] = df[payment_col] / df['hist_pay_max']\n",
    "df['is_new_recipient'] = df['hist_pay_avg'].isnull().astype(int)\n",
    "\n",
    "df['hist_pay_avg'] = df['hist_pay_avg'].fillna(global_median)\n",
    "df['amt_to_avg_ratio'] = df['amt_to_avg_ratio'].fillna(df[payment_col] / global_median)\n",
    "df[['hist_pay_count', 'hist_pay_total', 'hist_pay_std', 'hist_pay_max', 'amt_to_max_ratio']] = \\\n",
    "    df[['hist_pay_count', 'hist_pay_total', 'hist_pay_std', 'hist_pay_max', 'amt_to_max_ratio']].fillna(0)\n",
    "\n",
    "print(\"Baseline features created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf9a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_categories = ['Gift', 'Entertainment', 'Travel and Lodging']\n",
    "nature_col = 'nature_of_payment_or_transfer_of_value'\n",
    "\n",
    "if nature_col in df.columns:\n",
    "    df['is_high_risk_nature'] = df[nature_col].isin(risk_categories).astype(int)\n",
    "\n",
    "print(\"Risk features created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8166246",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols_df = df.select_dtypes(include=[np.number]).columns\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "for col in numeric_cols_df:\n",
    "    if col in df.columns:\n",
    "        q999 = df[col].quantile(0.999)\n",
    "        q001 = df[col].quantile(0.001)\n",
    "        df[col] = df[col].clip(lower=q001, upper=q999)\n",
    "\n",
    "print(\"Outliers capped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7244fea0",
   "metadata": {},
   "source": [
    "## 7. Isolation Forest Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01ccff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_payments = df.copy()\n",
    "\n",
    "numeric_cols = df_payments.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "cols_to_exclude = [\n",
    "    'EventTime', 'covered_recipient_profile_id', 'index',\n",
    "    'teaching_hospital_id', 'covered_recipient_npi',\n",
    "    'recipient_zip_code', 'recipient_province', 'recipient_postal_code'\n",
    "]\n",
    "\n",
    "numeric_features = [col for col in numeric_cols \n",
    "                   if col not in cols_to_exclude \n",
    "                   and not any(x in col.lower() for x in ['_id', '_code', '_province', '_postal'])]\n",
    "\n",
    "X = df_payments[numeric_features].copy().astype(float)\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "missing_pct = (X.isnull().sum() / len(X)) * 100\n",
    "cols_to_keep = missing_pct[missing_pct <= 50].index.tolist()\n",
    "X = X[cols_to_keep]\n",
    "\n",
    "for col in X.columns:\n",
    "    q1, q3 = X[col].quantile(0.25), X[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    X[col] = X[col].clip(lower=q1 - 3*iqr, upper=q3 + 3*iqr)\n",
    "\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "print(f\"Features prepared: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9045d678",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "X_train, X_test = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d5f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_params = {\n",
    "    'n_estimators': 200,\n",
    "    'contamination': 0.05,\n",
    "    'max_samples': 'auto',\n",
    "    'max_features': 1.0,\n",
    "    'bootstrap': False,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "isolation_forest = IsolationForest(**baseline_params, n_jobs=-1, verbose=0)\n",
    "\n",
    "print(\"Training Isolation Forest...\")\n",
    "start_time = time.time()\n",
    "isolation_forest.fit(X_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"Training completed in {training_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839da354",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = isolation_forest.predict(X_train)\n",
    "train_scores = isolation_forest.decision_function(X_train)\n",
    "\n",
    "test_predictions = isolation_forest.predict(X_test)\n",
    "test_scores = isolation_forest.decision_function(X_test)\n",
    "\n",
    "train_anomalies = (train_predictions == -1).astype(int)\n",
    "test_anomalies = (test_predictions == -1).astype(int)\n",
    "\n",
    "train_anomaly_count = train_anomalies.sum()\n",
    "test_anomaly_count = test_anomalies.sum()\n",
    "\n",
    "print(f\"Train Anomalies: {train_anomaly_count:,} ({train_anomaly_count/len(X_train)*100:.2f}%)\")\n",
    "print(f\"Test Anomalies: {test_anomaly_count:,} ({test_anomaly_count/len(X_test)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3486b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = isolation_forest.decision_function(X_scaled)\n",
    "all_predictions = isolation_forest.predict(X_scaled)\n",
    "\n",
    "anomaly_results = df_payments.copy()\n",
    "anomaly_results['anomaly_score'] = all_scores\n",
    "anomaly_results['is_anomaly'] = (all_predictions == -1).astype(int)\n",
    "anomaly_results['anomaly_score_percentile'] = pd.Series(all_scores).rank(pct=True) * 100\n",
    "\n",
    "anomalies_df = anomaly_results[anomaly_results['is_anomaly'] == 1].copy()\n",
    "anomalies_df = anomalies_df.sort_values('anomaly_score', ascending=True)\n",
    "normal_df = anomaly_results[anomaly_results['is_anomaly'] == 0]\n",
    "\n",
    "print(f\"Total Anomalies: {len(anomalies_df):,} ({len(anomalies_df)/len(anomaly_results)*100:.2f}%)\")\n",
    "\n",
    "top_anomalies = model_viz.display_top_anomalies(anomalies_df=anomalies_df, score_col='anomaly_score', top_n=10)\n",
    "display(top_anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d3a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = model_viz.plot_anomaly_scores(\n",
    "    train_scores=train_scores,\n",
    "    test_scores=test_scores,\n",
    "    threshold=isolation_forest.offset_,\n",
    "    model_name='Isolation Forest'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c1775",
   "metadata": {},
   "source": [
    "### Anomaly Analysis by Payment Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c3c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'total_amount_of_payment_usdollars' in anomaly_results.columns:\n",
    "    fig = model_viz.plot_anomaly_comparison(\n",
    "        normal_df=normal_df,\n",
    "        anomaly_df=anomalies_df,\n",
    "        amount_col='total_amount_of_payment_usdollars',\n",
    "        score_col='anomaly_score'\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fc18a6",
   "metadata": {},
   "source": [
    "### Training Performance Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaff25ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_features = ['total_amount_of_payment_usdollars']\n",
    "if 'amt_to_avg_ratio' in anomaly_results.columns:\n",
    "    comparison_features.append('amt_to_avg_ratio')\n",
    "if 'hist_pay_avg' in anomaly_results.columns:\n",
    "    comparison_features.append('hist_pay_avg')\n",
    "\n",
    "comparison_stats = model_viz.print_anomaly_stats(\n",
    "    normal_df=normal_df,\n",
    "    anomaly_df=anomalies_df,\n",
    "    score_col='anomaly_score',\n",
    "    comparison_features=comparison_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f72af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_anomaly_pct = (train_anomaly_count / len(X_train)) * 100\n",
    "test_anomaly_pct = (test_anomaly_count / len(X_test)) * 100\n",
    "\n",
    "print(f\"Training Set: {train_anomaly_count:,} anomalies ({train_anomaly_pct:.2f}%)\")\n",
    "print(f\"Test Set: {test_anomaly_count:,} anomalies ({test_anomaly_pct:.2f}%)\")\n",
    "print(f\"Decision Threshold: {isolation_forest.offset_:.6f}\")\n",
    "print(f\"Score Range: [{all_scores.min():.6f}, {all_scores.max():.6f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b861a2",
   "metadata": {},
   "source": [
    "## 8. Autoencoder Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d52af40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ae = df_payments[numeric_features].copy().astype(float)\n",
    "X_ae = X_ae.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "missing_pct_ae = (X_ae.isnull().sum() / len(X_ae)) * 100\n",
    "cols_to_keep_ae = missing_pct_ae[missing_pct_ae <= 50].index.tolist()\n",
    "X_ae = X_ae[cols_to_keep_ae]\n",
    "\n",
    "for col in X_ae.columns:\n",
    "    q1, q3 = X_ae[col].quantile(0.25), X_ae[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    X_ae[col] = X_ae[col].clip(lower=q1 - 3*iqr, upper=q3 + 3*iqr)\n",
    "\n",
    "X_ae = X_ae.fillna(X_ae.median())\n",
    "\n",
    "scaler_ae = MinMaxScaler(feature_range=(0, 1))\n",
    "X_ae_scaled = scaler_ae.fit_transform(X_ae)\n",
    "X_ae_scaled = pd.DataFrame(X_ae_scaled, columns=X_ae.columns)\n",
    "\n",
    "X_ae_train, X_ae_test = train_test_split(X_ae_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Data prepared: {X_ae_scaled.shape} | Train: {len(X_ae_train):,} | Test: {len(X_ae_test):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79795ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_ae_scaled.shape[1]\n",
    "encoding_dim_1 = max(input_dim // 2, 32)\n",
    "encoding_dim_2 = max(input_dim // 4, 16)\n",
    "bottleneck_dim = max(input_dim // 8, 8)\n",
    "\n",
    "autoencoder = Sequential([\n",
    "    layers.Dense(encoding_dim_1, activation='relu', input_shape=(input_dim,), name='encoder_input'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(encoding_dim_2, activation='relu', name='encoder_middle'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(bottleneck_dim, activation='relu', name='bottleneck'),\n",
    "    layers.Dense(encoding_dim_2, activation='relu', name='decoder_middle'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(encoding_dim_1, activation='relu', name='decoder_layer'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(input_dim, activation='sigmoid', name='decoder_output')\n",
    "], name='Autoencoder')\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "autoencoder.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "print(f\"Architecture: {input_dim} -> {encoding_dim_1} -> {encoding_dim_2} -> {bottleneck_dim} (bottleneck)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1973f470",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=7,\n",
    "    restore_best_weights=True,\n",
    "    verbose=0,\n",
    "    min_delta=1e-5\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history_ae = autoencoder.fit(\n",
    "    X_ae_train, X_ae_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "training_time_ae = time.time() - start_time\n",
    "print(f\"Training: {len(history_ae.history['loss'])} epochs in {training_time_ae:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9742793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_ae = autoencoder.predict(X_ae_train, verbose=0)\n",
    "train_mse_ae = np.mean(np.square(X_ae_train - train_predictions_ae), axis=1)\n",
    "\n",
    "test_predictions_ae = autoencoder.predict(X_ae_test, verbose=0)\n",
    "test_mse_ae = np.mean(np.square(X_ae_test - test_predictions_ae), axis=1)\n",
    "\n",
    "all_data_ae = np.vstack([X_ae_train, X_ae_test])\n",
    "all_predictions_ae = autoencoder.predict(all_data_ae, verbose=0)\n",
    "all_reconstruction_errors = np.mean(np.square(all_data_ae - all_predictions_ae), axis=1)\n",
    "\n",
    "threshold_ae = np.percentile(train_mse_ae, 95)\n",
    "anomaly_labels_ae = (all_reconstruction_errors > threshold_ae).astype(int)\n",
    "anomaly_count_ae = anomaly_labels_ae.sum()\n",
    "\n",
    "print(f\"Threshold: {threshold_ae:.6f} | Anomalies: {anomaly_count_ae:,} ({anomaly_count_ae/len(anomaly_labels_ae)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785d4eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_ae_full = autoencoder.predict(X_ae_scaled, verbose=0)\n",
    "all_reconstruction_errors_full = np.mean(np.square(X_ae_scaled - all_predictions_ae_full), axis=1)\n",
    "anomaly_labels_ae_full = (all_reconstruction_errors_full > threshold_ae).astype(int)\n",
    "\n",
    "anomaly_results_ae = df_payments.copy()\n",
    "anomaly_results_ae['reconstruction_error'] = all_reconstruction_errors_full\n",
    "anomaly_results_ae['is_anomaly'] = anomaly_labels_ae_full\n",
    "\n",
    "anomalies_df_ae = anomaly_results_ae[anomaly_results_ae['is_anomaly'] == 1].copy()\n",
    "anomalies_df_ae = anomalies_df_ae.sort_values('reconstruction_error', ascending=False)\n",
    "normal_df_ae = anomaly_results_ae[anomaly_results_ae['is_anomaly'] == 0]\n",
    "\n",
    "top_anomalies_ae = model_viz.display_top_anomalies(\n",
    "    anomalies_df=anomalies_df_ae,\n",
    "    score_col='reconstruction_error',\n",
    "    top_n=10\n",
    ")\n",
    "display(top_anomalies_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c08557",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = model_viz.plot_training_history(history=history_ae.history, figsize=(15, 5))\n",
    "plt.show()\n",
    "\n",
    "fig = model_viz.plot_reconstruction_error_analysis(\n",
    "    train_mse=train_mse_ae,\n",
    "    test_mse=test_mse_ae,\n",
    "    threshold=threshold_ae,\n",
    "    all_reconstruction_errors=all_reconstruction_errors,\n",
    "    figsize=(15, 5)\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "if 'total_amount_of_payment_usdollars' in anomaly_results_ae.columns:\n",
    "    fig = model_viz.plot_anomaly_comparison(\n",
    "        normal_df=normal_df_ae,\n",
    "        anomaly_df=anomalies_df_ae,\n",
    "        amount_col='total_amount_of_payment_usdollars',\n",
    "        score_col='reconstruction_error'\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42617ab9",
   "metadata": {},
   "source": [
    "## 9. XGBoost Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a618c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_xgb = df_payments[numeric_features].copy().astype(float)\n",
    "X_xgb = X_xgb.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "missing_pct_xgb = (X_xgb.isnull().sum() / len(X_xgb)) * 100\n",
    "cols_to_keep_xgb = missing_pct_xgb[missing_pct_xgb <= 50].index.tolist()\n",
    "X_xgb = X_xgb[cols_to_keep_xgb]\n",
    "\n",
    "for col in X_xgb.columns:\n",
    "    q1, q3 = X_xgb[col].quantile(0.25), X_xgb[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    X_xgb[col] = X_xgb[col].clip(lower=q1 - 3*iqr, upper=q3 + 3*iqr)\n",
    "\n",
    "X_xgb = X_xgb.fillna(X_xgb.median())\n",
    "\n",
    "outlier_indicators = []\n",
    "for col in X_xgb.columns:\n",
    "    z_scores = np.abs((X_xgb[col] - X_xgb[col].mean()) / X_xgb[col].std())\n",
    "    outlier_indicators.append((z_scores > 3).astype(int))\n",
    "\n",
    "outlier_score = pd.DataFrame(outlier_indicators).T.sum(axis=1)\n",
    "threshold_value = np.percentile(outlier_score, 95)\n",
    "y_pseudo = (outlier_score >= threshold_value).astype(int)\n",
    "\n",
    "print(f\"Pseudo-labels: {y_pseudo.sum():,} anomalies ({y_pseudo.sum()/len(y_pseudo)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaebdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_xgb = RobustScaler()\n",
    "X_xgb_scaled = scaler_xgb.fit_transform(X_xgb)\n",
    "X_xgb_scaled = pd.DataFrame(X_xgb_scaled, columns=X_xgb.columns)\n",
    "\n",
    "X_xgb_train, X_xgb_test, y_xgb_train, y_xgb_test = train_test_split(\n",
    "    X_xgb_scaled, y_pseudo, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_pseudo\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_xgb_train):,} ({y_xgb_train.sum()} anomalies) | Test: {len(X_xgb_test):,} ({y_xgb_test.sum()} anomalies)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5583d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos_weight = (y_xgb_train == 0).sum() / (y_xgb_train == 1).sum()\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 3,\n",
    "    'gamma': 0.1,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "\n",
    "start_time = time.time()\n",
    "xgb_model.fit(\n",
    "    X_xgb_train, y_xgb_train,\n",
    "    eval_set=[(X_xgb_train, y_xgb_train), (X_xgb_test, y_xgb_test)],\n",
    "    verbose=False\n",
    ")\n",
    "training_time_xgb = time.time() - start_time\n",
    "\n",
    "print(f\"Training: {training_time_xgb:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_xgb = xgb_model.predict(X_xgb_train)\n",
    "train_proba_xgb = xgb_model.predict_proba(X_xgb_train)[:, 1]\n",
    "\n",
    "test_pred_xgb = xgb_model.predict(X_xgb_test)\n",
    "test_proba_xgb = xgb_model.predict_proba(X_xgb_test)[:, 1]\n",
    "\n",
    "train_auc = roc_auc_score(y_xgb_train, train_proba_xgb)\n",
    "test_auc = roc_auc_score(y_xgb_test, test_proba_xgb)\n",
    "\n",
    "print(f\"AUC: Train={train_auc:.6f} | Test={test_auc:.6f}\")\n",
    "\n",
    "results_xgb = xgb_model.evals_result()\n",
    "best_iter = None\n",
    "try:\n",
    "    best_iter = xgb_model.best_iteration\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "fig = model_viz.plot_xgboost_training_curves(results=results_xgb, best_iteration=best_iter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea88366",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = model_viz.plot_confusion_matrices(\n",
    "    y_train=y_xgb_train,\n",
    "    train_pred=train_pred_xgb,\n",
    "    y_test=y_xgb_test,\n",
    "    test_pred=test_pred_xgb\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "fig = model_viz.plot_roc_curves(\n",
    "    y_train=y_xgb_train,\n",
    "    train_proba=train_proba_xgb,\n",
    "    y_test=y_xgb_test,\n",
    "    test_proba=test_proba_xgb,\n",
    "    train_auc=train_auc,\n",
    "    test_auc=test_auc\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2346bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, feature_importance = model_viz.plot_feature_importance(\n",
    "    feature_names=X_xgb.columns,\n",
    "    feature_importances=xgb_model.feature_importances_,\n",
    "    top_n=15\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 Important Features:\")\n",
    "display(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709abce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_proba_xgb_full = xgb_model.predict_proba(X_xgb_scaled)[:, 1]\n",
    "\n",
    "threshold_xgb = 0.5\n",
    "anomaly_labels_xgb_full = (all_proba_xgb_full >= threshold_xgb).astype(int)\n",
    "\n",
    "anomaly_results_xgb = df_payments.copy()\n",
    "anomaly_results_xgb['anomaly_score'] = all_proba_xgb_full\n",
    "anomaly_results_xgb['is_anomaly'] = anomaly_labels_xgb_full\n",
    "\n",
    "anomalies_df_xgb = anomaly_results_xgb[anomaly_results_xgb['is_anomaly'] == 1].copy()\n",
    "anomalies_df_xgb = anomalies_df_xgb.sort_values('anomaly_score', ascending=False)\n",
    "normal_df_xgb = anomaly_results_xgb[anomaly_results_xgb['is_anomaly'] == 0]\n",
    "\n",
    "print(f\"Anomalies: {len(anomalies_df_xgb):,} ({len(anomalies_df_xgb)/len(anomaly_results_xgb)*100:.2f}%)\")\n",
    "\n",
    "top_anomalies_xgb = model_viz.display_top_anomalies(\n",
    "    anomalies_df=anomalies_df_xgb,\n",
    "    score_col='anomaly_score',\n",
    "    top_n=10\n",
    ")\n",
    "display(top_anomalies_xgb)\n",
    "\n",
    "if 'total_amount_of_payment_usdollars' in anomaly_results_xgb.columns:\n",
    "    fig = model_viz.plot_anomaly_comparison(\n",
    "        normal_df=normal_df_xgb,\n",
    "        anomaly_df=anomalies_df_xgb,\n",
    "        amount_col='total_amount_of_payment_usdollars',\n",
    "        score_col='anomaly_score'\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e9c33f",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d753dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_score_separation(estimator, X):\n",
    "    try:\n",
    "        scores = estimator.decision_function(X)\n",
    "        predictions = estimator.predict(X)\n",
    "        \n",
    "        if np.any(~np.isfinite(scores)):\n",
    "            return 0.0\n",
    "        \n",
    "        anomaly_scores = scores[predictions == -1]\n",
    "        normal_scores = scores[predictions == 1]\n",
    "        \n",
    "        if len(anomaly_scores) < 3 or len(normal_scores) < 3:\n",
    "            return 0.0\n",
    "        \n",
    "        mean_diff = normal_scores.mean() - anomaly_scores.mean()\n",
    "        pooled_std = np.sqrt((normal_scores.std()**2 + anomaly_scores.std()**2) / 2)\n",
    "        \n",
    "        if pooled_std < 1e-6:\n",
    "            return 0.0\n",
    "        \n",
    "        separation = mean_diff / pooled_std\n",
    "        \n",
    "        if not np.isfinite(separation):\n",
    "            return 0.0\n",
    "            \n",
    "        return float(separation)\n",
    "        \n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "anomaly_scorer = make_scorer(anomaly_score_separation, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a5d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_param_grid = {\n",
    "    'n_estimators': [150, 250],\n",
    "    'contamination': [0.03, 0.05, 0.07],\n",
    "    'max_samples': [512],\n",
    "    'max_features': [0.8, 1.0],\n",
    "    'bootstrap': [False, True]\n",
    "}\n",
    "\n",
    "print(f\"Starting Grid Search...\")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=IsolationForest(random_state=42, n_jobs=-1, verbose=0),\n",
    "    param_grid=grid_param_grid,\n",
    "    scoring=anomaly_scorer,\n",
    "    cv=2,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train)\n",
    "print(f\"Grid Search complete | Best Score: {grid_search.best_score_:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a81a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_param_dist = {\n",
    "    'n_estimators': randint(50, 500),\n",
    "    'contamination': uniform(0.01, 0.15),\n",
    "    'max_samples': [512, 1024, 2048],\n",
    "    'max_features': uniform(0.5, 0.5),\n",
    "    'bootstrap': [False, True]\n",
    "}\n",
    "\n",
    "print(f\"Starting Randomized Search...\")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=IsolationForest(random_state=42, n_jobs=-1, verbose=0),\n",
    "    param_distributions=random_param_dist,\n",
    "    n_iter=30,\n",
    "    scoring=anomaly_scorer,\n",
    "    cv=2,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "random_search.fit(X_train)\n",
    "print(f\"Randomized Search complete | Best Score: {random_search.best_score_:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dcec7f",
   "metadata": {},
   "source": [
    "### Hyperparameter Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89ffdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "random_results_df = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "fig = model_viz.plot_grid_search_results(grid_results_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = model_viz.plot_random_search_results(random_results_df, random_search.best_score_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25049680",
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search.best_score_ > random_search.best_score_:\n",
    "    best_search = grid_search\n",
    "    best_method = 'Grid Search'\n",
    "else:\n",
    "    best_search = random_search\n",
    "    best_method = 'Randomized Search'\n",
    "\n",
    "optimal_params = best_search.best_params_.copy()\n",
    "optimal_params['random_state'] = 42\n",
    "optimal_params['n_jobs'] = -1\n",
    "optimal_params['verbose'] = 0\n",
    "\n",
    "print(f\"Best Method: {best_method}\")\n",
    "print(f\"Optimal Parameters: {optimal_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64b74ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training optimized model...\")\n",
    "\n",
    "optimized_model = IsolationForest(**optimal_params)\n",
    "optimized_model.fit(X_train)\n",
    "\n",
    "optimized_test_predictions = optimized_model.predict(X_test)\n",
    "optimized_test_anomalies = (optimized_test_predictions == -1).sum()\n",
    "\n",
    "print(f\"Optimized Model - Test Anomalies: {optimized_test_anomalies:,} ({optimized_test_anomalies/len(X_test)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fce299d",
   "metadata": {},
   "source": [
    "### Baseline vs Optimized Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bdf44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_test_scores = test_scores\n",
    "optimized_test_scores = optimized_model.decision_function(X_test)\n",
    "optimized_train_predictions = optimized_model.predict(X_train)\n",
    "optimized_train_anomalies = (optimized_train_predictions == -1).sum()\n",
    "\n",
    "baseline_score_mean = baseline_test_scores.mean()\n",
    "baseline_score_std = baseline_test_scores.std()\n",
    "optimized_score_mean = optimized_test_scores.mean()\n",
    "optimized_score_std = optimized_test_scores.std()\n",
    "\n",
    "fig = model_viz.plot_model_comparison(\n",
    "    baseline_test_scores,\n",
    "    optimized_test_scores,\n",
    "    train_anomaly_count,\n",
    "    test_anomaly_count,\n",
    "    optimized_train_anomalies,\n",
    "    optimized_test_anomalies,\n",
    "    len(X_train),\n",
    "    len(X_test),\n",
    "    baseline_score_mean,\n",
    "    optimized_score_mean,\n",
    "    baseline_score_std,\n",
    "    optimized_score_std\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c0b453",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c770bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_summary = pd.DataFrame({\n",
    "    'Method': ['Baseline', 'Grid Search', 'Randomized Search'],\n",
    "    'Best Score': ['N/A', f\"{grid_search.best_score_:.6f}\", f\"{random_search.best_score_:.6f}\"],\n",
    "    'N Estimators': [\n",
    "        baseline_params['n_estimators'],\n",
    "        grid_search.best_params_['n_estimators'],\n",
    "        random_search.best_params_['n_estimators']\n",
    "    ],\n",
    "    'Contamination': [\n",
    "        baseline_params['contamination'],\n",
    "        grid_search.best_params_['contamination'],\n",
    "        random_search.best_params_['contamination']\n",
    "    ],\n",
    "    'Test Anomalies': [\n",
    "        f\"{test_anomaly_count:,} ({test_anomaly_count/len(X_test)*100:.2f}%)\",\n",
    "        'N/A',\n",
    "        f\"{optimized_test_anomalies:,} ({optimized_test_anomalies/len(X_test)*100:.2f}%)\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Hyperparameter Tuning Comparison:\")\n",
    "display(tuning_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af1a72",
   "metadata": {},
   "source": [
    "## 11. Model Deployment to SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750ec791",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sagemaker_region = sagemaker_session.boto_region_name\n",
    "sagemaker_bucket = sagemaker_session.default_bucket()\n",
    "s3_prefix = 'cms-anomaly-detection'\n",
    "\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "print(f\"SageMaker Region: {sagemaker_region} | Bucket: {sagemaker_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4776e3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = {\n",
    "    \"model_details\": {\n",
    "        \"name\": \"CMS Open Payments Anomaly Detector\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"type\": \"Isolation Forest\",\n",
    "        \"framework\": \"scikit-learn\",\n",
    "        \"created_date\": datetime.now().strftime('%Y-%m-%d'),\n",
    "        \"created_by\": \"AAI-540 Team\"\n",
    "    },\n",
    "    \"model_parameters\": {\n",
    "        \"contamination\": optimal_params.get('contamination', 0.05),\n",
    "        \"n_estimators\": optimal_params.get('n_estimators', 200),\n",
    "        \"max_samples\": optimal_params.get('max_samples', 'auto'),\n",
    "        \"max_features\": optimal_params.get('max_features', 1.0)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('model_card.json', 'w') as f:\n",
    "    json.dump(model_card, f, indent=2)\n",
    "\n",
    "print(\"Model card created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1900e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_script = '''import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model_path = os.path.join(model_dir, 'model.joblib')\n",
    "    scaler_path = os.path.join(model_dir, 'scaler.joblib')\n",
    "    \n",
    "    model = joblib.load(model_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    \n",
    "    return {'model': model, 'scaler': scaler}\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    if request_content_type == 'application/json':\n",
    "        data = json.loads(request_body)\n",
    "        return pd.DataFrame(data)\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported content type: {request_content_type}')\n",
    "\n",
    "def predict_fn(input_data, model_artifacts):\n",
    "    model = model_artifacts['model']\n",
    "    scaler = model_artifacts['scaler']\n",
    "    \n",
    "    scaled_data = scaler.transform(input_data)\n",
    "    predictions = model.predict(scaled_data)\n",
    "    scores = model.decision_function(scaled_data)\n",
    "    \n",
    "    anomaly_labels = (predictions == -1).astype(int).tolist()\n",
    "    \n",
    "    return {\n",
    "        'predictions': anomaly_labels,\n",
    "        'anomaly_scores': scores.tolist()\n",
    "    }\n",
    "\n",
    "def output_fn(prediction, response_content_type):\n",
    "    if response_content_type == 'application/json':\n",
    "        return json.dumps(prediction)\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported content type: {response_content_type}')\n",
    "'''\n",
    "\n",
    "with open('inference.py', 'w') as f:\n",
    "    f.write(inference_script)\n",
    "\n",
    "print(\"Inference script created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb906b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path('model_artifacts')\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "joblib.dump(optimized_model, model_dir / 'model.joblib')\n",
    "joblib.dump(scaler, model_dir / 'scaler.joblib')\n",
    "\n",
    "import shutil\n",
    "shutil.copy('model_card.json', model_dir / 'model_card.json')\n",
    "\n",
    "model_archive = 'model.tar.gz'\n",
    "with tarfile.open(model_archive, 'w:gz') as tar:\n",
    "    tar.add(model_dir, arcname='.')\n",
    "\n",
    "print(f\"Model archived: {model_archive}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12263571",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s3_key = f\"{s3_prefix}/models/{model_archive}\"\n",
    "model_s3_uri = f\"s3://{sagemaker_bucket}/{model_s3_key}\"\n",
    "\n",
    "s3_client.upload_file(model_archive, sagemaker_bucket, model_s3_key)\n",
    "\n",
    "print(f\"Model uploaded to S3: {model_s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7847ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn import SKLearnModel\n",
    "\n",
    "sklearn_model = SKLearnModel(\n",
    "    model_data=model_s3_uri,\n",
    "    role=role,\n",
    "    entry_point='inference.py',\n",
    "    framework_version='1.4-1',\n",
    "    py_version='py3',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    name=f\"cms-anomaly-model-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    ")\n",
    "\n",
    "print(f\"SKLearn Model created: {sklearn_model.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06798f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f\"cms-anomaly-endpoint-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "print(f\"Deploying to endpoint: {endpoint_name}\")\n",
    "\n",
    "predictor = sklearn_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")\n",
    "\n",
    "print(f\"Endpoint deployed: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea0bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = df.sample(n=100, random_state=42)\n",
    "X_test_sample = test_sample[numeric_features].copy()\n",
    "X_test_sample = X_test_sample.fillna(X_test_sample.median())\n",
    "\n",
    "single_record = X_test_sample.iloc[0:1].to_dict('records')\n",
    "\n",
    "start_time = time.time()\n",
    "response = predictor.predict(single_record)\n",
    "inference_time = time.time() - start_time\n",
    "\n",
    "print(f\"Single prediction: {inference_time*1000:.2f} ms\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a07339",
   "metadata": {},
   "outputs": [],
   "source": [
    "latencies = []\n",
    "n_tests = 50\n",
    "\n",
    "for i in range(n_tests):\n",
    "    test_record = X_test_sample.sample(n=1).to_dict('records')\n",
    "    start = time.time()\n",
    "    _ = predictor.predict(test_record)\n",
    "    latencies.append((time.time() - start) * 1000)\n",
    "\n",
    "latency_stats = {\n",
    "    'Mean': np.mean(latencies),\n",
    "    'Median': np.median(latencies),\n",
    "    'P95': np.percentile(latencies, 95),\n",
    "    'P99': np.percentile(latencies, 99)\n",
    "}\n",
    "\n",
    "print(f\"Latency Stats: Mean={latency_stats['Mean']:.2f}ms, P95={latency_stats['P95']:.2f}ms, P99={latency_stats['P99']:.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9739da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = {\n",
    "    'endpoint_name': endpoint_name,\n",
    "    'model_s3_uri': model_s3_uri,\n",
    "    'region': sagemaker_region,\n",
    "    'bucket': sagemaker_bucket,\n",
    "    'deployment_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'instance_type': 'ml.m5.large',\n",
    "    'framework': 'scikit-learn',\n",
    "    'framework_version': '1.4-1'\n",
    "}\n",
    "\n",
    "with open('deployment_config.json', 'w') as f:\n",
    "    json.dump(deployment_config, f, indent=2)\n",
    "\n",
    "print(\"Deployment configuration saved\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
