{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f81b16",
   "metadata": {},
   "source": [
    "# Autoencoder-Based Anomaly Detection for CMS Open Payments\n",
    "\n",
    "**Project:** AAI-540 Machine Learning Operations - Final Team Project  \n",
    "**Context:** Continuation of notebook 03 - Feature Engineering & Model Preparation  \n",
    "**Objective:** Train an Autoencoder model to detect anomalous payment patterns using reconstruction error as anomaly score\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup & Data Loading](#setup)\n",
    "2. [Load Feature Store Data](#loading)\n",
    "3. [Data Preparation & Normalization](#preparation)\n",
    "4. [Autoencoder Architecture Design](#architecture)\n",
    "5. [Model Training with Early Stopping](#training)\n",
    "6. [Performance Evaluation](#evaluation)\n",
    "7. [Anomaly Score Calculation](#scoring)\n",
    "8. [Visualizations & Metrics](#visualizations)\n",
    "9. [Confusion Matrix & ROC Analysis](#confusion)\n",
    "10. [Summary & Outputs](#summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b3046e",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading\n",
    "\n",
    "Load dependencies and restore configuration from notebook 03 (Feature Engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d49056c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.18.0\n",
      "GPU Available: False\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "import boto3\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "609baa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring configuration from notebook 03 (Feature Engineering)...\n",
      "no stored variable or alias cms_payments_fg\n",
      "Feature Group: cms-payments-fg-07-21-49-56\n",
      "Record Identifier: covered_recipient_profile_id\n",
      "Region: us-east-1\n",
      "S3 Bucket: cmsopenpaymentsystemslight\n",
      "Athena Staging: s3://cmsopenpaymentsystemslight/athena/staging\n"
     ]
    }
   ],
   "source": [
    "# Restore configuration and data from notebook 03\n",
    "print(\"Restoring configuration from notebook 03 (Feature Engineering)...\")\n",
    "\n",
    "# Check required variables\n",
    "required_vars = ['cms_payments_feature_group_name', 'record_identifier_feature_name', \n",
    "                 'region', 'bucket', 's3_athena_staging']\n",
    "                 \n",
    "try:\n",
    "    %store -r cms_payments_feature_group_name\n",
    "    %store -r record_identifier_feature_name\n",
    "    %store -r featurestore_runtime\n",
    "    %store -r cms_payments_fg\n",
    "    %store -r df\n",
    "    %store -r region\n",
    "    %store -r bucket\n",
    "    %store -r s3_athena_staging\n",
    "    %store -r database_name\n",
    "    %store -r table_name_parquet\n",
    "    \n",
    "    # Validate critical variables\n",
    "    missing = []\n",
    "    if 'cms_payments_feature_group_name' not in dir():\n",
    "        missing.append('cms_payments_feature_group_name')\n",
    "    if 'region' not in dir():\n",
    "        missing.append('region')\n",
    "    if 'bucket' not in dir():\n",
    "        missing.append('bucket')\n",
    "        \n",
    "    if missing:\n",
    "        raise NameError(f\"Missing required variables: {missing}\")\n",
    "    \n",
    "    print(f\"Feature Group: {cms_payments_feature_group_name}\")\n",
    "    print(f\"Record Identifier: {record_identifier_feature_name}\")\n",
    "    print(f\"Region: {region}\")\n",
    "    print(f\"S3 Bucket: {bucket}\")\n",
    "    print(f\"Athena Staging: {s3_athena_staging}\")\n",
    "    \n",
    "except NameError as e:\n",
    "    print(f\"\\nERROR: {str(e)}\")\n",
    "    print(\"\\nPREREQUISITE: Please run notebook 03 (03_feature_engineering.ipynb) first\")\n",
    "    print(\"That notebook creates the Feature Store and stores required variables.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5850b043",
   "metadata": {},
   "source": [
    "## 2. Load Feature Store & Production Data from Athena\n",
    "\n",
    "Query production S3 parquet data from Feature Store offline store using Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "700e4cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Athena Staging: s3://cmsopenpaymentsystemslight/athena/staging\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import awswrangler as wr\n",
    "from sagemaker.session import Session\n",
    "\n",
    "# Initialize AWS clients\n",
    "region = boto3.Session().region_name\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_session = Session(boto_session=boto_session)\n",
    "sagemaker_client = boto_session.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Athena Staging: {s3_athena_staging}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e3763cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Group: cms-payments-fg-07-21-49-56\n",
      "\n",
      "Attempting to load from Feature Store offline table...\n",
      "Recreated FeatureGroup connection\n",
      "Offline table: cms_payments_fg_07_21_49_56_864106638709_offline\n",
      "Executing Athena query...\n",
      "\n",
      "Error querying Feature Store: TABLE_NOT_FOUND: line 2:10: Table 'awsdatacatalog.sagemaker_featurestore.cms_payments_fg_07_21_49_56\n",
      "Data Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭─────────────────────────────── </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Traceback </span><span style=\"color: #ff7f7f; text-decoration-color: #ff7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> in &lt;module&gt;:65                                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">62 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">63 # Display data preview</span>                                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">64 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Data Preview:\"</span>)                                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>65 <span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Columns: {</span><span style=\"font-weight: bold; text-decoration: underline\">df_payments</span>.shape[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>]<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>)                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">66 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Data Types: {</span>df_payments.dtypes.value_counts().to_dict()<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>)                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">67 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Sample features: {</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>(df_payments.columns[:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">10</span>])<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>)                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">68 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008700; text-decoration-color: #008700\">'df_payments'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;0;0m╭─\u001b[0m\u001b[38;2;255;0;0m──────────────────────────────\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0mTraceback \u001b[0m\u001b[1;2;38;2;255;0;0m(most recent call last)\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[38;2;255;0;0m───────────────────────────────\u001b[0m\u001b[38;2;255;0;0m─╮\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m in <module>:65                                                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m62 \u001b[0m                                                                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m63 \u001b[0m\u001b[2m# Display data preview\u001b[0m                                                                      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m64 \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mData Preview:\u001b[0m\u001b[33m\"\u001b[0m)                                                                      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m65 \u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mColumns: \u001b[0m\u001b[33m{\u001b[0m\u001b[1;4mdf_payments\u001b[0m.shape[\u001b[94m1\u001b[0m]\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m66 \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mData Types: \u001b[0m\u001b[33m{\u001b[0mdf_payments.dtypes.value_counts().to_dict()\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                         \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m67 \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mSample features: \u001b[0m\u001b[33m{\u001b[0m\u001b[96mlist\u001b[0m(df_payments.columns[:\u001b[94m10\u001b[0m])\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                                 \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m68 \u001b[0m                                                                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[38;2;0;135;0m'df_payments'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load production data from Feature Store offline store\n",
    "print(f\"\\nFeature Group: {cms_payments_feature_group_name}\")\n",
    "\n",
    "start_load = time.time()\n",
    "\n",
    "try:\n",
    "    # Get Feature Store offline table information\n",
    "    print(\"\\nAttempting to load from Feature Store offline table...\")\n",
    "    \n",
    "    # Connect to Feature Group to get offline table name\n",
    "    from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "    import sagemaker\n",
    "    \n",
    "    # Use stored Feature Group object or recreate it\n",
    "    if 'cms_payments_fg' in dir() and cms_payments_fg is not None:\n",
    "        feature_group = cms_payments_fg\n",
    "        print(\"  Using stored FeatureGroup object\")\n",
    "    else:\n",
    "        # Recreate Feature Group connection\n",
    "        sagemaker_session = sagemaker.Session()\n",
    "        feature_group = FeatureGroup(\n",
    "            name=cms_payments_feature_group_name,\n",
    "            sagemaker_session=sagemaker_session\n",
    "        )\n",
    "        print(\"Recreated FeatureGroup connection\")\n",
    "    \n",
    "    # Get account ID for offline table name\n",
    "    sts_client = boto3.client('sts')\n",
    "    account_id = sts_client.get_caller_identity()['Account']\n",
    "    \n",
    "    # Feature Store offline table follows naming convention\n",
    "    offline_table_name = f\"{cms_payments_feature_group_name.replace('-', '_')}_{account_id}_offline\"\n",
    "    \n",
    "    # Query Feature Store offline data using Athena\n",
    "    query = f\"\"\"\n",
    "    SELECT * \n",
    "    FROM \"sagemaker_featurestore\".\"{offline_table_name}\"\n",
    "    WHERE is_deleted = false\n",
    "    LIMIT 1000000\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Offline table: {offline_table_name}\")\n",
    "    print(f\"Executing Athena query...\")\n",
    "    \n",
    "    # Query using awswrangler\n",
    "    df_payments = wr.athena.read_sql_query(\n",
    "        sql=query,\n",
    "        database=\"sagemaker_featurestore\",\n",
    "        s3_output=s3_athena_staging,\n",
    "        ctas_approach=False\n",
    "    )\n",
    "    \n",
    "    load_time = time.time() - start_load\n",
    "    \n",
    "    print(f\"\\nData loaded from Feature Store in {load_time:.2f} seconds\")\n",
    "    print(f\"Shape: {df_payments.shape[0]:,} rows x {df_payments.shape[1]} columns\")\n",
    "    print(f\"Memory: {df_payments.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "    print(f\"Source: Feature Store offline table (engineered features)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError querying Feature Store: {str(e)[:100]}\")\n",
    "\n",
    "# Display data preview\n",
    "print(\"Data Preview:\")\n",
    "print(f\"Columns: {df_payments.shape[1]}\")\n",
    "print(f\"Data Types: {df_payments.dtypes.value_counts().to_dict()}\")\n",
    "print(f\"Sample features: {list(df_payments.columns[:10])}\")\n",
    "\n",
    "# Check for engineered features\n",
    "engineered_features = ['hist_pay_avg', 'amt_to_avg_ratio', 'is_new_recipient']\n",
    "present_features = [f for f in engineered_features if f in df_payments.columns]\n",
    "print(f\"Engineered features present: {len(present_features)}/{len(engineered_features)}\")\n",
    "\n",
    "if len(present_features) < len(engineered_features):\n",
    "    missing = [f for f in engineered_features if f not in df_payments.columns]\n",
    "    print(f\"WARNING: Missing engineered features: {missing}\")\n",
    "\n",
    "\n",
    "display(df_payments.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4213e5c9",
   "metadata": {},
   "source": [
    "## 3. Data Preparation & Normalization\n",
    "\n",
    "Prepare features for Autoencoder training with appropriate scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e5a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric features for anomaly detection\n",
    "numeric_cols = df_payments.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Remove unwanted columns: IDs, timestamps, encoded text fields, and metadata\n",
    "cols_to_exclude = [\n",
    "    'EventTime', 'covered_recipient_profile_id', 'index',\n",
    "    'teaching_hospital_id', 'covered_recipient_npi',\n",
    "    'covered_recipient_first_name', 'covered_recipient_middle_name',\n",
    "    'covered_recipient_last_name', 'covered_recipient_name_suffix',\n",
    "    'recipient_primary_business_street_address_line2',\n",
    "    'recipient_zip_code', 'recipient_province', 'recipient_postal_code',\n",
    "    'submitting_applicable_manufacturer_or_applicable_gpo_name',\n",
    "    'applicable_manufacturer_or_applicable_gpo_making_payment_id',\n",
    "    'applicable_manufacturer_or_applicable_gpo_making_payment_name'\n",
    "]\n",
    "\n",
    "# Filter numeric features - keep only actual numeric measurements\n",
    "numeric_features = [col for col in numeric_cols \n",
    "                   if col not in cols_to_exclude \n",
    "                   and not any(x in col.lower() for x in ['_id', '_name', '_address', '_code', '_province', '_postal'])]\n",
    "\n",
    "print(f\"Total numeric features selected: {len(numeric_features)}\")\n",
    "print(f\"Sample features: {numeric_features[:15]}\")\n",
    "\n",
    "# Prepare feature matrix - convert to float\n",
    "X = df_payments[numeric_features].copy().astype(float)\n",
    "\n",
    "# Handle infinity values\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Check initial missing value percentage per column\n",
    "missing_pct = (X.isnull().sum() / len(X)) * 100\n",
    "print(f\"\\nColumns with >50% missing: {(missing_pct > 50).sum()}\")\n",
    "\n",
    "# Drop columns with more than 50% missing values\n",
    "cols_to_keep = missing_pct[missing_pct <= 50].index.tolist()\n",
    "X = X[cols_to_keep]\n",
    "print(f\"Features after dropping high-missing columns: {len(cols_to_keep)}\")\n",
    "\n",
    "# Clip extreme outliers using IQR method for each column\n",
    "for col in X.columns:\n",
    "    q1 = X[col].quantile(0.25)\n",
    "    q3 = X[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 3 * iqr\n",
    "    upper_bound = q3 + 3 * iqr\n",
    "    X[col] = X[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "# Handle remaining missing values with median\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Final validation\n",
    "print(f\"\\nFinal Data shape: {X.shape}\")\n",
    "print(f\"Missing values: {X.isnull().sum().sum()}\")\n",
    "print(f\"Infinity values: {np.isinf(X.values).sum()}\")\n",
    "print(f\"Features range - Min: {X.min().min():.2f}, Max: {X.max().max():.2f}\")\n",
    "print(f\"Sample feature names: {X.columns.tolist()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022aded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features using MinMaxScaler (0-1 range) for better Autoencoder performance\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "print(f\"Scaled data shape: {X_scaled.shape}\")\n",
    "print(f\"Scaled range - Min: {X_scaled.min().min():.4f}, Max: {X_scaled.max().max():.4f}\")\n",
    "print(f\"Mean: {X_scaled.mean().mean():.4f}, Std: {X_scaled.std().mean():.4f}\")\n",
    "\n",
    "# Split into train and test sets (80-20 split)\n",
    "# Training set contains mostly normal data; test set may contain anomalies\n",
    "X_train, X_test = train_test_split(\n",
    "    X_scaled, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {len(X_train):,}\")\n",
    "print(f\"Test set size: {len(X_test):,}\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9af3c9",
   "metadata": {},
   "source": [
    "## 4. Autoencoder Architecture Design\n",
    "\n",
    "Design a deep Autoencoder with bottleneck layer for feature compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccbaf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder architecture parameters\n",
    "input_dim = X_scaled.shape[1]\n",
    "encoding_dim_1 = max(input_dim // 2, 32)\n",
    "encoding_dim_2 = max(input_dim // 4, 16)\n",
    "bottleneck_dim = max(input_dim // 8, 8)\n",
    "\n",
    "print(f\"Input Dimension: {input_dim}\")\n",
    "print(f\"Encoding Layer 1: {encoding_dim_1}\")\n",
    "print(f\"Encoding Layer 2: {encoding_dim_2}\")\n",
    "print(f\"Bottleneck Dimension: {bottleneck_dim}\")\n",
    "\n",
    "# Build Autoencoder model\n",
    "autoencoder = Sequential([\n",
    "    # Encoder\n",
    "    layers.Dense(\n",
    "        encoding_dim_1, \n",
    "        activation='relu', \n",
    "        input_shape=(input_dim,),\n",
    "        name='encoder_input'\n",
    "    ),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(\n",
    "        encoding_dim_2, \n",
    "        activation='relu',\n",
    "        name='encoder_middle'\n",
    "    ),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(\n",
    "        bottleneck_dim, \n",
    "        activation='relu',\n",
    "        name='bottleneck'\n",
    "    ),\n",
    "    \n",
    "    # Decoder\n",
    "    layers.Dense(\n",
    "        encoding_dim_2, \n",
    "        activation='relu',\n",
    "        name='decoder_middle'\n",
    "    ),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(\n",
    "        encoding_dim_1, \n",
    "        activation='relu',\n",
    "        name='decoder_layer'\n",
    "    ),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(\n",
    "        input_dim, \n",
    "        activation='sigmoid',\n",
    "        name='decoder_output'\n",
    "    )\n",
    "], name='Autoencoder')\n",
    "\n",
    "# Display model architecture\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8da64b0",
   "metadata": {},
   "source": [
    "## 5. Model Training with Early Stopping\n",
    "\n",
    "Train the Autoencoder with optimal configuration and early stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b01867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model with Adam optimizer\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "autoencoder.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully\")\n",
    "print(f\"Optimizer: Adam (lr=0.001)\")\n",
    "print(f\"Loss function: Mean Squared Error (MSE)\")\n",
    "print(f\"Metric: Mean Absolute Error (MAE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f9cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for training\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1,\n",
    "    min_delta=1e-5\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training configuration configured:\")\n",
    "print(\"Early Stopping: patience=10, min_delta=1e-5\")\n",
    "print(\"Reduce LR on Plateau: factor=0.5, patience=5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2097cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Autoencoder\n",
    "print(\"Starting Autoencoder training...\")\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Validation samples (20% of train): Test set size {len(X_test):,}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = autoencoder.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "print(f\"Total epochs trained: {len(history.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb1e5b",
   "metadata": {},
   "source": [
    "## 6. Performance Evaluation\n",
    "\n",
    "Evaluate model performance on training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb96065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on training set\n",
    "train_predictions = autoencoder.predict(X_train, verbose=0)\n",
    "train_mse = np.mean(np.square(X_train - train_predictions), axis=1)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_predictions = autoencoder.predict(X_test, verbose=0)\n",
    "test_mse = np.mean(np.square(X_test - test_predictions), axis=1)\n",
    "\n",
    "print(\"Model Evaluation:\")\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"Reconstruction MSE - Mean: {train_mse.mean():.6f}, Std: {train_mse.std():.6f}\")\n",
    "print(f\"Reconstruction MSE - Min: {train_mse.min():.6f}, Max: {train_mse.max():.6f}\")\n",
    "\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"Reconstruction MSE - Mean: {test_mse.mean():.6f}, Std: {test_mse.std():.6f}\")\n",
    "print(f\"Reconstruction MSE - Min: {test_mse.min():.6f}, Max: {test_mse.max():.6f}\")\n",
    "\n",
    "# Model performance metrics\n",
    "train_final_loss = history.history['loss'][-1]\n",
    "train_final_mae = history.history['mae'][-1]\n",
    "val_final_loss = history.history['val_loss'][-1]\n",
    "val_final_mae = history.history['val_mae'][-1]\n",
    "\n",
    "print(f\"\\nFinal Epoch Performance:\")\n",
    "print(f\"Training Loss (MSE): {train_final_loss:.6f}, MAE: {train_final_mae:.6f}\")\n",
    "print(f\"Validation Loss (MSE): {val_final_loss:.6f}, MAE: {val_final_mae:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ea4d7",
   "metadata": {},
   "source": [
    "## 7. Anomaly Score Calculation\n",
    "\n",
    "Generate anomaly scores and identify outliers using reconstruction error threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c9c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and test predictions for comprehensive analysis\n",
    "all_data = np.vstack([X_train, X_test])\n",
    "all_predictions = autoencoder.predict(all_data, verbose=0)\n",
    "all_reconstruction_errors = np.mean(np.square(all_data - all_predictions), axis=1)\n",
    "\n",
    "# Calculate anomaly threshold (95th percentile of training errors)\n",
    "threshold_percentile = 95\n",
    "threshold = np.percentile(train_mse, threshold_percentile)\n",
    "\n",
    "print(f\"Anomaly Detection Configuration:\")\n",
    "print(f\"Percentile for threshold: {threshold_percentile}\")\n",
    "print(f\"Anomaly threshold: {threshold:.6f}\")\n",
    "\n",
    "# Label anomalies (1 = anomaly, 0 = normal)\n",
    "anomaly_labels = (all_reconstruction_errors > threshold).astype(int)\n",
    "anomaly_count = anomaly_labels.sum()\n",
    "anomaly_percentage = (anomaly_count / len(anomaly_labels)) * 100\n",
    "\n",
    "print(f\"\\nAnomaly Detection Results:\")\n",
    "print(f\"Total records: {len(anomaly_labels):,}\")\n",
    "print(f\"Anomalies detected: {anomaly_count:,} ({anomaly_percentage:.2f}%)\")\n",
    "print(f\"Normal records: {len(anomaly_labels) - anomaly_count:,}\")\n",
    "\n",
    "# Anomaly score distribution\n",
    "print(f\"\\nReconstruction Error (Anomaly Score) Statistics:\")\n",
    "print(f\"Mean: {all_reconstruction_errors.mean():.6f}\")\n",
    "print(f\"Median: {np.median(all_reconstruction_errors):.6f}\")\n",
    "print(f\"Std Dev: {all_reconstruction_errors.std():.6f}\")\n",
    "print(f\"Min: {all_reconstruction_errors.min():.6f}\")\n",
    "print(f\"Max: {all_reconstruction_errors.max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd360c0f",
   "metadata": {},
   "source": [
    "## 8. Visualizations & Metrics\n",
    "\n",
    "Visualize training history, loss distributions, and anomaly scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba1d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Training and Validation Loss\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('Loss (MSE)', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Model Training History - Loss', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Training and Validation MAE\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
    "ax2.plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('MAE', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Model Training History - MAE', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Reconstruction Error Distribution\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(train_mse, bins=50, alpha=0.7, label='Training', color='blue', edgecolor='black')\n",
    "ax3.hist(test_mse, bins=50, alpha=0.7, label='Test', color='red', edgecolor='black')\n",
    "ax3.axvline(threshold, color='green', linestyle='--', linewidth=2.5, label=f'Threshold ({threshold:.6f})')\n",
    "ax3.set_xlabel('Reconstruction Error (MSE)', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Distribution of Reconstruction Errors', fontsize=12, fontweight='bold')\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Anomaly Score Distribution\n",
    "ax4 = axes[1, 1]\n",
    "ax4.hist(all_reconstruction_errors, bins=100, alpha=0.8, color='purple', edgecolor='black')\n",
    "ax4.axvline(threshold, color='red', linestyle='--', linewidth=2.5, label=f'Anomaly Threshold')\n",
    "ax4.axvline(all_reconstruction_errors.mean(), color='orange', linestyle=':', linewidth=2.5, label='Mean Score')\n",
    "ax4.set_xlabel('Anomaly Score', fontsize=11, fontweight='bold')\n",
    "ax4.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "ax4.set_title('Complete Anomaly Score Distribution', fontsize=12, fontweight='bold')\n",
    "ax4.legend(fontsize=10)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('autoencoder_training_analysis.png', dpi=150, bbox_inches='tight')\n",
    "print(\"Training analysis visualization saved\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b1ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional visualization: Anomaly score by percentile\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "sorted_errors = np.sort(all_reconstruction_errors)\n",
    "percentiles = np.arange(1, 101)\n",
    "\n",
    "ax.plot(percentiles, np.percentile(all_reconstruction_errors, percentiles), \n",
    "        linewidth=2.5, color='darkblue', marker='o', markersize=3)\n",
    "ax.axhline(y=threshold, color='red', linestyle='--', linewidth=2.5, \n",
    "           label=f'95th Percentile Threshold: {threshold:.6f}')\n",
    "ax.fill_between(percentiles, 0, np.percentile(all_reconstruction_errors, percentiles), \n",
    "                alpha=0.2, color='blue')\n",
    "ax.set_xlabel('Percentile', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Anomaly Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Anomaly Score Percentile Distribution', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f036150",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrix & ROC Analysis\n",
    "\n",
    "Analyze model performance with statistical metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c571c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic labels for evaluation (assuming normal data in train set, potential anomalies in test)\n",
    "y_train = np.zeros(len(X_train))  # All training data is normal\n",
    "y_test = np.zeros(len(X_test))    # Test labels (normally would have true labels)\n",
    "\n",
    "# Combine for analysis\n",
    "y_true = np.hstack([y_train, y_test])\n",
    "y_pred = anomaly_labels\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"Confusion Matrix Analysis:\")\n",
    "print(f\"\\n{cm}\")\n",
    "print(f\"\\nTrue Negatives (TN): {tn:,}\")\n",
    "print(f\"False Positives (FP): {fp:,}\")\n",
    "print(f\"False Negatives (FN): {fn:,}\")\n",
    "print(f\"True Positives (TP): {tp:,}\")\n",
    "\n",
    "# Calculate metrics\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "f1 = 2 * (precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Try to calculate ROC-AUC with continuous scores\n",
    "try:\n",
    "    roc_auc = roc_auc_score(y_true, all_reconstruction_errors)\n",
    "    print(f\"  ROC-AUC Score: {roc_auc:.4f}\")\n",
    "except:\n",
    "    print(\"  ROC-AUC: Unable to compute (possibly all samples in one class)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59c0f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Confusion Matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion Matrix Heatmap\n",
    "ax1 = axes[0]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, ax=ax1,\n",
    "            xticklabels=['Normal', 'Anomaly'],\n",
    "            yticklabels=['Normal', 'Anomaly'])\n",
    "ax1.set_ylabel('True Label', fontsize=11, fontweight='bold')\n",
    "ax1.set_xlabel('Predicted Label', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Metrics Comparison\n",
    "ax2 = axes[1]\n",
    "metrics = ['Sensitivity', 'Specificity', 'Precision', 'F1-Score']\n",
    "values = [sensitivity, specificity, precision, f1]\n",
    "colors_bar = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "bars = ax2.bar(metrics, values, color=colors_bar, edgecolor='black', linewidth=1.5)\n",
    "ax2.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Performance Metrics', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylim([0, 1.1])\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668cee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve (if enough variation in classes)\n",
    "try:\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, all_reconstruction_errors)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.plot(fpr, tpr, color='darkorange', lw=2.5, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "    ax.set_xlim([-0.01, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('ROC Curve - Autoencoder Anomaly Detection', fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc=\"lower right\", fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"ROC curve generation note: {str(e)[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31d1dbe",
   "metadata": {},
   "source": [
    "## 10. Summary & Outputs\n",
    "\n",
    "Save model and anomaly detection results for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c649691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_path = 'cms_autoencoder_model'\n",
    "autoencoder.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save scaler for future inference\n",
    "import pickle\n",
    "scaler_path = 'feature_scaler.pkl'\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"Feature scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Create results summary\n",
    "results_summary = pd.DataFrame({\n",
    "    'Metric': ['Total Records', 'Anomalies Detected', 'Anomaly Percentage', \n",
    "               'Anomaly Threshold', 'Mean Anomaly Score', 'Training Time (sec)',\n",
    "               'Epochs Trained', 'Final Training Loss', 'Final Validation Loss'],\n",
    "    'Value': [len(anomaly_labels), anomaly_count, f'{anomaly_percentage:.2f}%',\n",
    "              f'{threshold:.6f}', f'{all_reconstruction_errors.mean():.6f}',\n",
    "              f'{training_time:.2f}', len(history.history['loss']),\n",
    "              f'{train_final_loss:.6f}', f'{val_final_loss:.6f}']\n",
    "})\n",
    "\n",
    "print(\"\\nExecution Summary:\")\n",
    "print(results_summary.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
