{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f81b16",
   "metadata": {},
   "source": [
    "# Autoencoder-Based Anomaly Detection for CMS Open Payments\n",
    "\n",
    "**Project:** AAI-540 Machine Learning Operations - Final Team Project  \n",
    "**Context:** Continuation of notebook 03 - Feature Engineering & Model Preparation  \n",
    "**Objective:** Train an Autoencoder model to detect anomalous payment patterns using reconstruction error as anomaly score\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup & Data Loading](#setup)\n",
    "2. [Load Data from Stored Variables](#loading)\n",
    "3. [Data Preparation & Normalization](#preparation)\n",
    "4. [Autoencoder Architecture Design](#architecture)\n",
    "5. [Model Training with Early Stopping](#training)\n",
    "6. [Performance Evaluation](#evaluation)\n",
    "7. [Anomaly Score Calculation](#scoring)\n",
    "8. [Visualizations & Metrics](#visualizations)\n",
    "9. [Confusion Matrix & ROC Analysis](#confusion)\n",
    "10. [Summary & Outputs](#summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b3046e",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading\n",
    "\n",
    "Load dependencies and restore configuration from notebook 03 (Feature Engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49056c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "import boto3\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609baa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r cms_payments_feature_group_name\n",
    "%store -r record_identifier_feature_name\n",
    "%store -r region\n",
    "%store -r bucket\n",
    "%store -r s3_athena_staging\n",
    "%store -r database_name\n",
    "%store -r table_name_parquet\n",
    "%store -r offline_s3_uri\n",
    "%store -r df\n",
    "\n",
    "if 'df' not in dir() or df is None:\n",
    "    raise NameError(\"Missing required variable 'df'. Run notebook 03 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5850b043",
   "metadata": {},
   "source": [
    "## 2. Load Data from Stored Variables\n",
    "\n",
    "Use the engineered dataset from notebook 03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e4cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_payments = df.copy()\n",
    "display(df_payments.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4213e5c9",
   "metadata": {},
   "source": [
    "## 3. Data Preparation & Normalization\n",
    "\n",
    "Prepare features for Autoencoder training with appropriate scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e5a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df_payments.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "cols_to_exclude = [\n",
    "    'EventTime', 'covered_recipient_profile_id', 'index',\n",
    "    'teaching_hospital_id', 'covered_recipient_npi',\n",
    "    'covered_recipient_first_name', 'covered_recipient_middle_name',\n",
    "    'covered_recipient_last_name', 'covered_recipient_name_suffix',\n",
    "    'recipient_primary_business_street_address_line2',\n",
    "    'recipient_zip_code', 'recipient_province', 'recipient_postal_code',\n",
    "    'submitting_applicable_manufacturer_or_applicable_gpo_name',\n",
    "    'applicable_manufacturer_or_applicable_gpo_making_payment_id',\n",
    "    'applicable_manufacturer_or_applicable_gpo_making_payment_name'\n",
    "]\n",
    "\n",
    "numeric_features = [col for col in numeric_cols \n",
    "                   if col not in cols_to_exclude \n",
    "                   and not any(x in col.lower() for x in ['_id', '_name', '_address', '_code', '_province', '_postal'])]\n",
    "\n",
    "X = df_payments[numeric_features].copy().astype(float)\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "missing_pct = (X.isnull().sum() / len(X)) * 100\n",
    "cols_to_keep = missing_pct[missing_pct <= 50].index.tolist()\n",
    "X = X[cols_to_keep]\n",
    "\n",
    "for col in X.columns:\n",
    "    q1, q3 = X[col].quantile(0.25), X[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    X[col] = X[col].clip(lower=q1 - 3*iqr, upper=q3 + 3*iqr)\n",
    "\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "print(f\"Data prepared: {X.shape} | Range: [{X.min().min():.2f}, {X.max().max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022aded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "print(f\"Scaled: {X_scaled.shape} | Range: [{X_scaled.min().min():.4f}, {X_scaled.max().max():.4f}]\")\n",
    "\n",
    "X_train, X_test = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9af3c9",
   "metadata": {},
   "source": [
    "## 4. Autoencoder Architecture Design\n",
    "\n",
    "Design a deep Autoencoder with bottleneck layer for feature compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccbaf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_scaled.shape[1]\n",
    "encoding_dim_1 = max(input_dim // 2, 32)\n",
    "encoding_dim_2 = max(input_dim // 4, 16)\n",
    "bottleneck_dim = max(input_dim // 8, 8)\n",
    "\n",
    "print(f\"Architecture: {input_dim} -> {encoding_dim_1} -> {encoding_dim_2} -> {bottleneck_dim} (bottleneck)\")\n",
    "\n",
    "autoencoder = Sequential([\n",
    "    layers.Dense(encoding_dim_1, activation='relu', input_shape=(input_dim,), name='encoder_input'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(encoding_dim_2, activation='relu', name='encoder_middle'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(bottleneck_dim, activation='relu', name='bottleneck'),\n",
    "    layers.Dense(encoding_dim_2, activation='relu', name='decoder_middle'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(encoding_dim_1, activation='relu', name='decoder_layer'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(input_dim, activation='sigmoid', name='decoder_output')\n",
    "], name='Autoencoder')\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8da64b0",
   "metadata": {},
   "source": [
    "## 5. Model Training with Early Stopping\n",
    "\n",
    "Train the Autoencoder with optimal configuration and early stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b01867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model with Adam optimizer\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "autoencoder.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f9cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=0,\n",
    "    min_delta=1e-5\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2097cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "history = autoencoder.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training: {len(history.history['loss'])} epochs in {training_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb1e5b",
   "metadata": {},
   "source": [
    "## 6. Performance Evaluation\n",
    "\n",
    "Evaluate model performance on training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb96065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = autoencoder.predict(X_train, verbose=0)\n",
    "train_mse = np.mean(np.square(X_train - train_predictions), axis=1)\n",
    "\n",
    "test_predictions = autoencoder.predict(X_test, verbose=0)\n",
    "test_mse = np.mean(np.square(X_test - test_predictions), axis=1)\n",
    "\n",
    "train_final_loss = history.history['loss'][-1]\n",
    "val_final_loss = history.history['val_loss'][-1]\n",
    "\n",
    "print(f\"MSE - Train: {train_mse.mean():.6f} | Test: {test_mse.mean():.6f}\")\n",
    "print(f\"Loss - Train: {train_final_loss:.6f} | Val: {val_final_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ea4d7",
   "metadata": {},
   "source": [
    "## 7. Anomaly Score Calculation\n",
    "\n",
    "Generate anomaly scores and identify outliers using reconstruction error threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c9c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.vstack([X_train, X_test])\n",
    "all_predictions = autoencoder.predict(all_data, verbose=0)\n",
    "all_reconstruction_errors = np.mean(np.square(all_data - all_predictions), axis=1)\n",
    "\n",
    "threshold_percentile = 95\n",
    "threshold = np.percentile(train_mse, threshold_percentile)\n",
    "\n",
    "anomaly_labels = (all_reconstruction_errors > threshold).astype(int)\n",
    "anomaly_count = anomaly_labels.sum()\n",
    "anomaly_percentage = (anomaly_count / len(anomaly_labels)) * 100\n",
    "\n",
    "print(f\"Threshold (95th percentile): {threshold:.6f}\")\n",
    "print(f\"Anomalies: {anomaly_count:,}/{len(anomaly_labels):,} ({anomaly_percentage:.2f}%)\")\n",
    "print(f\"Score: Mean={all_reconstruction_errors.mean():.6f} | Median={np.median(all_reconstruction_errors):.6f} | Std={all_reconstruction_errors.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd360c0f",
   "metadata": {},
   "source": [
    "## 8. Visualizations & Metrics\n",
    "\n",
    "Visualize training history, loss distributions, and anomaly scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba1d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('Loss (MSE)', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Model Training History', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
    "ax2.plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('MAE', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Model Training MAE', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(train_mse, bins=50, alpha=0.7, label='Training', color='blue', edgecolor='black')\n",
    "ax3.hist(test_mse, bins=50, alpha=0.7, label='Test', color='red', edgecolor='black')\n",
    "ax3.axvline(threshold, color='green', linestyle='--', linewidth=2.5, label=f'Threshold')\n",
    "ax3.set_xlabel('Reconstruction Error (MSE)', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Reconstruction Error Distribution', fontsize=12, fontweight='bold')\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "ax4 = axes[1, 1]\n",
    "ax4.hist(all_reconstruction_errors, bins=100, alpha=0.8, color='purple', edgecolor='black')\n",
    "ax4.axvline(threshold, color='red', linestyle='--', linewidth=2.5, label='Threshold')\n",
    "ax4.axvline(all_reconstruction_errors.mean(), color='orange', linestyle=':', linewidth=2.5, label='Mean')\n",
    "ax4.set_xlabel('Anomaly Score', fontsize=11, fontweight='bold')\n",
    "ax4.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "ax4.set_title('Anomaly Score Distribution', fontsize=12, fontweight='bold')\n",
    "ax4.legend(fontsize=10)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('autoencoder_training_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b1ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "percentiles = np.arange(1, 101)\n",
    "ax.plot(percentiles, np.percentile(all_reconstruction_errors, percentiles), \n",
    "        linewidth=2.5, color='darkblue', marker='o', markersize=3)\n",
    "ax.axhline(y=threshold, color='red', linestyle='--', linewidth=2.5, \n",
    "           label=f'95th Percentile: {threshold:.6f}')\n",
    "ax.fill_between(percentiles, 0, np.percentile(all_reconstruction_errors, percentiles), \n",
    "                alpha=0.2, color='blue')\n",
    "ax.set_xlabel('Percentile', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Anomaly Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Anomaly Score Percentile Distribution', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f036150",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrix & ROC Analysis\n",
    "\n",
    "Analyze model performance with statistical metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c571c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.zeros(len(X_train))\n",
    "y_test = np.zeros(len(X_test))\n",
    "\n",
    "y_true = np.hstack([y_train, y_test])\n",
    "y_pred = anomaly_labels\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "f1 = 2 * (precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0\n",
    "\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "print(f\"\\nMetrics: Sensitivity={sensitivity:.4f} | Specificity={specificity:.4f} | Precision={precision:.4f} | F1={f1:.4f}\")\n",
    "\n",
    "try:\n",
    "    roc_auc = roc_auc_score(y_true, all_reconstruction_errors)\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "except:\n",
    "    print(\"ROC-AUC: Unable to compute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59c0f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, ax=ax1,\n",
    "            xticklabels=['Normal', 'Anomaly'],\n",
    "            yticklabels=['Normal', 'Anomaly'])\n",
    "ax1.set_ylabel('True Label', fontsize=11, fontweight='bold')\n",
    "ax1.set_xlabel('Predicted Label', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax2 = axes[1]\n",
    "metrics = ['Sensitivity', 'Specificity', 'Precision', 'F1-Score']\n",
    "values = [sensitivity, specificity, precision, f1]\n",
    "colors_bar = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "bars = ax2.bar(metrics, values, color=colors_bar, edgecolor='black', linewidth=1.5)\n",
    "ax2.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Performance Metrics', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylim([0, 1.1])\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, val in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668cee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, all_reconstruction_errors)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.plot(fpr, tpr, color='darkorange', lw=2.5, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "    ax.set_xlim([-0.01, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('ROC Curve - Autoencoder Anomaly Detection', fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc=\"lower right\", fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"ROC curve: {str(e)[:80]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31d1dbe",
   "metadata": {},
   "source": [
    "## 10. Summary & Outputs\n",
    "\n",
    "Save model and anomaly detection results for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c649691",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'cms_autoencoder_model'\n",
    "autoencoder.save(model_path)\n",
    "print(f\"Model saved: {model_path}\")\n",
    "\n",
    "import pickle\n",
    "scaler_path = 'feature_scaler.pkl'\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"Scaler saved: {scaler_path}\")\n",
    "\n",
    "results_summary = pd.DataFrame({\n",
    "    'Metric': ['Total Records', 'Anomalies Detected', 'Anomaly Percentage', \n",
    "               'Threshold', 'Mean Score', 'Training Time (sec)',\n",
    "               'Epochs', 'Final Train Loss', 'Final Val Loss'],\n",
    "    'Value': [len(anomaly_labels), anomaly_count, f'{anomaly_percentage:.2f}%',\n",
    "              f'{threshold:.6f}', f'{all_reconstruction_errors.mean():.6f}',\n",
    "              f'{training_time:.2f}', len(history.history['loss']),\n",
    "              f'{train_final_loss:.6f}', f'{val_final_loss:.6f}']\n",
    "})\n",
    "\n",
    "print(\"\\nExecution Summary:\")\n",
    "display(results_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
