{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38cacf92",
   "metadata": {},
   "source": [
    "# CMS Open Payments Datalake Setup\n",
    "\n",
    "**Project:** AAI-540 Machine Learning Operations - Final Team Project  \n",
    "**Purpose:** Setup AWS S3 Datalake for CMS Open Payments Data  \n",
    "**Dataset:** CMS Open Payments Program Year 2024\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Environment Setup](#setup)\n",
    "2. [AWS Configuration & S3 Bucket Creation](#aws-config)\n",
    "3. [Download CMS Open Payments Data](#download)\n",
    "4. [Upload Data to S3](#upload)\n",
    "5. [Create Athena Database](#athena)\n",
    "6. [Register Data with Athena](#register)\n",
    "7. [Convert CSV to Parquet](#parquet)\n",
    "8. [Query Data with AWS Data Wrangler](#query)\n",
    "9. [Validation & Verification](#validation)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c0f2a8",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Install and import necessary libraries for AWS integration and data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47384732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required AWS packages\n",
    "!pip install boto3 sagemaker awswrangler pyathena --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7a2337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from io import BytesIO, StringIO\n",
    "import awswrangler as wr\n",
    "from pyathena import connect\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c36822",
   "metadata": {},
   "source": [
    "## 2. AWS Configuration & S3 Bucket Creation\n",
    "\n",
    "Configure AWS session and create S3 bucket for the datalake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c2bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AWS session and SageMaker\n",
    "sess = sagemaker.Session()\n",
    "bucket = \"cmsopenpaymentsystems\"\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "\n",
    "# Initialize AWS clients\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "print(f\"AWS Configuration:\")\n",
    "print(f\"  Region: {region}\")\n",
    "print(f\"  Account ID: {account_id}\")\n",
    "print(f\"  S3 Bucket: {bucket}\")\n",
    "print(f\"  Role: {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacede33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define S3 paths for CMS data\n",
    "cms_data_prefix = \"cms-open-payments\"\n",
    "raw_data_prefix = f\"{cms_data_prefix}/raw\"\n",
    "processed_data_prefix = f\"{cms_data_prefix}/processed\"\n",
    "parquet_data_prefix = f\"{cms_data_prefix}/parquet\"\n",
    "\n",
    "s3_raw_path = f\"s3://{bucket}/{raw_data_prefix}\"\n",
    "s3_processed_path = f\"s3://{bucket}/{processed_data_prefix}\"\n",
    "s3_parquet_path = f\"s3://{bucket}/{parquet_data_prefix}\"\n",
    "\n",
    "print(f\"S3 Data Paths:\")\n",
    "print(f\"  Raw Data: {s3_raw_path}\")\n",
    "print(f\"  Processed Data: {s3_processed_path}\")\n",
    "print(f\"  Parquet Data: {s3_parquet_path}\")\n",
    "\n",
    "# Store paths for use in other notebooks\n",
    "%store bucket\n",
    "%store region\n",
    "%store s3_raw_path\n",
    "%store s3_processed_path\n",
    "%store s3_parquet_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c3ff86",
   "metadata": {},
   "source": [
    "## 3. Download CMS Open Payments Data\n",
    "\n",
    "Download the CMS Open Payments Program Year 2024 General Payments dataset.\n",
    "\n",
    "**Data Source:** CMS Open Payments  \n",
    "**Dataset:** Program Year 2024 General Payments  \n",
    "**Published:** June 30, 2025  \n",
    "**Coverage:** January 1, 2024 - December 31, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baf9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CMS Open Payments data URL - Direct CSV download\n",
    "cms_data_url = \"https://download.cms.gov/openpayments/PGYR2024_P06302025_06162025/OP_DTL_GNRL_PGYR2024_P06302025_06162025.csv\"\n",
    "\n",
    "# Alternative: If the above URL doesn't work, use this approach:\n",
    "# 1. Go to https://openpaymentsdata.cms.gov/datasets\n",
    "# 2. Select \"Program Year 2024\" and \"General Payments\"\n",
    "# 3. Download the CSV file manually and place it in ../data/ directory\n",
    "\n",
    "print(f\"CMS Data URL: {cms_data_url}\")\n",
    "print(f\"\\nNote: This dataset is approximately 3-4 GB.\")\n",
    "print(f\"Download may take several minutes depending on your connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c50632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local data directory if it doesn't exist\n",
    "local_data_dir = Path(\"../data\")\n",
    "local_data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Local CSV file path\n",
    "local_csv_file = local_data_dir / \"OP_DTL_GNRL_PGYR2024_P06302025_06162025.csv\"\n",
    "\n",
    "print(f\"Local data directory: {local_data_dir.absolute()}\")\n",
    "print(f\"Target CSV file: {local_csv_file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07991ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CMS data if not already present\n",
    "if local_csv_file.exists():\n",
    "    print(f\"CSV file already exists: {local_csv_file}\")\n",
    "    print(f\"  File size: {local_csv_file.stat().st_size / (1024**3):.2f} GB\")\n",
    "else:\n",
    "    print(f\"Downloading CMS Open Payments data...\")\n",
    "    print(f\"This may take 10-20 minutes depending on your connection.\")\n",
    "    \n",
    "    try:\n",
    "        # Download CSV file with progress indication\n",
    "        response = requests.get(cms_data_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        print(f\"Total download size: {total_size / (1024**3):.2f} GB\")\n",
    "        \n",
    "        # Save CSV file directly\n",
    "        with open(local_csv_file, 'wb') as f:\n",
    "            downloaded = 0\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    downloaded += len(chunk)\n",
    "                    if total_size > 0:\n",
    "                        percent = (downloaded / total_size) * 100\n",
    "                        print(f\"\\rProgress: {percent:.1f}%\", end=\"\")\n",
    "        \n",
    "        print(f\"\\nDownload complete: {local_csv_file}\")\n",
    "        print(f\"  File size: {local_csv_file.stat().st_size / (1024**3):.2f} GB\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError downloading data: {e}\")\n",
    "        print(f\"\\nAlternative approach:\")\n",
    "        print(f\"1. Visit: https://openpaymentsdata.cms.gov/datasets\")\n",
    "        print(f\"2. Select 'Program Year 2024' and 'General Payments'\")\n",
    "        print(f\"3. Download CSV and save to: {local_data_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c67f2b8",
   "metadata": {},
   "source": [
    "## 4. Upload Data to S3 upload\n",
    "\n",
    "Upload the downloaded CMS data to S3 for datalake storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905d1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data before upload\n",
    "print(\"Loading sample of data for preview...\")\n",
    "df_sample = pd.read_csv(local_csv_file, nrows=5)\n",
    "\n",
    "print(f\"\\nDataset Preview:\")\n",
    "print(f\"  Columns: {len(df_sample.columns)}\")\n",
    "print(f\"  Sample rows:\")\n",
    "display(df_sample.head())\n",
    "\n",
    "print(f\"\\nColumn names:\")\n",
    "for i, col in enumerate(df_sample.columns, 1):\n",
    "    print(f\"  {i}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753fa4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload raw CSV to S3\n",
    "print(f\"Uploading data to S3...\")\n",
    "print(f\"  Source: {local_csv_file}\")\n",
    "print(f\"  Destination: {s3_raw_path}/\")\n",
    "\n",
    "s3_raw_file_path = f\"{s3_raw_path}/{local_csv_file.name}\"\n",
    "\n",
    "try:\n",
    "    # Upload file with progress callback\n",
    "    file_size = local_csv_file.stat().st_size\n",
    "    \n",
    "    def upload_progress(bytes_uploaded):\n",
    "        percent = (bytes_uploaded / file_size) * 100\n",
    "        print(f\"\\rUpload progress: {percent:.1f}%\", end=\"\")\n",
    "    \n",
    "    s3_client.upload_file(\n",
    "        str(local_csv_file),\n",
    "        bucket,\n",
    "        f\"{raw_data_prefix}/{local_csv_file.name}\",\n",
    "        Callback=upload_progress\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nUpload complete\")\n",
    "    print(f\"  S3 URI: {s3_raw_file_path}\")\n",
    "    \n",
    "    # Store the S3 file path\n",
    "    %store s3_raw_file_path\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError uploading to S3: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fbe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify upload\n",
    "print(\"Verifying S3 upload...\")\n",
    "\n",
    "response = s3_client.list_objects_v2(\n",
    "    Bucket=bucket,\n",
    "    Prefix=raw_data_prefix\n",
    ")\n",
    "\n",
    "if 'Contents' in response:\n",
    "    print(f\"\\nFiles in S3 bucket:\")\n",
    "    for obj in response['Contents']:\n",
    "        size_gb = obj['Size'] / (1024**3)\n",
    "        print(f\"  {obj['Key']} ({size_gb:.2f} GB)\")\n",
    "else:\n",
    "    print(f\"\\nNo files found in S3 bucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667e90c0",
   "metadata": {},
   "source": [
    "## 5. Create Athena Database\n",
    "\n",
    "Create an Amazon Athena database for querying CMS data using SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9701a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Athena database name\n",
    "database_name = \"cms_open_payments\"\n",
    "\n",
    "# Set S3 staging directory for Athena queries\n",
    "s3_athena_staging = f\"s3://{bucket}/athena/staging\"\n",
    "\n",
    "print(f\"Athena Configuration:\")\n",
    "print(f\"  Database: {database_name}\")\n",
    "print(f\"  Staging Directory: {s3_athena_staging}\")\n",
    "\n",
    "# Store for use in other notebooks\n",
    "%store database_name\n",
    "%store s3_athena_staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf19b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Athena connection\n",
    "athena_conn = connect(\n",
    "    region_name=region,\n",
    "    s3_staging_dir=s3_athena_staging\n",
    ")\n",
    "\n",
    "print(\"Athena connection established\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database\n",
    "create_db_query = f\"CREATE DATABASE IF NOT EXISTS {database_name}\"\n",
    "\n",
    "print(f\"Creating Athena database...\")\n",
    "print(f\"  Query: {create_db_query}\")\n",
    "\n",
    "try:\n",
    "    result = pd.read_sql(create_db_query, athena_conn)\n",
    "    print(f\"Database created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating database: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703a47ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify database creation\n",
    "show_db_query = \"SHOW DATABASES\"\n",
    "\n",
    "print(\"Verifying database creation...\")\n",
    "databases = pd.read_sql(show_db_query, athena_conn)\n",
    "\n",
    "print(f\"\\n Available Databases:\")\n",
    "display(databases)\n",
    "\n",
    "if database_name in databases.values:\n",
    "    print(f\"\\nDatabase '{database_name}' exists\")\n",
    "else:\n",
    "    print(f\"\\nDatabase '{database_name}' not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c0345",
   "metadata": {},
   "source": [
    "## 6. Register Data with Athena\n",
    "\n",
    "Create an external table in Athena to query the CSV data stored in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552735e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table name\n",
    "table_name_csv = \"general_payments_csv\"\n",
    "\n",
    "print(f\"Table Configuration:\")\n",
    "print(f\"  Database: {database_name}\")\n",
    "print(f\"  Table: {table_name_csv}\")\n",
    "print(f\"  Location: {s3_raw_path}/\")\n",
    "\n",
    "%store table_name_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3e2638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get actual column names from the CSV\n",
    "df_schema = pd.read_csv(local_csv_file, nrows=1)\n",
    "\n",
    "# Create column definitions for Athena\n",
    "# Map pandas dtypes to Athena types\n",
    "def get_athena_type(dtype):\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return 'BIGINT'\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return 'DOUBLE'\n",
    "    elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "        return 'TIMESTAMP'\n",
    "    else:\n",
    "        return 'STRING'\n",
    "\n",
    "# Create column definitions\n",
    "columns_def = []\n",
    "for col in df_schema.columns:\n",
    "    # Clean column name for Athena (replace spaces and special chars)\n",
    "    clean_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_')\n",
    "    athena_type = get_athena_type(df_schema[col].dtype)\n",
    "    columns_def.append(f\"`{col}` {athena_type}\")\n",
    "\n",
    "columns_str = ',\\n    '.join(columns_def)\n",
    "\n",
    "print(f\"Schema preview (first 10 columns):\")\n",
    "for i, col_def in enumerate(columns_def[:10], 1):\n",
    "    print(f\"  {i}. {col_def}\")\n",
    "print(f\"  ... ({len(columns_def)} columns total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd87b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create external table for CSV data\n",
    "create_table_query = f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {database_name}.{table_name_csv} (\n",
    "    {columns_str}\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\\\n'\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '{s3_raw_path}/'\n",
    "TBLPROPERTIES (\n",
    "    'skip.header.line.count'='1',\n",
    "    'serialization.null.format'=''\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Creating external table...\")\n",
    "print(f\"\\nQuery preview:\")\n",
    "print(create_table_query[:500] + \"...\")\n",
    "\n",
    "try:\n",
    "    result = pd.read_sql(create_table_query, athena_conn)\n",
    "    print(f\"\\nTable '{table_name_csv}' created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError creating table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3435350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify table creation\n",
    "show_tables_query = f\"SHOW TABLES IN {database_name}\"\n",
    "\n",
    "print(\"Verifying table creation...\")\n",
    "tables = pd.read_sql(show_tables_query, athena_conn)\n",
    "\n",
    "print(f\"\\nTables in database '{database_name}':\")\n",
    "display(tables)\n",
    "\n",
    "if table_name_csv in tables.values:\n",
    "    print(f\"\\nTable '{table_name_csv}' exists\")\n",
    "else:\n",
    "    print(f\"\\nTable '{table_name_csv}' not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0c229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query - count rows\n",
    "count_query = f\"\"\"\n",
    "SELECT COUNT(*) as row_count\n",
    "FROM {database_name}.{table_name_csv}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Testing table access...\")\n",
    "print(f\"Query: {count_query}\")\n",
    "\n",
    "try:\n",
    "    result = pd.read_sql(count_query, athena_conn)\n",
    "    print(f\"\\nQuery successful\")\n",
    "    print(f\"  Total rows: {result['row_count'][0]:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError querying table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e153c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample query - preview data\n",
    "sample_query = f\"\"\"\n",
    "SELECT *\n",
    "FROM {database_name}.{table_name_csv}\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "print(\"Fetching sample data...\")\n",
    "\n",
    "try:\n",
    "    sample_data = pd.read_sql(sample_query, athena_conn)\n",
    "    print(f\"\\nSample data retrieved\")\n",
    "    print(f\"  Shape: {sample_data.shape}\")\n",
    "    display(sample_data.head())\n",
    "except Exception as e:\n",
    "    print(f\"\\nError fetching sample data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d3faaa",
   "metadata": {},
   "source": [
    "## 7. Convert CSV to Parquet\n",
    "\n",
    "Convert the CSV data to Parquet format for better performance and compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Parquet table name\n",
    "table_name_parquet = \"general_payments_parquet\"\n",
    "\n",
    "print(f\"Parquet Conversion Configuration:\")\n",
    "print(f\"  Source Table: {database_name}.{table_name_csv}\")\n",
    "print(f\"  Target Table: {database_name}.{table_name_parquet}\")\n",
    "print(f\"  Target Location: {s3_parquet_path}/\")\n",
    "\n",
    "%store table_name_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d0889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CTAS (Create Table As Select) query to convert CSV to Parquet\n",
    "create_parquet_query = f\"\"\"\n",
    "CREATE TABLE {database_name}.{table_name_parquet}\n",
    "WITH (\n",
    "    format = 'PARQUET',\n",
    "    parquet_compression = 'SNAPPY',\n",
    "    external_location = '{s3_parquet_path}/',\n",
    "    partitioned_by = ARRAY['program_year']\n",
    ")\n",
    "AS\n",
    "SELECT \n",
    "    *,\n",
    "    '2024' as program_year\n",
    "FROM {database_name}.{table_name_csv}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Converting CSV to Parquet format...\")\n",
    "print(\"Note: This operation may take 15-30 minutes for large datasets\")\n",
    "print(f\"\\nQuery:\")\n",
    "print(create_parquet_query)\n",
    "\n",
    "try:\n",
    "    # Execute conversion\n",
    "    result = pd.read_sql(create_parquet_query, athena_conn)\n",
    "    print(f\"\\nConversion complete\")\n",
    "    print(f\"  Parquet table '{table_name_parquet}' created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during conversion: {e}\")\n",
    "    print(f\"\\nNote: If table already exists, drop it first:\")\n",
    "    print(f\"  DROP TABLE IF EXISTS {database_name}.{table_name_parquet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c7afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Parquet table\n",
    "count_parquet_query = f\"\"\"\n",
    "SELECT COUNT(*) as row_count\n",
    "FROM {database_name}.{table_name_parquet}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Verifying Parquet table...\")\n",
    "\n",
    "try:\n",
    "    result = pd.read_sql(count_parquet_query, athena_conn)\n",
    "    print(f\"\\nParquet table verified\")\n",
    "    print(f\"  Total rows: {result['row_count'][0]:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError verifying Parquet table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eaa880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare file sizes\n",
    "print(\"Comparing CSV vs Parquet storage:\")\n",
    "\n",
    "# Get CSV size\n",
    "csv_objects = s3_client.list_objects_v2(\n",
    "    Bucket=bucket,\n",
    "    Prefix=raw_data_prefix\n",
    ")\n",
    "\n",
    "csv_size = sum(obj['Size'] for obj in csv_objects.get('Contents', []))\n",
    "\n",
    "# Get Parquet size\n",
    "parquet_objects = s3_client.list_objects_v2(\n",
    "    Bucket=bucket,\n",
    "    Prefix=parquet_data_prefix\n",
    ")\n",
    "\n",
    "parquet_size = sum(obj['Size'] for obj in parquet_objects.get('Contents', []))\n",
    "\n",
    "print(f\"\\nStorage Comparison:\")\n",
    "print(f\"  CSV Size: {csv_size / (1024**3):.2f} GB\")\n",
    "print(f\"  Parquet Size: {parquet_size / (1024**3):.2f} GB\")\n",
    "if parquet_size > 0:\n",
    "    compression_ratio = (1 - parquet_size/csv_size) * 100\n",
    "    print(f\"  Compression: {compression_ratio:.1f}% reduction\")\n",
    "    print(f\"  Space Saved: {(csv_size - parquet_size) / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6925b16",
   "metadata": {},
   "source": [
    "## 8. Query Data with AWS Data Wrangler\n",
    "\n",
    "Use AWS Data Wrangler for more efficient data querying and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e7d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query using AWS Data Wrangler\n",
    "sample_query_wr = f\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_payments,\n",
    "    SUM(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as total_amount,\n",
    "    AVG(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as avg_amount,\n",
    "    MIN(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as min_amount,\n",
    "    MAX(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as max_amount\n",
    "FROM {database_name}.{table_name_parquet}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Querying payment statistics with AWS Data Wrangler...\")\n",
    "print(f\"\\nQuery: {sample_query_wr}\")\n",
    "\n",
    "try:\n",
    "    df_stats = wr.athena.read_sql_query(\n",
    "        sql=sample_query_wr,\n",
    "        database=database_name,\n",
    "        ctas_approach=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nQuery successful\")\n",
    "    print(f\"\\nPayment Statistics:\")\n",
    "    display(df_stats)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError querying data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data by recipient type\n",
    "recipient_query = f\"\"\"\n",
    "SELECT \n",
    "    Covered_Recipient_Type,\n",
    "    COUNT(*) as payment_count,\n",
    "    SUM(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as total_amount\n",
    "FROM {database_name}.{table_name_parquet}\n",
    "GROUP BY Covered_Recipient_Type\n",
    "ORDER BY total_amount DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"Analyzing payments by recipient type...\")\n",
    "\n",
    "try:\n",
    "    df_recipients = wr.athena.read_sql_query(\n",
    "        sql=recipient_query,\n",
    "        database=database_name,\n",
    "        ctas_approach=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nQuery successful\")\n",
    "    print(f\"\\nPayments by Recipient Type:\")\n",
    "    display(df_recipients)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError querying data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf9fb6",
   "metadata": {},
   "source": [
    "## 9. Validation & Verification\n",
    "\n",
    "Perform final validation checks on the datalake setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae064097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive validation\n",
    "print(\"=\" * 70)\n",
    "print(\"DATALAKE SETUP VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "validation_passed = True\n",
    "\n",
    "# Check 1: S3 Buckets\n",
    "print(\"\\n1. S3 Storage:\")\n",
    "try:\n",
    "    for prefix in [raw_data_prefix, parquet_data_prefix]:\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=1)\n",
    "        if 'Contents' in response:\n",
    "            print(f\"   [OK] {prefix}/\")\n",
    "        else:\n",
    "            print(f\"   [FAIL] {prefix}/ (empty or missing)\")\n",
    "            validation_passed = False\n",
    "except Exception as e:\n",
    "    print(f\"   [FAIL] Error checking S3: {e}\")\n",
    "    validation_passed = False\n",
    "\n",
    "# Check 2: Athena Database\n",
    "print(\"\\n2. Athena Database:\")\n",
    "try:\n",
    "    databases = pd.read_sql(\"SHOW DATABASES\", athena_conn)\n",
    "    if database_name in databases.values:\n",
    "        print(f\"   [OK] Database '{database_name}' exists\")\n",
    "    else:\n",
    "        print(f\"   [FAIL] Database '{database_name}' not found\")\n",
    "        validation_passed = False\n",
    "except Exception as e:\n",
    "    print(f\"   [FAIL] Error checking database: {e}\")\n",
    "    validation_passed = False\n",
    "\n",
    "# Check 3: Tables\n",
    "print(\"\\n3. Athena Tables:\")\n",
    "try:\n",
    "    tables = pd.read_sql(f\"SHOW TABLES IN {database_name}\", athena_conn)\n",
    "    for table in [table_name_csv, table_name_parquet]:\n",
    "        if table in tables.values:\n",
    "            print(f\"   [OK] Table '{table}' exists\")\n",
    "        else:\n",
    "            print(f\"   [FAIL] Table '{table}' not found\")\n",
    "            validation_passed = False\n",
    "except Exception as e:\n",
    "    print(f\"   [FAIL] Error checking tables: {e}\")\n",
    "    validation_passed = False\n",
    "\n",
    "# Check 4: Data Accessibility\n",
    "print(\"\\n4. Data Accessibility:\")\n",
    "try:\n",
    "    count_result = pd.read_sql(\n",
    "        f\"SELECT COUNT(*) as cnt FROM {database_name}.{table_name_parquet}\",\n",
    "        athena_conn\n",
    "    )\n",
    "    row_count = count_result['cnt'][0]\n",
    "    print(f\"   [OK] Query successful ({row_count:,} rows)\")\n",
    "except Exception as e:\n",
    "    print(f\"   [FAIL] Error querying data: {e}\")\n",
    "    validation_passed = False\n",
    "\n",
    "# Final result\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "if validation_passed:\n",
    "    print(\"ALL VALIDATION CHECKS PASSED\")\n",
    "    print(\"Datalake setup complete and operational\")\n",
    "    setup_datalake_passed = True\n",
    "else:\n",
    "    print(\"SOME VALIDATION CHECKS FAILED\")\n",
    "    print(\"Please review the errors above and re-run failed steps\")\n",
    "    setup_datalake_passed = False\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Store validation result\n",
    "%store setup_datalake_passed"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
