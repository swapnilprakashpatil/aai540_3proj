{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38cacf92",
   "metadata": {},
   "source": [
    "# CMS Open Payments Datalake Setup\n",
    "\n",
    "**Project:** AAI-540 Machine Learning Operations - Final Team Project  \n",
    "**Purpose:** Setup AWS S3 Datalake for CMS Open Payments Data  \n",
    "**Dataset:** CMS Open Payments Program Year 2024\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Environment Setup](#setup)\n",
    "2. [AWS Configuration & S3 Bucket Creation](#aws-config)\n",
    "3. [Download CMS Open Payments Data](#download)\n",
    "4. [Upload Data to S3](#upload)\n",
    "5. [Create Athena Database](#athena)\n",
    "6. [Register Data with Athena](#register)\n",
    "7. [Convert CSV to Parquet](#parquet)\n",
    "8. [Query Data with AWS Data Wrangler](#query)\n",
    "9. [Validation & Verification](#validation)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c0f2a8",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Install and import necessary libraries for AWS integration and data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47384732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.12/site-packages (1.37.3)\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.12/site-packages (2.245.0)\n",
      "Requirement already satisfied: awswrangler in /opt/conda/lib/python3.12/site-packages (3.14.0)\n",
      "Requirement already satisfied: pyathena in /opt/conda/lib/python3.12/site-packages (3.22.0)\n",
      "Requirement already satisfied: botocore<1.38.0,>=1.37.3 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.37.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.12/site-packages (from boto3) (0.11.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.3->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.3->boto3) (1.26.20)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.3->boto3) (1.17.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (23.2.0)\n",
      "Requirement already satisfied: cloudpickle>=2.2.1 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (3.1.2)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.12/site-packages (from sagemaker) (7.1.0)\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.12/site-packages (from sagemaker) (0.124.4)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.12/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: graphene<4,>=3 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (3.4.3)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (6.10.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.12/site-packages (from sagemaker) (4.23.0)\n",
      "Requirement already satisfied: numpy==1.26.4 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (1.26.4)\n",
      "Requirement already satisfied: omegaconf<3,>=2.2 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (2.3.0)\n",
      "Requirement already satisfied: packaging<25,>=23.0 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (24.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from sagemaker) (2.3.3)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.12/site-packages (from sagemaker) (0.3.4)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.12/site-packages (from sagemaker) (4.5.1)\n",
      "Requirement already satisfied: protobuf<6.0,>=3.12 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (5.28.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from sagemaker) (5.9.8)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (6.0.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from sagemaker) (2.32.5)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.17 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (1.0.67)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.12/site-packages (from sagemaker) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from sagemaker) (3.2.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from sagemaker) (4.67.1)\n",
      "Requirement already satisfied: uvicorn in /opt/conda/lib/python3.12/site-packages (from sagemaker) (0.38.0)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /opt/conda/lib/python3.12/site-packages (from graphene<4,>=3->sagemaker) (3.2.7)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /opt/conda/lib/python3.12/site-packages (from graphene<4,>=3->sagemaker) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7.1 in /opt/conda/lib/python3.12/site-packages (from graphene<4,>=3->sagemaker) (4.15.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.12/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.23.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /opt/conda/lib/python3.12/site-packages (from omegaconf<3,>=2.2->sagemaker) (4.9.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.12.5)\n",
      "Requirement already satisfied: rich<15.0.0,>=13.0.0 in /opt/conda/lib/python3.12/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (14.2.0)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /opt/conda/lib/python3.12/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.12/site-packages (from jsonschema->sagemaker) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.12/site-packages (from jsonschema->sagemaker) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from jsonschema->sagemaker) (0.30.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (0.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich<15.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich<15.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.19.2)\n",
      "Requirement already satisfied: pyarrow<22.0.0,>=8.0.0 in /opt/conda/lib/python3.12/site-packages (from awswrangler) (19.0.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from awswrangler) (80.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->sagemaker) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->sagemaker) (2025.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from pyathena) (2024.12.0)\n",
      "Requirement already satisfied: tenacity>=4.1.0 in /opt/conda/lib/python3.12/site-packages (from pyathena) (9.1.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->sagemaker) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->sagemaker) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->sagemaker) (2025.11.12)\n",
      "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /opt/conda/lib/python3.12/site-packages (from fastapi->sagemaker) (0.50.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /opt/conda/lib/python3.12/site-packages (from fastapi->sagemaker) (0.0.4)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /opt/conda/lib/python3.12/site-packages (from starlette<0.51.0,>=0.40.0->fastapi->sagemaker) (4.12.0)\n",
      "Requirement already satisfied: ppft>=1.7.7 in /opt/conda/lib/python3.12/site-packages (from pathos->sagemaker) (1.7.7)\n",
      "Requirement already satisfied: dill>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pathos->sagemaker) (0.4.0)\n",
      "Requirement already satisfied: pox>=0.3.6 in /opt/conda/lib/python3.12/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.18 in /opt/conda/lib/python3.12/site-packages (from pathos->sagemaker) (0.70.18)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.12/site-packages (from uvicorn->sagemaker) (8.3.1)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.12/site-packages (from uvicorn->sagemaker) (0.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required AWS packages\n",
    "%pip install boto3 sagemaker awswrangler pyathena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a7a2337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from io import BytesIO, StringIO\n",
    "import awswrangler as wr\n",
    "from pyathena import connect\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c36822",
   "metadata": {},
   "source": [
    "## 2. AWS Configuration & S3 Bucket Creation\n",
    "\n",
    "Configure AWS session and create S3 bucket for the datalake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b497665a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS Configuration:\n",
      "  Region: us-east-1\n",
      "  Account ID: 092122669768\n",
      "  S3 Bucket: cmsopenpaymentsystemslight\n",
      "  Role: arn:aws:iam::092122669768:role/LabRole\n",
      "Bucket already exists: cmsopenpaymentsystemslight\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize AWS session\n",
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "\n",
    "# Get account information\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity().get('Account')\n",
    "\n",
    "# Initialize AWS clients\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "# Dataset file name\n",
    "dataset_file_name = \"lightdataset.csv\" #\"OP_DTL_GNRL_PGYR2024_P06302025_06162025.csv\"\n",
    "\n",
    "# Define bucket name\n",
    "bucket = f\"cmsopenpaymentsystemslight\"\n",
    "cms_data_prefix = \"cms-open-payments-light\"\n",
    "\n",
    "# Define Athena database name\n",
    "database_name = \"cms_open_payments_light\"\n",
    "\n",
    "# Get IAM role (if needed)\n",
    "iam = boto3.client('iam')\n",
    "try:\n",
    "    role = iam.get_role(RoleName='LabRole')['Role']['Arn']\n",
    "except:\n",
    "    role = \"Role not found\"\n",
    "\n",
    "print(f\"AWS Configuration:\")\n",
    "print(f\"  Region: {region}\")\n",
    "print(f\"  Account ID: {account_id}\")\n",
    "print(f\"  S3 Bucket: {bucket}\")\n",
    "print(f\"  Role: {role}\")\n",
    "\n",
    "# Check if S3 bucket exists\n",
    "def check_bucket_exists(bucket_name):\n",
    "    \"\"\"Check if bucket exists and is accessible\"\"\"\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def ensure_bucket_exists(bucket_name, region):\n",
    "    \"\"\"Create bucket only if it doesn't exist\"\"\"\n",
    "    try:\n",
    "        if check_bucket_exists(bucket_name):\n",
    "            print(f\"Bucket already exists: {bucket_name}\")\n",
    "            return True\n",
    "        \n",
    "        # Create bucket if it doesn't exist\n",
    "        if region == 'us-east-1':\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "        print(f\"Created new bucket: {bucket_name}\")\n",
    "        return True\n",
    "    except s3_client.exceptions.BucketAlreadyOwnedByYou:\n",
    "        print(f\"Bucket already exists: {bucket_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error with bucket {bucket_name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Ensure bucket exists\n",
    "bucket_exists = ensure_bucket_exists(bucket, region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aacede33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Data Paths:\n",
      "  Raw Data: s3://cmsopenpaymentsystemslight/cms-open-payments-light/raw\n",
      "  Processed Data: s3://cmsopenpaymentsystemslight/cms-open-payments-light/processed\n",
      "  Parquet Data: s3://cmsopenpaymentsystemslight/cms-open-payments-light/parquet\n",
      "\n",
      "Data Status:\n",
      "  Raw Data in S3: EXISTS\n",
      "  Parquet Data in S3: EXISTS\n",
      "\n",
      "Data already ingested. You can skip to Step 5 (Create Athena Database)\n",
      "Stored 'bucket' (str)\n",
      "Stored 'region' (str)\n",
      "Stored 's3_raw_path' (str)\n",
      "Stored 's3_processed_path' (str)\n",
      "Stored 's3_parquet_path' (str)\n",
      "Stored 'raw_data_exists' (bool)\n",
      "Stored 'parquet_data_exists' (bool)\n",
      "Stored 'skip_ingestion' (bool)\n"
     ]
    }
   ],
   "source": [
    "# Define S3 paths for CMS data\n",
    "raw_data_prefix = f\"{cms_data_prefix}/raw\"\n",
    "processed_data_prefix = f\"{cms_data_prefix}/processed\"\n",
    "parquet_data_prefix = f\"{cms_data_prefix}/parquet\"\n",
    "\n",
    "s3_raw_path = f\"s3://{bucket}/{raw_data_prefix}\"\n",
    "s3_processed_path = f\"s3://{bucket}/{processed_data_prefix}\"\n",
    "s3_parquet_path = f\"s3://{bucket}/{parquet_data_prefix}\"\n",
    "\n",
    "print(f\"S3 Data Paths:\")\n",
    "print(f\"  Raw Data: {s3_raw_path}\")\n",
    "print(f\"  Processed Data: {s3_processed_path}\")\n",
    "print(f\"  Parquet Data: {s3_parquet_path}\")\n",
    "\n",
    "# Check if data already exists in S3\n",
    "def check_s3_data_exists(bucket_name, prefix):\n",
    "    \"\"\"Check if data exists in S3 path\"\"\"\n",
    "    try:\n",
    "        response = s3_client.list_objects_v2(\n",
    "            Bucket=bucket_name,\n",
    "            Prefix=prefix,\n",
    "            MaxKeys=1\n",
    "        )\n",
    "        return 'Contents' in response and len(response['Contents']) > 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking S3 path {prefix}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Check for existing data\n",
    "raw_data_exists = check_s3_data_exists(bucket, raw_data_prefix)\n",
    "parquet_data_exists = check_s3_data_exists(bucket, parquet_data_prefix)\n",
    "\n",
    "print(f\"\\nData Status:\")\n",
    "print(f\"  Raw Data in S3: {'EXISTS' if raw_data_exists else 'NOT FOUND'}\")\n",
    "print(f\"  Parquet Data in S3: {'EXISTS' if parquet_data_exists else 'NOT FOUND'}\")\n",
    "\n",
    "if raw_data_exists and parquet_data_exists:\n",
    "    print(f\"\\nData already ingested. You can skip to Step 5 (Create Athena Database)\")\n",
    "    skip_ingestion = True\n",
    "elif raw_data_exists:\n",
    "    print(f\"\\nRaw data exists. You may skip to Step 7 (Convert to Parquet)\")\n",
    "    skip_ingestion = True\n",
    "else:\n",
    "    print(f\"\\nData ingestion required. Continue with Steps 3-4.\")\n",
    "    skip_ingestion = False\n",
    "\n",
    "# Store paths and flags for use in other notebooks\n",
    "%store bucket\n",
    "%store region\n",
    "%store s3_raw_path\n",
    "%store s3_processed_path\n",
    "%store s3_parquet_path\n",
    "%store raw_data_exists\n",
    "%store parquet_data_exists\n",
    "%store skip_ingestion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c3ff86",
   "metadata": {},
   "source": [
    "## 3. Download CMS Open Payments Data\n",
    "\n",
    "Download the CMS Open Payments Program Year 2024 General Payments dataset.\n",
    "\n",
    "**Data Source:** CMS Open Payments  \n",
    "**Dataset:** Program Year 2024 General Payments  \n",
    "**Published:** June 30, 2025  \n",
    "**Coverage:** January 1, 2024 - December 31, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0baf9258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping download - data already exists in S3\n",
      "  S3 Path: s3://cmsopenpaymentsystemslight/cms-open-payments-light/raw\n"
     ]
    }
   ],
   "source": [
    "# CMS Open Payments data URL - Direct CSV download\n",
    "cms_data_url = \"https://download.cms.gov/openpayments/PGYR2024_P06302025_06162025/OP_DTL_GNRL_PGYR2024_P06302025_06162025.csv\"\n",
    "\n",
    "# Alternative: If the above URL doesn't work, use this approach:\n",
    "# 1. Go to https://openpaymentsdata.cms.gov/datasets\n",
    "# 2. Select \"Program Year 2024\" and \"General Payments\"\n",
    "# 3. Download the CSV file manually and place it in ../data/ directory\n",
    "\n",
    "if skip_ingestion and raw_data_exists:\n",
    "    print(f\"Skipping download - data already exists in S3\")\n",
    "    print(f\"  S3 Path: {s3_raw_path}\")\n",
    "else:\n",
    "    print(f\"CMS Data URL: {cms_data_url}\")\n",
    "    print(f\"\\nNote: This dataset is approximately 3-4 GB.\")\n",
    "    print(f\"Download may take several minutes depending on your connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92c50632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping download - data already exists in S3\n",
      "  S3 Path: s3://cmsopenpaymentsystemslight/cms-open-payments-light/raw\n",
      "Headers file: headers.csv\n"
     ]
    }
   ],
   "source": [
    "# Create local data directory if it doesn't exist\n",
    "local_data_dir = Path(\"../data\")\n",
    "local_data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "if skip_ingestion and raw_data_exists:\n",
    "    print(f\"Skipping download - data already exists in S3\")\n",
    "    print(f\"  S3 Path: {s3_raw_path}\")\n",
    "    local_csv_file = local_data_dir / \"headers.csv\"\n",
    "    print(f\"Headers file: {local_csv_file.name}\")\n",
    "else:\n",
    "    # Local CSV file path\n",
    "    local_csv_file = local_data_dir / dataset_file_name\n",
    "    print(f\"Local data directory: {local_data_dir.absolute()}\")\n",
    "    print(f\"Target CSV file: {local_csv_file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b07991ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping download - data already exists in S3\n",
      "  S3 Path: s3://cmsopenpaymentsystemslight/cms-open-payments-light/raw\n",
      "  Local file also present: ../data/headers.csv\n",
      "  File size: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Download CMS data if not already present\n",
    "if skip_ingestion and raw_data_exists:\n",
    "    print(f\"Skipping download - data already exists in S3\")\n",
    "    print(f\"  S3 Path: {s3_raw_path}\")\n",
    "    \n",
    "    # Check local file for reference\n",
    "    if local_csv_file.exists():\n",
    "        print(f\"  Local file also present: {local_csv_file}\")\n",
    "        print(f\"  File size: {local_csv_file.stat().st_size / (1024**3):.2f} GB\")\n",
    "    else:\n",
    "        print(f\"  Note: Local file not present, but S3 data is available\")\n",
    "\n",
    "elif local_csv_file.exists():\n",
    "    print(f\"CSV file already exists locally: {local_csv_file}\")\n",
    "    print(f\"  File size: {local_csv_file.stat().st_size / (1024**3):.2f} GB\")\n",
    "    print(f\"  Skipping download\")\n",
    "else:\n",
    "    print(f\"Downloading CMS Open Payments data...\")\n",
    "    print(f\"  This may take 10-20 minutes depending on your connection.\")\n",
    "    \n",
    "    try:\n",
    "        # Download CSV file with progress indication\n",
    "        response = requests.get(cms_data_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        print(f\"  Total download size: {total_size / (1024**3):.2f} GB\")\n",
    "        \n",
    "        # Save CSV file directly\n",
    "        with open(local_csv_file, 'wb') as f:\n",
    "            downloaded = 0\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    downloaded += len(chunk)\n",
    "                    if total_size > 0:\n",
    "                        percent = (downloaded / total_size) * 100\n",
    "                        print(f\"\\rProgress: {percent:.1f}%\", end=\"\")\n",
    "        \n",
    "        print(f\"\\nDownload complete: {local_csv_file}\")\n",
    "        print(f\"  File size: {local_csv_file.stat().st_size / (1024**3):.2f} GB\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError downloading data: {e}\")\n",
    "        print(f\"\\nAlternative approach:\")\n",
    "        print(f\"1. Visit: https://openpaymentsdata.cms.gov/datasets\")\n",
    "        print(f\"2. Select 'Program Year 2024' and 'General Payments'\")\n",
    "        print(f\"3. Download CSV and save to: {local_data_dir.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c67f2b8",
   "metadata": {},
   "source": [
    "## 4. Upload Data to S3 upload\n",
    "\n",
    "Upload the downloaded CMS data to S3 for datalake storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "905d1a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping preview - data already in S3\n",
      "  Run queries in Step 6 to view data\n"
     ]
    }
   ],
   "source": [
    "# Preview the data before upload\n",
    "if skip_ingestion and raw_data_exists:\n",
    "    print(\"Skipping preview - data already in S3\")\n",
    "    print(f\"  Run queries in Step 6 to view data\")\n",
    "elif local_csv_file.exists():\n",
    "    print(\"Loading sample of data for preview...\")\n",
    "    df_sample = pd.read_csv(local_csv_file, nrows=5)\n",
    "\n",
    "    print(f\"\\nDataset Preview:\")\n",
    "    print(f\"  Columns: {len(df_sample.columns)}\")\n",
    "    print(f\"  Sample rows:\")\n",
    "    display(df_sample.head())\n",
    "\n",
    "    print(f\"\\nColumn names:\")\n",
    "    for i, col in enumerate(df_sample.columns, 1):\n",
    "        print(f\"  {i}. {col}\")\n",
    "else:\n",
    "    print(\"Local CSV file not found. Cannot preview data.\")\n",
    "    print(f\"  Expected location: {local_csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "753fa4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping upload - data already exists in S3\n",
      "  S3 URI: s3://cmsopenpaymentsystemslight/cms-open-payments-light/raw/headers.csv\n",
      "Stored 's3_raw_file_path' (str)\n"
     ]
    }
   ],
   "source": [
    "# Upload raw CSV to S3\n",
    "s3_raw_file_path = f\"{s3_raw_path}/{local_csv_file.name}\"\n",
    "\n",
    "if skip_ingestion and raw_data_exists:\n",
    "    print(f\"Skipping upload - data already exists in S3\")\n",
    "    print(f\"  S3 URI: {s3_raw_file_path}\")\n",
    "    \n",
    "    # Store the S3 file path\n",
    "    %store s3_raw_file_path\n",
    "    \n",
    "elif not local_csv_file.exists():\n",
    "    print(f\"Cannot upload - local CSV file not found\")\n",
    "    print(f\"  Expected location: {local_csv_file}\")\n",
    "    print(f\"  Please download the file first (Step 3) or data already exists in S3\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Uploading data to S3...\")\n",
    "    print(f\"  Source: {local_csv_file}\")\n",
    "    print(f\"  Destination: {s3_raw_path}/\")\n",
    "\n",
    "    try:\n",
    "        # Upload file with progress callback\n",
    "        file_size = local_csv_file.stat().st_size\n",
    "        \n",
    "        def upload_progress(bytes_uploaded):\n",
    "            percent = (bytes_uploaded / file_size) * 100\n",
    "            print(f\"\\rUpload progress: {percent:.1f}%\", end=\"\")\n",
    "        \n",
    "        s3_client.upload_file(\n",
    "            str(local_csv_file),\n",
    "            bucket,\n",
    "            f\"{raw_data_prefix}/{local_csv_file.name}\",\n",
    "            Callback=upload_progress\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nUpload complete\")\n",
    "        print(f\"  S3 URI: {s3_raw_file_path}\")\n",
    "        \n",
    "        # Store the S3 file path\n",
    "        %store s3_raw_file_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError uploading to S3: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "998fbe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying S3 upload...\n",
      "\n",
      "Files in S3 bucket:\n",
      "  cms-open-payments-light/raw/lightdataset.csv (0.51 GB)\n"
     ]
    }
   ],
   "source": [
    "# Verify upload\n",
    "print(\"Verifying S3 upload...\")\n",
    "\n",
    "response = s3_client.list_objects_v2(\n",
    "    Bucket=bucket,\n",
    "    Prefix=raw_data_prefix\n",
    ")\n",
    "\n",
    "if 'Contents' in response:\n",
    "    print(f\"\\nFiles in S3 bucket:\")\n",
    "    for obj in response['Contents']:\n",
    "        size_gb = obj['Size'] / (1024**3)\n",
    "        print(f\"  {obj['Key']} ({size_gb:.2f} GB)\")\n",
    "else:\n",
    "    print(f\"\\nNo files found in S3 bucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667e90c0",
   "metadata": {},
   "source": [
    "## 5. Create Athena Database\n",
    "\n",
    "Create an Amazon Athena database for querying CMS data using SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9701a647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Athena Configuration:\n",
      "  Database: cms_open_payments_light\n",
      "  Staging Directory: s3://cmsopenpaymentsystemslight/athena/staging\n",
      "Stored 'database_name' (str)\n",
      "Stored 's3_athena_staging' (str)\n"
     ]
    }
   ],
   "source": [
    "# Set S3 staging directory for Athena queries\n",
    "s3_athena_staging = f\"s3://{bucket}/athena/staging\"\n",
    "\n",
    "print(f\"Athena Configuration:\")\n",
    "print(f\"  Database: {database_name}\")\n",
    "print(f\"  Staging Directory: {s3_athena_staging}\")\n",
    "\n",
    "# Store for use in other notebooks\n",
    "%store database_name\n",
    "%store s3_athena_staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf19b172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Athena connection established\n"
     ]
    }
   ],
   "source": [
    "# Create Athena connection\n",
    "athena_conn = connect(\n",
    "    region_name=region,\n",
    "    s3_staging_dir=s3_athena_staging\n",
    ")\n",
    "\n",
    "print(\"Athena connection established\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f70fd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Athena database...\n",
      "  Query: CREATE DATABASE IF NOT EXISTS cms_open_payments_light\n",
      "Database created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create database\n",
    "create_db_query = f\"CREATE DATABASE IF NOT EXISTS {database_name}\"\n",
    "\n",
    "print(f\"Creating Athena database...\")\n",
    "print(f\"  Query: {create_db_query}\")\n",
    "\n",
    "try:\n",
    "    result = pd.read_sql(create_db_query, athena_conn)\n",
    "    print(f\"Database created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating database: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "703a47ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying database creation...\n",
      "\n",
      " Available Databases:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>assignment_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cms_open_payments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cms_open_payments_light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dsoaws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sagemaker_featurestore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             database_name\n",
       "0             assignment_2\n",
       "1        cms_open_payments\n",
       "2  cms_open_payments_light\n",
       "3                  default\n",
       "4                   dsoaws\n",
       "5   sagemaker_featurestore"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Database 'cms_open_payments_light' exists\n"
     ]
    }
   ],
   "source": [
    "# Verify database creation\n",
    "show_db_query = \"SHOW DATABASES\"\n",
    "\n",
    "print(\"Verifying database creation...\")\n",
    "databases = pd.read_sql(show_db_query, athena_conn)\n",
    "\n",
    "print(f\"\\n Available Databases:\")\n",
    "display(databases)\n",
    "\n",
    "if database_name in databases.values:\n",
    "    print(f\"\\nDatabase '{database_name}' exists\")\n",
    "else:\n",
    "    print(f\"\\nDatabase '{database_name}' not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c0345",
   "metadata": {},
   "source": [
    "## 6. Register Data with Athena\n",
    "\n",
    "Create an external table in Athena to query the CSV data stored in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "552735e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Configuration:\n",
      "  Database: cms_open_payments_light\n",
      "  Table: general_payments_csv\n",
      "  Location: s3://cmsopenpaymentsystemslight/cms-open-payments-light/raw/\n",
      "Stored 'table_name_csv' (str)\n"
     ]
    }
   ],
   "source": [
    "# Define table name\n",
    "table_name_csv = \"general_payments_csv\"\n",
    "\n",
    "print(f\"Table Configuration:\")\n",
    "print(f\"  Database: {database_name}\")\n",
    "print(f\"  Table: {table_name_csv}\")\n",
    "print(f\"  Location: {s3_raw_path}/\")\n",
    "\n",
    "%store table_name_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac3e2638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema preview (first 10 columns):\n",
      "  1. `Change_Type` STRING\n",
      "  2. `Covered_Recipient_Type` STRING\n",
      "  3. `Teaching_Hospital_CCN` DOUBLE\n",
      "  4. `Teaching_Hospital_ID` DOUBLE\n",
      "  5. `Teaching_Hospital_Name` STRING\n",
      "  6. `Covered_Recipient_Profile_ID` DOUBLE\n",
      "  7. `Covered_Recipient_NPI` DOUBLE\n",
      "  8. `Covered_Recipient_First_Name` DOUBLE\n",
      "  9. `Covered_Recipient_Middle_Name` DOUBLE\n",
      "  10. `Covered_Recipient_Last_Name` DOUBLE\n",
      "  ... (91 columns total)\n"
     ]
    }
   ],
   "source": [
    "# Get actual column names from the CSV\n",
    "df_schema = pd.read_csv(local_csv_file, nrows=1)\n",
    "\n",
    "# Create column definitions for Athena\n",
    "# Map pandas dtypes to Athena types\n",
    "def get_athena_type(dtype):\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return 'BIGINT'\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return 'DOUBLE'\n",
    "    elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "        return 'TIMESTAMP'\n",
    "    else:\n",
    "        return 'STRING'\n",
    "\n",
    "# Create column definitions\n",
    "columns_def = []\n",
    "for col in df_schema.columns:\n",
    "    # Clean column name for Athena (replace spaces and special chars)\n",
    "    clean_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_')\n",
    "    athena_type = get_athena_type(df_schema[col].dtype)\n",
    "    columns_def.append(f\"`{col}` {athena_type}\")\n",
    "\n",
    "columns_str = ',\\n    '.join(columns_def)\n",
    "\n",
    "print(f\"Schema preview (first 10 columns):\")\n",
    "for i, col_def in enumerate(columns_def[:10], 1):\n",
    "    print(f\"  {i}. {col_def}\")\n",
    "print(f\"  ... ({len(columns_def)} columns total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dd87b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating external table...\n",
      "\n",
      "Query preview:\n",
      "\n",
      "CREATE EXTERNAL TABLE IF NOT EXISTS cms_open_payments_light.general_payments_csv (\n",
      "    `Change_Type` STRING,\n",
      "    `Covered_Recipient_Type` STRING,\n",
      "    `Teaching_Hospital_CCN` DOUBLE,\n",
      "    `Teaching_Hospital_ID` DOUBLE,\n",
      "    `Teaching_Hospital_Name` STRING,\n",
      "    `Covered_Recipient_Profile_ID` DOUBLE,\n",
      "    `Covered_Recipient_NPI` DOUBLE,\n",
      "    `Covered_Recipient_First_Name` DOUBLE,\n",
      "    `Covered_Recipient_Middle_Name` DOUBLE,\n",
      "    `Covered_Recipient_Last_Name` DOUBLE,\n",
      "    `Covered_Recipient_Name_Suffix` D...\n",
      "\n",
      "Table 'general_payments_csv' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create external table for CSV data\n",
    "create_table_query = f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {database_name}.{table_name_csv} (\n",
    "    {columns_str}\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\\\n'\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '{s3_raw_path}/'\n",
    "TBLPROPERTIES (\n",
    "    'skip.header.line.count'='1',\n",
    "    'serialization.null.format'=''\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Creating external table...\")\n",
    "print(f\"\\nQuery preview:\")\n",
    "print(create_table_query[:500] + \"...\")\n",
    "\n",
    "try:\n",
    "    result = pd.read_sql(create_table_query, athena_conn)\n",
    "    print(f\"\\nTable '{table_name_csv}' created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError creating table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3435350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying table creation...\n",
      "\n",
      "Tables in database 'cms_open_payments_light':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tab_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>general_payments_csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>general_payments_parquet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   tab_name\n",
       "0      general_payments_csv\n",
       "1  general_payments_parquet"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table 'general_payments_csv' exists\n"
     ]
    }
   ],
   "source": [
    "# Verify table creation\n",
    "show_tables_query = f\"SHOW TABLES IN {database_name}\"\n",
    "\n",
    "print(\"Verifying table creation...\")\n",
    "tables = pd.read_sql(show_tables_query, athena_conn)\n",
    "\n",
    "print(f\"\\nTables in database '{database_name}':\")\n",
    "display(tables)\n",
    "\n",
    "if table_name_csv in tables.values:\n",
    "    print(f\"\\nTable '{table_name_csv}' exists\")\n",
    "else:\n",
    "    print(f\"\\nTable '{table_name_csv}' not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf0c229f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing table access...\n",
      "Query: \n",
      "SELECT COUNT(*) as row_count\n",
      "FROM cms_open_payments_light.general_payments_csv\n",
      "\n",
      "\n",
      "Query successful\n",
      "  Total rows: 1,000,000\n"
     ]
    }
   ],
   "source": [
    "# Test query - count rows\n",
    "count_query = f\"\"\"\n",
    "SELECT COUNT(*) as row_count\n",
    "FROM {database_name}.{table_name_csv}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Testing table access...\")\n",
    "print(f\"Query: {count_query}\")\n",
    "\n",
    "try:\n",
    "    result = pd.read_sql(count_query, athena_conn)\n",
    "    print(f\"\\nQuery successful\")\n",
    "    print(f\"  Total rows: {result['row_count'][0]:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError querying table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e153c8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching sample data...\n",
      "\n",
      "Sample data retrieved\n",
      "  Shape: (5, 91)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>change_type</th>\n",
       "      <th>covered_recipient_type</th>\n",
       "      <th>teaching_hospital_ccn</th>\n",
       "      <th>teaching_hospital_id</th>\n",
       "      <th>teaching_hospital_name</th>\n",
       "      <th>covered_recipient_profile_id</th>\n",
       "      <th>covered_recipient_npi</th>\n",
       "      <th>covered_recipient_first_name</th>\n",
       "      <th>covered_recipient_middle_name</th>\n",
       "      <th>covered_recipient_last_name</th>\n",
       "      <th>covered_recipient_name_suffix</th>\n",
       "      <th>recipient_primary_business_street_address_line1</th>\n",
       "      <th>recipient_primary_business_street_address_line2</th>\n",
       "      <th>recipient_city</th>\n",
       "      <th>recipient_state</th>\n",
       "      <th>recipient_zip_code</th>\n",
       "      <th>recipient_country</th>\n",
       "      <th>recipient_province</th>\n",
       "      <th>recipient_postal_code</th>\n",
       "      <th>covered_recipient_primary_type_1</th>\n",
       "      <th>covered_recipient_primary_type_2</th>\n",
       "      <th>covered_recipient_primary_type_3</th>\n",
       "      <th>covered_recipient_primary_type_4</th>\n",
       "      <th>covered_recipient_primary_type_5</th>\n",
       "      <th>covered_recipient_primary_type_6</th>\n",
       "      <th>...</th>\n",
       "      <th>indicate_drug_or_biological_or_device_or_medical_supply_2</th>\n",
       "      <th>product_category_or_therapeutic_area_2</th>\n",
       "      <th>name_of_drug_or_biological_or_device_or_medical_supply_2</th>\n",
       "      <th>associated_drug_or_biological_ndc_2</th>\n",
       "      <th>associated_device_or_medical_supply_pdi_2</th>\n",
       "      <th>covered_or_noncovered_indicator_3</th>\n",
       "      <th>indicate_drug_or_biological_or_device_or_medical_supply_3</th>\n",
       "      <th>product_category_or_therapeutic_area_3</th>\n",
       "      <th>name_of_drug_or_biological_or_device_or_medical_supply_3</th>\n",
       "      <th>associated_drug_or_biological_ndc_3</th>\n",
       "      <th>associated_device_or_medical_supply_pdi_3</th>\n",
       "      <th>covered_or_noncovered_indicator_4</th>\n",
       "      <th>indicate_drug_or_biological_or_device_or_medical_supply_4</th>\n",
       "      <th>product_category_or_therapeutic_area_4</th>\n",
       "      <th>name_of_drug_or_biological_or_device_or_medical_supply_4</th>\n",
       "      <th>associated_drug_or_biological_ndc_4</th>\n",
       "      <th>associated_device_or_medical_supply_pdi_4</th>\n",
       "      <th>covered_or_noncovered_indicator_5</th>\n",
       "      <th>indicate_drug_or_biological_or_device_or_medical_supply_5</th>\n",
       "      <th>product_category_or_therapeutic_area_5</th>\n",
       "      <th>name_of_drug_or_biological_or_device_or_medical_supply_5</th>\n",
       "      <th>associated_drug_or_biological_ndc_5</th>\n",
       "      <th>associated_device_or_medical_supply_pdi_5</th>\n",
       "      <th>program_year</th>\n",
       "      <th>payment_publication_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEW</td>\n",
       "      <td>Covered Recipient Physician</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>121431.0</td>\n",
       "      <td>1.669764e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4645 NW 8TH AVE</td>\n",
       "      <td>None</td>\n",
       "      <td>GAINESVILLE</td>\n",
       "      <td>FL</td>\n",
       "      <td>32605.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>06/30/2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NEW</td>\n",
       "      <td>Covered Recipient Physician</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1177044.0</td>\n",
       "      <td>1.598078e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4921 PARKVIEW PL</td>\n",
       "      <td>None</td>\n",
       "      <td>STE 5C\"</td>\n",
       "      <td>SAINT LOUIS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63110</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEW</td>\n",
       "      <td>Covered Recipient Non-Physician Practitioner</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>11193457.0</td>\n",
       "      <td>1.417324e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3051 BUFFALO RD</td>\n",
       "      <td>None</td>\n",
       "      <td>LAWRENCEBURG</td>\n",
       "      <td>TN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEW</td>\n",
       "      <td>Covered Recipient Physician</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2792925.0</td>\n",
       "      <td>1.396190e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>12670 CREEKSIDE LN STE 202</td>\n",
       "      <td>None</td>\n",
       "      <td>FORT MYERS</td>\n",
       "      <td>FL</td>\n",
       "      <td>33919.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>06/30/2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEW</td>\n",
       "      <td>Covered Recipient Non-Physician Practitioner</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>11108217.0</td>\n",
       "      <td>1.891773e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>421 SE ALFRED MARKHAM ST</td>\n",
       "      <td>None</td>\n",
       "      <td>LAKE CITY</td>\n",
       "      <td>FL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  change_type                        covered_recipient_type  \\\n",
       "0         NEW                   Covered Recipient Physician   \n",
       "1         NEW                   Covered Recipient Physician   \n",
       "2         NEW  Covered Recipient Non-Physician Practitioner   \n",
       "3         NEW                   Covered Recipient Physician   \n",
       "4         NEW  Covered Recipient Non-Physician Practitioner   \n",
       "\n",
       "  teaching_hospital_ccn teaching_hospital_id teaching_hospital_name  \\\n",
       "0                  None                 None                   None   \n",
       "1                  None                 None                   None   \n",
       "2                  None                 None                   None   \n",
       "3                  None                 None                   None   \n",
       "4                  None                 None                   None   \n",
       "\n",
       "   covered_recipient_profile_id  covered_recipient_npi  \\\n",
       "0                      121431.0           1.669764e+09   \n",
       "1                     1177044.0           1.598078e+09   \n",
       "2                    11193457.0           1.417324e+09   \n",
       "3                     2792925.0           1.396190e+09   \n",
       "4                    11108217.0           1.891773e+09   \n",
       "\n",
       "  covered_recipient_first_name covered_recipient_middle_name  \\\n",
       "0                         None                          None   \n",
       "1                         None                          None   \n",
       "2                         None                          None   \n",
       "3                         None                          None   \n",
       "4                         None                          None   \n",
       "\n",
       "  covered_recipient_last_name covered_recipient_name_suffix  \\\n",
       "0                        None                          None   \n",
       "1                        None                          None   \n",
       "2                        None                          None   \n",
       "3                        None                          None   \n",
       "4                        None                          None   \n",
       "\n",
       "  recipient_primary_business_street_address_line1  \\\n",
       "0                                 4645 NW 8TH AVE   \n",
       "1                                4921 PARKVIEW PL   \n",
       "2                                 3051 BUFFALO RD   \n",
       "3                      12670 CREEKSIDE LN STE 202   \n",
       "4                        421 SE ALFRED MARKHAM ST   \n",
       "\n",
       "  recipient_primary_business_street_address_line2 recipient_city  \\\n",
       "0                                            None    GAINESVILLE   \n",
       "1                                            None        STE 5C\"   \n",
       "2                                            None   LAWRENCEBURG   \n",
       "3                                            None     FORT MYERS   \n",
       "4                                            None      LAKE CITY   \n",
       "\n",
       "  recipient_state  recipient_zip_code recipient_country recipient_province  \\\n",
       "0              FL             32605.0     United States               None   \n",
       "1     SAINT LOUIS                 NaN             63110               None   \n",
       "2              TN                 NaN     United States               None   \n",
       "3              FL             33919.0     United States               None   \n",
       "4              FL                 NaN     United States               None   \n",
       "\n",
       "  recipient_postal_code covered_recipient_primary_type_1  \\\n",
       "0                  None                             None   \n",
       "1                  None                             None   \n",
       "2                  None                             None   \n",
       "3                  None                             None   \n",
       "4                  None                             None   \n",
       "\n",
       "  covered_recipient_primary_type_2 covered_recipient_primary_type_3  \\\n",
       "0                             None                             None   \n",
       "1                             None                             None   \n",
       "2                             None                             None   \n",
       "3                             None                             None   \n",
       "4                             None                             None   \n",
       "\n",
       "  covered_recipient_primary_type_4 covered_recipient_primary_type_5  \\\n",
       "0                             None                             None   \n",
       "1                             None                             None   \n",
       "2                             None                             None   \n",
       "3                             None                             None   \n",
       "4                             None                             None   \n",
       "\n",
       "  covered_recipient_primary_type_6  ...  \\\n",
       "0                             None  ...   \n",
       "1                             None  ...   \n",
       "2                             None  ...   \n",
       "3                             None  ...   \n",
       "4                             None  ...   \n",
       "\n",
       "  indicate_drug_or_biological_or_device_or_medical_supply_2  \\\n",
       "0                                               None          \n",
       "1                                               None          \n",
       "2                                               None          \n",
       "3                                               None          \n",
       "4                                               None          \n",
       "\n",
       "  product_category_or_therapeutic_area_2  \\\n",
       "0                                   None   \n",
       "1                                   None   \n",
       "2                                   None   \n",
       "3                                   None   \n",
       "4                                   None   \n",
       "\n",
       "  name_of_drug_or_biological_or_device_or_medical_supply_2  \\\n",
       "0                                               None         \n",
       "1                                               None         \n",
       "2                                               None         \n",
       "3                                               None         \n",
       "4                                               None         \n",
       "\n",
       "  associated_drug_or_biological_ndc_2  \\\n",
       "0                                None   \n",
       "1                                None   \n",
       "2                                None   \n",
       "3                                None   \n",
       "4                                None   \n",
       "\n",
       "  associated_device_or_medical_supply_pdi_2 covered_or_noncovered_indicator_3  \\\n",
       "0                                      None                              None   \n",
       "1                                      None                              None   \n",
       "2                                      None                              None   \n",
       "3                                      None                              None   \n",
       "4                                      None                              None   \n",
       "\n",
       "  indicate_drug_or_biological_or_device_or_medical_supply_3  \\\n",
       "0                                               None          \n",
       "1                                               None          \n",
       "2                                               None          \n",
       "3                                               None          \n",
       "4                                               None          \n",
       "\n",
       "  product_category_or_therapeutic_area_3  \\\n",
       "0                                   None   \n",
       "1                                   None   \n",
       "2                                   None   \n",
       "3                                   None   \n",
       "4                                   None   \n",
       "\n",
       "  name_of_drug_or_biological_or_device_or_medical_supply_3  \\\n",
       "0                                               None         \n",
       "1                                               None         \n",
       "2                                               None         \n",
       "3                                               None         \n",
       "4                                               None         \n",
       "\n",
       "  associated_drug_or_biological_ndc_3  \\\n",
       "0                                None   \n",
       "1                                None   \n",
       "2                                None   \n",
       "3                                None   \n",
       "4                                None   \n",
       "\n",
       "  associated_device_or_medical_supply_pdi_3 covered_or_noncovered_indicator_4  \\\n",
       "0                                      None                              None   \n",
       "1                                      None                              None   \n",
       "2                                      None                              None   \n",
       "3                                      None                              None   \n",
       "4                                      None                              None   \n",
       "\n",
       "   indicate_drug_or_biological_or_device_or_medical_supply_4  \\\n",
       "0                                               None           \n",
       "1                                               None           \n",
       "2                                               None           \n",
       "3                                               None           \n",
       "4                                               None           \n",
       "\n",
       "  product_category_or_therapeutic_area_4  \\\n",
       "0                                   None   \n",
       "1                                   None   \n",
       "2                                   None   \n",
       "3                                   None   \n",
       "4                                   None   \n",
       "\n",
       "  name_of_drug_or_biological_or_device_or_medical_supply_4  \\\n",
       "0                                               None         \n",
       "1                                               None         \n",
       "2                                               None         \n",
       "3                                               None         \n",
       "4                                               None         \n",
       "\n",
       "  associated_drug_or_biological_ndc_4  \\\n",
       "0                                None   \n",
       "1                                None   \n",
       "2                                None   \n",
       "3                                None   \n",
       "4                                None   \n",
       "\n",
       "   associated_device_or_medical_supply_pdi_4  \\\n",
       "0                                       None   \n",
       "1                                       None   \n",
       "2                                       None   \n",
       "3                                       None   \n",
       "4                                       None   \n",
       "\n",
       "  covered_or_noncovered_indicator_5  \\\n",
       "0                              None   \n",
       "1                              None   \n",
       "2                              None   \n",
       "3                              None   \n",
       "4                              None   \n",
       "\n",
       "   indicate_drug_or_biological_or_device_or_medical_supply_5  \\\n",
       "0                                               None           \n",
       "1                                               None           \n",
       "2                                               None           \n",
       "3                                               None           \n",
       "4                                               None           \n",
       "\n",
       "  product_category_or_therapeutic_area_5  \\\n",
       "0                                   None   \n",
       "1                                   None   \n",
       "2                                   None   \n",
       "3                                   None   \n",
       "4                                   None   \n",
       "\n",
       "  name_of_drug_or_biological_or_device_or_medical_supply_5  \\\n",
       "0                                               None         \n",
       "1                                               None         \n",
       "2                                               None         \n",
       "3                                               None         \n",
       "4                                               None         \n",
       "\n",
       "  associated_drug_or_biological_ndc_5  \\\n",
       "0                                None   \n",
       "1                                None   \n",
       "2                                None   \n",
       "3                                None   \n",
       "4                                None   \n",
       "\n",
       "  associated_device_or_medical_supply_pdi_5 program_year  \\\n",
       "0                                      None       2024.0   \n",
       "1                                      None          NaN   \n",
       "2                                      None          NaN   \n",
       "3                                      None       2024.0   \n",
       "4                                      None          NaN   \n",
       "\n",
       "  payment_publication_date  \n",
       "0               06/30/2025  \n",
       "1                     None  \n",
       "2                     None  \n",
       "3               06/30/2025  \n",
       "4                     2024  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample query - preview data\n",
    "sample_query = f\"\"\"\n",
    "SELECT *\n",
    "FROM {database_name}.{table_name_csv}\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "print(\"Fetching sample data...\")\n",
    "\n",
    "try:\n",
    "    sample_data = pd.read_sql(sample_query, athena_conn)\n",
    "    print(f\"\\nSample data retrieved\")\n",
    "    print(f\"  Shape: {sample_data.shape}\")\n",
    "    display(sample_data.head())\n",
    "except Exception as e:\n",
    "    print(f\"\\nError fetching sample data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d3faaa",
   "metadata": {},
   "source": [
    "## 7. Convert CSV to Parquet\n",
    "\n",
    "Convert the CSV data to Parquet format for better performance and compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e730f1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet Conversion Configuration:\n",
      "  Source Table: cms_open_payments_light.general_payments_csv\n",
      "  Target Table: cms_open_payments_light.general_payments_parquet\n",
      "  Target Location: s3://cmsopenpaymentsystemslight/cms-open-payments-light/parquet/\n",
      "Stored 'table_name_parquet' (str)\n"
     ]
    }
   ],
   "source": [
    "# Define Parquet table name\n",
    "table_name_parquet = \"general_payments_parquet\"\n",
    "\n",
    "print(f\"Parquet Conversion Configuration:\")\n",
    "print(f\"  Source Table: {database_name}.{table_name_csv}\")\n",
    "print(f\"  Target Table: {database_name}.{table_name_parquet}\")\n",
    "print(f\"  Target Location: {s3_parquet_path}/\")\n",
    "\n",
    "%store table_name_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8ae138e-59a6-4bc6-ba14-c56977126285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up physical S3 files...\n",
      "S3 Parquet folder cleared.\n",
      "\n",
      "Dropping the old table metadata...\n",
      "Table general_payments_parquet dropped successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*** SIGTERM received at time=1770199681 on cpu 2 ***\n",
      "PC: @     0x7f5f9fdcfe9e  (unknown)  epoll_wait\n",
      "    @     0x7f5f47806b0d         64  absl::lts_20240722::AbslFailureSignalHandler()\n",
      "    @     0x7f5f9fcec520  (unknown)  (unknown)\n",
      "[2026-02-04 10:08:01,765 E 1287 1287] logging.cc:497: *** SIGTERM received at time=1770199681 on cpu 2 ***\n",
      "[2026-02-04 10:08:01,765 E 1287 1287] logging.cc:497: PC: @     0x7f5f9fdcfe9e  (unknown)  epoll_wait\n",
      "[2026-02-04 10:08:01,767 E 1287 1287] logging.cc:497:     @     0x7f5f47806b39         64  absl::lts_20240722::AbslFailureSignalHandler()\n",
      "[2026-02-04 10:08:01,767 E 1287 1287] logging.cc:497:     @     0x7f5f9fcec520  (unknown)  (unknown)\n"
     ]
    }
   ],
   "source": [
    "# clean up the parquet files, as it was causing weird issues for me (JN)\n",
    "# probably don't need to do this? \n",
    "print(\"Cleaning up physical S3 files...\")\n",
    "try:\n",
    "    wr.s3.delete_objects(f\"s3://{bucket}/{parquet_data_prefix}\")\n",
    "    print(\"S3 Parquet folder cleared.\")\n",
    "except Exception as e:\n",
    "    print(f\"S3 cleanup skipped or failed (might already be empty): {e}\")\n",
    "\n",
    "# removes the table definition from Glue/Athena\n",
    "print(\"\\nDropping the old table metadata...\")\n",
    "try:\n",
    "    cursor = athena_conn.cursor()\n",
    "    cursor.execute(f\"DROP TABLE IF EXISTS {database_name}.{table_name_parquet}\")\n",
    "    print(f\"Table {table_name_parquet} dropped successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error dropping table: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54c8b8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting CSV to Parquet format with enforced schema...\n",
      "Query:\n",
      "\n",
      "    CREATE TABLE cms_open_payments_light.general_payments_parquet\n",
      "    WITH (\n",
      "        format = 'PARQUET',\n",
      "        parquet_compression = 'SNAPPY',\n",
      "        external_location = 's3://cmsopenpaymentsystemslight/cms-open-payments-light/parquet/',\n",
      "        partitioned_by = ARRAY['program_year']\n",
      "    )\n",
      "    AS\n",
      "    SELECT \n",
      "        change_type,\n",
      "        covered_recipient_type,\n",
      "        CAST(teaching_hospital_ccn AS VARCHAR) AS teaching_hospital_ccn,\n",
      "        teaching_hospital_id,\n",
      "        teaching_hospital_name,\n",
      "        covered_recipient_profile_id,\n",
      "        covered_recipient_npi,\n",
      "        covered_recipient_first_name,\n",
      "        covered_recipient_middle_name,\n",
      "        covered_recipient_last_name,\n",
      "        covered_recipient_name_suffix,\n",
      "        recipient_primary_business_street_address_line1,\n",
      "        recipient_primary_business_street_address_line2,\n",
      "        recipient_city,\n",
      "        recipient_state,\n",
      "        recipient_zip_code,\n",
      "        recipient_country,\n",
      "        recipient_province,\n",
      "        recipient_postal_code,\n",
      "        covered_recipient_primary_type_1,\n",
      "        covered_recipient_primary_type_2,\n",
      "        covered_recipient_primary_type_3,\n",
      "        covered_recipient_primary_type_4,\n",
      "        covered_recipient_primary_type_5,\n",
      "        covered_recipient_primary_type_6,\n",
      "        covered_recipient_specialty_1,\n",
      "        covered_recipient_specialty_2,\n",
      "        covered_recipient_specialty_3,\n",
      "        covered_recipient_specialty_4,\n",
      "        covered_recipient_specialty_5,\n",
      "        covered_recipient_specialty_6,\n",
      "        covered_recipient_license_state_code1,\n",
      "        covered_recipient_license_state_code2,\n",
      "        covered_recipient_license_state_code3,\n",
      "        covered_recipient_license_state_code4,\n",
      "        covered_recipient_license_state_code5,\n",
      "        submitting_applicable_manufacturer_or_applicable_gpo_name,\n",
      "        CAST(applicable_manufacturer_or_applicable_gpo_making_payment_id AS VARCHAR) AS applicable_manufacturer_or_applicable_gpo_making_payment_id,\n",
      "        applicable_manufacturer_or_applicable_gpo_making_payment_name,\n",
      "        applicable_manufacturer_or_applicable_gpo_making_payment_state,\n",
      "        applicable_manufacturer_or_applicable_gpo_making_payment_country,\n",
      "        total_amount_of_payment_usdollars,\n",
      "        date_of_payment,\n",
      "        number_of_payments_included_in_total_amount,\n",
      "        form_of_payment_or_transfer_of_value,\n",
      "        nature_of_payment_or_transfer_of_value,\n",
      "        city_of_travel,\n",
      "        state_of_travel,\n",
      "        country_of_travel,\n",
      "        physician_ownership_indicator,\n",
      "        third_party_payment_recipient_indicator,\n",
      "        name_of_third_party_entity_receiving_payment_or_transfer_of_value,\n",
      "        charity_indicator,\n",
      "        third_party_equals_covered_recipient_indicator,\n",
      "        contextual_information,\n",
      "        delay_in_publication_indicator,\n",
      "        record_id,\n",
      "        dispute_status_for_publication,\n",
      "        related_product_indicator,\n",
      "        covered_or_noncovered_indicator_1,\n",
      "        indicate_drug_or_biological_or_device_or_medical_supply_1,\n",
      "        product_category_or_therapeutic_area_1,\n",
      "        name_of_drug_or_biological_or_device_or_medical_supply_1,\n",
      "        associated_drug_or_biological_ndc_1,\n",
      "        associated_device_or_medical_supply_pdi_1,\n",
      "        covered_or_noncovered_indicator_2,\n",
      "        indicate_drug_or_biological_or_device_or_medical_supply_2,\n",
      "        product_category_or_therapeutic_area_2,\n",
      "        name_of_drug_or_biological_or_device_or_medical_supply_2,\n",
      "        associated_drug_or_biological_ndc_2,\n",
      "        associated_device_or_medical_supply_pdi_2,\n",
      "        covered_or_noncovered_indicator_3,\n",
      "        indicate_drug_or_biological_or_device_or_medical_supply_3,\n",
      "        product_category_or_therapeutic_area_3,\n",
      "        name_of_drug_or_biological_or_device_or_medical_supply_3,\n",
      "        associated_drug_or_biological_ndc_3,\n",
      "        associated_device_or_medical_supply_pdi_3,\n",
      "        covered_or_noncovered_indicator_4,\n",
      "        indicate_drug_or_biological_or_device_or_medical_supply_4,\n",
      "        product_category_or_therapeutic_area_4,\n",
      "        name_of_drug_or_biological_or_device_or_medical_supply_4,\n",
      "        associated_drug_or_biological_ndc_4,\n",
      "        associated_device_or_medical_supply_pdi_4,\n",
      "        covered_or_noncovered_indicator_5,\n",
      "        indicate_drug_or_biological_or_device_or_medical_supply_5,\n",
      "        product_category_or_therapeutic_area_5,\n",
      "        name_of_drug_or_biological_or_device_or_medical_supply_5,\n",
      "        associated_drug_or_biological_ndc_5,\n",
      "        associated_device_or_medical_supply_pdi_5,\n",
      "        payment_publication_date,\n",
      "        '2024' as program_year\n",
      "    FROM cms_open_payments_light.general_payments_csv\n",
      "    \n",
      "\n",
      "Conversion complete. Table 'general_payments_parquet' created correctly.\n"
     ]
    }
   ],
   "source": [
    "# define the missing query first\n",
    "columns_query = f\"SELECT * FROM {database_name}.{table_name_csv} LIMIT 1\"\n",
    "\n",
    "# define columns that MUST be strings to prevent schema mismatch errors\n",
    "force_string_cols = [\n",
    "    'teaching_hospital_ccn', \n",
    "    'applicable_manufacturer_or_applicable_gpo_making_payment_id'\n",
    "]\n",
    "\n",
    "try:\n",
    "    # get all columns from source table\n",
    "    columns_df = pd.read_sql(columns_query, athena_conn)\n",
    "    columns = columns_df.columns.tolist()\n",
    "    \n",
    "    # remove program_year if it exists (it will be added manually in the SELECT)\n",
    "    if 'program_year' in columns:\n",
    "        columns.remove('program_year')\n",
    "    \n",
    "    # create column selection string with explicit CASTing\n",
    "    formatted_columns = []\n",
    "    for col in columns:\n",
    "        if col in force_string_cols:\n",
    "            # this forces the column to be a String (VARCHAR) in the Parquet file\n",
    "            formatted_columns.append(f\"CAST({col} AS VARCHAR) AS {col}\")\n",
    "        else:\n",
    "            formatted_columns.append(col)\n",
    "            \n",
    "    columns_str = ',\\n        '.join(formatted_columns)\n",
    "    \n",
    "    # 3. create the query\n",
    "    create_parquet_query = f\"\"\"\n",
    "    CREATE TABLE {database_name}.{table_name_parquet}\n",
    "    WITH (\n",
    "        format = 'PARQUET',\n",
    "        parquet_compression = 'SNAPPY',\n",
    "        external_location = '{s3_parquet_path}/',\n",
    "        partitioned_by = ARRAY['program_year']\n",
    "    )\n",
    "    AS\n",
    "    SELECT \n",
    "        {columns_str},\n",
    "        '2024' as program_year\n",
    "    FROM {database_name}.{table_name_csv}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Converting CSV to Parquet format with enforced schema...\")\n",
    "    print(f\"Query:\\n{create_parquet_query}\")\n",
    "\n",
    "    # execute conversion\n",
    "    pd.read_sql(create_parquet_query, athena_conn)\n",
    "    print(f\"\\nConversion complete. Table '{table_name_parquet}' created correctly.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during conversion: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "177c7afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying Parquet table...\n",
      "\n",
      "Parquet table verified\n",
      "  Total rows: 1,000,000\n"
     ]
    }
   ],
   "source": [
    "# verify Parquet table\n",
    "count_parquet_query = f\"\"\"\n",
    "SELECT COUNT(*) as row_count\n",
    "FROM {database_name}.{table_name_parquet}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Verifying Parquet table...\")\n",
    "\n",
    "try:\n",
    "    result = pd.read_sql(count_parquet_query, athena_conn)\n",
    "    print(f\"\\nParquet table verified\")\n",
    "    print(f\"  Total rows: {result['row_count'][0]:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError verifying Parquet table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89eaa880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing CSV vs Parquet storage:\n",
      "\n",
      "Storage Comparison:\n",
      "  CSV Size: 0.51 GB\n",
      "  Parquet Size: 0.06 GB\n",
      "  Compression: 89.0% reduction\n",
      "  Space Saved: 0.46 GB\n"
     ]
    }
   ],
   "source": [
    "# Compare file sizes\n",
    "print(\"Comparing CSV vs Parquet storage:\")\n",
    "\n",
    "# Get CSV size\n",
    "csv_objects = s3_client.list_objects_v2(\n",
    "    Bucket=bucket,\n",
    "    Prefix=raw_data_prefix\n",
    ")\n",
    "\n",
    "csv_size = sum(obj['Size'] for obj in csv_objects.get('Contents', []))\n",
    "\n",
    "# Get Parquet size\n",
    "parquet_objects = s3_client.list_objects_v2(\n",
    "    Bucket=bucket,\n",
    "    Prefix=parquet_data_prefix\n",
    ")\n",
    "\n",
    "parquet_size = sum(obj['Size'] for obj in parquet_objects.get('Contents', []))\n",
    "\n",
    "print(f\"\\nStorage Comparison:\")\n",
    "print(f\"  CSV Size: {csv_size / (1024**3):.2f} GB\")\n",
    "print(f\"  Parquet Size: {parquet_size / (1024**3):.2f} GB\")\n",
    "if parquet_size > 0:\n",
    "    compression_ratio = (1 - parquet_size/csv_size) * 100\n",
    "    print(f\"  Compression: {compression_ratio:.1f}% reduction\")\n",
    "    print(f\"  Space Saved: {(csv_size - parquet_size) / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6925b16",
   "metadata": {},
   "source": [
    "## 8. Query Data with AWS Data Wrangler\n",
    "\n",
    "Use AWS Data Wrangler for more efficient data querying and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56e7d541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying payment statistics with AWS Data Wrangler...\n",
      "\n",
      "Query: \n",
      "SELECT \n",
      "    COUNT(*) as total_payments,\n",
      "    SUM(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as total_amount,\n",
      "    AVG(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as avg_amount,\n",
      "    MIN(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as min_amount,\n",
      "    MAX(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as max_amount\n",
      "FROM cms_open_payments_light.general_payments_parquet\n",
      "\n",
      "\n",
      "Query successful\n",
      "\n",
      "Payment Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_payments</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>avg_amount</th>\n",
       "      <th>min_amount</th>\n",
       "      <th>max_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000</td>\n",
       "      <td>4.500100e+12</td>\n",
       "      <td>8.206366e+06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.000010e+11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_payments  total_amount    avg_amount  min_amount    max_amount\n",
       "0         1000000  4.500100e+12  8.206366e+06        0.01  1.000010e+11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query using AWS Data Wrangler\n",
    "sample_query_wr = f\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_payments,\n",
    "    SUM(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as total_amount,\n",
    "    AVG(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as avg_amount,\n",
    "    MIN(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as min_amount,\n",
    "    MAX(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as max_amount\n",
    "FROM {database_name}.{table_name_parquet}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Querying payment statistics with AWS Data Wrangler...\")\n",
    "print(f\"\\nQuery: {sample_query_wr}\")\n",
    "\n",
    "try:\n",
    "    df_stats = wr.athena.read_sql_query(\n",
    "        sql=sample_query_wr,\n",
    "        database=database_name,\n",
    "        ctas_approach=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nQuery successful\")\n",
    "    print(f\"\\nPayment Statistics:\")\n",
    "    display(df_stats)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError querying data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2163587f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing payments by recipient type...\n",
      "\n",
      "Query successful\n",
      "\n",
      "Payments by Recipient Type:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Covered_Recipient_Type</th>\n",
       "      <th>payment_count</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Covered Recipient Physician</td>\n",
       "      <td>642145</td>\n",
       "      <td>2.800069e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Covered Recipient Non-Physician Practitioner</td>\n",
       "      <td>355575</td>\n",
       "      <td>1.700011e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Covered Recipient Teaching Hospital</td>\n",
       "      <td>2280</td>\n",
       "      <td>1.961271e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Covered_Recipient_Type  payment_count  total_amount\n",
       "0                   Covered Recipient Physician         642145  2.800069e+12\n",
       "1  Covered Recipient Non-Physician Practitioner         355575  1.700011e+12\n",
       "2           Covered Recipient Teaching Hospital           2280  1.961271e+07"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample data by recipient type\n",
    "recipient_query = f\"\"\"\n",
    "SELECT \n",
    "    Covered_Recipient_Type,\n",
    "    COUNT(*) as payment_count,\n",
    "    SUM(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as total_amount\n",
    "FROM {database_name}.{table_name_parquet}\n",
    "GROUP BY Covered_Recipient_Type\n",
    "ORDER BY total_amount DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"Analyzing payments by recipient type...\")\n",
    "\n",
    "try:\n",
    "    df_recipients = wr.athena.read_sql_query(\n",
    "        sql=recipient_query,\n",
    "        database=database_name,\n",
    "        ctas_approach=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nQuery successful\")\n",
    "    print(f\"\\nPayments by Recipient Type:\")\n",
    "    display(df_recipients)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError querying data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf9fb6",
   "metadata": {},
   "source": [
    "## 9. Validation & Verification\n",
    "\n",
    "Perform final validation checks on the datalake setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae064097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATALAKE SETUP VALIDATION\n",
      "======================================================================\n",
      "\n",
      "1. S3 Storage:\n",
      "  cms-open-payments-light/raw/\n",
      "  cms-open-payments-light/parquet/\n",
      "\n",
      "2. Athena Database:\n",
      "Database 'cms_open_payments_light' exists\n",
      "\n",
      "3. Athena Tables:\n",
      "Table 'general_payments_csv' exists\n",
      "Table 'general_payments_parquet' exists\n",
      "\n",
      "4. Data Accessibility:\n",
      "Query successful (1,000,000 rows)\n",
      "\n",
      "======================================================================\n",
      "ALL VALIDATION CHECKS PASSED\n",
      "Datalake setup complete and operational\n",
      "======================================================================\n",
      "Stored 'setup_datalake_passed' (bool)\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive validation\n",
    "print(\"=\" * 70)\n",
    "print(\"DATALAKE SETUP VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "validation_passed = True\n",
    "\n",
    "# Check 1: S3 Buckets\n",
    "print(\"\\n1. S3 Storage:\")\n",
    "try:\n",
    "    for prefix in [raw_data_prefix, parquet_data_prefix]:\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=1)\n",
    "        if 'Contents' in response:\n",
    "            print(f\"  {prefix}/\")\n",
    "        else:\n",
    "            print(f\"  [FAIL]{prefix}/ (empty or missing)\")\n",
    "            validation_passed = False\n",
    "except Exception as e:\n",
    "    print(f\"Error checking S3: {e}\")\n",
    "    validation_passed = False\n",
    "\n",
    "# Check 2: Athena Database\n",
    "print(\"\\n2. Athena Database:\")\n",
    "try:\n",
    "    databases = pd.read_sql(\"SHOW DATABASES\", athena_conn)\n",
    "    if database_name in databases.values:\n",
    "        print(f\"Database '{database_name}' exists\")\n",
    "    else:\n",
    "        print(f\"Database '{database_name}' not found\")\n",
    "        validation_passed = False\n",
    "except Exception as e:\n",
    "    print(f\"Error checking database: {e}\")\n",
    "    validation_passed = False\n",
    "\n",
    "# Check 3: Tables\n",
    "print(\"\\n3. Athena Tables:\")\n",
    "try:\n",
    "    tables = pd.read_sql(f\"SHOW TABLES IN {database_name}\", athena_conn)\n",
    "    for table in [table_name_csv, table_name_parquet]:\n",
    "        if table in tables.values:\n",
    "            print(f\"Table '{table}' exists\")\n",
    "        else:\n",
    "            print(f\"Table '{table}' not found\")\n",
    "            validation_passed = False\n",
    "except Exception as e:\n",
    "    print(f\"Error checking tables: {e}\")\n",
    "    validation_passed = False\n",
    "\n",
    "# Check 4: Data Accessibility\n",
    "print(\"\\n4. Data Accessibility:\")\n",
    "try:\n",
    "    count_result = pd.read_sql(\n",
    "        f\"SELECT COUNT(*) as cnt FROM {database_name}.{table_name_parquet}\",\n",
    "        athena_conn\n",
    "    )\n",
    "    row_count = count_result['cnt'][0]\n",
    "    print(f\"Query successful ({row_count:,} rows)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error querying data: {e}\")\n",
    "    validation_passed = False\n",
    "\n",
    "# Final result\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "if validation_passed:\n",
    "    print(\"ALL VALIDATION CHECKS PASSED\")\n",
    "    print(\"Datalake setup complete and operational\")\n",
    "    setup_datalake_passed = True\n",
    "else:\n",
    "    print(\"SOME VALIDATION CHECKS FAILED\")\n",
    "    print(\"Please review the errors above and re-run failed steps\")\n",
    "    setup_datalake_passed = False\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Store validation result\n",
    "%store setup_datalake_passed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
