{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38cacf92",
   "metadata": {},
   "source": [
    "# CMS Open Payments Datalake Setup\n",
    "\n",
    "**Project:** AAI-540 Machine Learning Operations - Final Team Project  \n",
    "**Purpose:** Setup AWS S3 Datalake for CMS Open Payments Data  \n",
    "**Dataset:** CMS Open Payments Program Year 2024\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Environment Setup](#setup)\n",
    "2. [AWS Configuration & S3 Bucket Creation](#aws-config)\n",
    "3. [Download CMS Open Payments Data](#download)\n",
    "4. [Upload Data to S3](#upload)\n",
    "5. [Create Athena Database](#athena)\n",
    "6. [Register Data with Athena](#register)\n",
    "7. [Convert CSV to Parquet](#parquet)\n",
    "8. [Query Data with AWS Data Wrangler](#query)\n",
    "9. [Validation & Verification](#validation)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c0f2a8",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Install and import necessary libraries for AWS integration and data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47384732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.42.35-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-3.4.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting awswrangler\n",
      "  Using cached awswrangler-3.15.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting pyathena\n",
      "  Using cached pyathena-3.25.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting botocore<1.43.0,>=1.42.35 (from boto3)\n",
      "  Downloading botocore-1.42.35-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from boto3) (1.0.1)\n",
      "Collecting s3transfer<0.17.0,>=0.16.0 (from boto3)\n",
      "  Using cached s3transfer-0.16.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from botocore<1.43.0,>=1.42.35->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from botocore<1.43.0,>=1.42.35->boto3) (2.6.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.43.0,>=1.42.35->boto3) (1.17.0)\n",
      "Collecting sagemaker-core<3.0.0,>=2.4.0 (from sagemaker)\n",
      "  Using cached sagemaker_core-2.4.0-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting sagemaker-train<2.0.0,>=1.4.0 (from sagemaker)\n",
      "  Downloading sagemaker_train-1.4.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting sagemaker-serve<2.0.0,>=1.4.0 (from sagemaker)\n",
      "  Downloading sagemaker_serve-1.4.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting sagemaker-mlops<2.0.0,>=1.4.0 (from sagemaker)\n",
      "  Downloading sagemaker_mlops-1.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting pydantic<3.0.0,>=2.0.0 (from sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: PyYAML<7.0,>=6.0 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from sagemaker-core<3.0.0,>=2.4.0->sagemaker) (6.0.3)\n",
      "Requirement already satisfied: jsonschema<5.0.0 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from sagemaker-core<3.0.0,>=2.4.0->sagemaker) (4.25.1)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=4.0.0 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from sagemaker-core<3.0.0,>=2.4.0->sagemaker) (4.5.1)\n",
      "Collecting rich<14.0.0,>=13.0.0 (from sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting mock<5.0,>4.0 (from sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached mock-4.0.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting importlib-metadata<7.0,>=1.4.0 (from sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from sagemaker-core<3.0.0,>=2.4.0->sagemaker) (4.15.0)\n",
      "Requirement already satisfied: pytz>=2021.1 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from sagemaker-core<3.0.0,>=2.4.0->sagemaker) (2025.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.20.0 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from sagemaker-core<3.0.0,>=2.4.0->sagemaker) (2.32.5)\n",
      "Requirement already satisfied: attrs>=20.3.0 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from sagemaker-core<3.0.0,>=2.4.0->sagemaker) (25.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from sagemaker-core<3.0.0,>=2.4.0->sagemaker) (25.0)\n",
      "Collecting protobuf<7.0.0,>=3.12 (from sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached protobuf-6.33.4-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting pandas>=1.0.0 (from sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached pandas-3.0.0-cp314-cp314-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
      "Collecting numpy>=1.9.0 (from sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached numpy-2.4.1-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting smdebug_rulesconfig>=1.0.1 (from sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting schema>=0.7.5 (from sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached schema-0.7.8-py2.py3-none-any.whl.metadata (34 kB)\n",
      "Collecting omegaconf>=2.1.0 (from sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting torch>=1.9.0 (from sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached torch-2.10.0-cp314-cp314-manylinux_2_28_x86_64.whl.metadata (31 kB)\n",
      "Collecting scipy>=1.5.0 (from sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached scipy-1.17.0-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting cloudpickle>=2.0.0 (from sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting paramiko>=2.11.0 (from sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached paramiko-4.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tblib>=1.7.0 (from sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached tblib-3.2.2-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker) (3.23.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from jsonschema<5.0.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from jsonschema<5.0.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from jsonschema<5.0.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker) (0.30.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.0.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.0.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached pydantic_core-2.41.5-cp314-cp314-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.0.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from requests<3.0.0,>=2.20.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from requests<3.0.0,>=2.20.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from requests<3.0.0,>=2.20.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker) (2025.11.12)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14.0.0,>=13.0.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker) (2.19.2)\n",
      "Collecting deepdiff (from sagemaker-serve<2.0.0,>=1.4.0->sagemaker)\n",
      "  Using cached deepdiff-8.6.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting mlflow (from sagemaker-serve<2.0.0,>=1.4.0->sagemaker)\n",
      "  Using cached mlflow-3.8.1-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting sagemaker_schema_inference_artifacts (from sagemaker-serve<2.0.0,>=1.4.0->sagemaker)\n",
      "  Using cached sagemaker_schema_inference_artifacts-0.0.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pytest (from sagemaker-serve<2.0.0,>=1.4.0->sagemaker)\n",
      "  Using cached pytest-9.0.2-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting tqdm (from sagemaker-serve<2.0.0,>=1.4.0->sagemaker)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from sagemaker-serve<2.0.0,>=1.4.0->sagemaker) (7.1.3)\n",
      "Collecting tritonclient[http] (from sagemaker-serve<2.0.0,>=1.4.0->sagemaker)\n",
      "  Using cached tritonclient-2.64.0-py3-none-manylinux1_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting onnx (from sagemaker-serve<2.0.0,>=1.4.0->sagemaker)\n",
      "  Using cached onnx-1.20.1-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
      "INFO: pip is looking at multiple versions of sagemaker-serve to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting sagemaker\n",
      "  Using cached sagemaker-3.3.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sagemaker-serve<2.0.0,>=1.3.1 (from sagemaker)\n",
      "  Using cached sagemaker_serve-1.3.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting sagemaker\n",
      "  Using cached sagemaker-3.3.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sagemaker-serve<2.0.0,>=1.3.0 (from sagemaker)\n",
      "  Using cached sagemaker_serve-1.3.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting sagemaker\n",
      "  Using cached sagemaker-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "INFO: pip is still looking at multiple versions of sagemaker-serve to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting sagemaker-serve<2.0.0,>=1.2.0 (from sagemaker)\n",
      "  Using cached sagemaker_serve-1.2.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting sagemaker\n",
      "  Using cached sagemaker-3.1.1-py3-none-any.whl.metadata (21 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting sagemaker-serve<2.0.0,>=1.1.1 (from sagemaker)\n",
      "  Using cached sagemaker_serve-1.1.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting sagemaker\n",
      "  Using cached sagemaker-3.1.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sagemaker-serve<2.0.0,>=1.1.0 (from sagemaker)\n",
      "  Using cached sagemaker_serve-1.1.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting sagemaker\n",
      "  Using cached sagemaker-3.0.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting sagemaker-serve<2.0.0 (from sagemaker)\n",
      "  Using cached sagemaker_serve-1.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Using cached sagemaker_serve-0.1.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting sagemaker\n",
      "  Using cached sagemaker-3.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pandas>=1.0.0 (from sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached pandas-2.3.3-cp314-cp314-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Collecting pyarrow<23.0.0,>=8.0.0 (from awswrangler)\n",
      "  Using cached pyarrow-22.0.0-cp314-cp314-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from awswrangler) (80.9.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from pandas>=1.0.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker) (2025.3)\n",
      "Collecting fsspec (from pyathena)\n",
      "  Using cached fsspec-2026.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tenacity>=4.1.0 (from pyathena)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.1.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl\n",
      "Collecting bcrypt>=3.2 (from paramiko>=2.11.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Collecting cryptography>=3.3 (from paramiko>=2.11.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached cryptography-46.0.3-cp311-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting invoke>=2.0 (from paramiko>=2.11.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached invoke-2.2.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pynacl>=1.5 (from paramiko>=2.11.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached pynacl-1.6.2-cp38-abi3-manylinux_2_34_x86_64.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from cryptography>=3.3->paramiko>=2.11.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from cffi>=2.0.0->cryptography>=3.3->paramiko>=2.11.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker) (2.22)\n",
      "Collecting filelock (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker) (3.1.6)\n",
      "Collecting cuda-bindings==12.9.4 (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached cuda_bindings-12.9.4-cp314-cp314-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.6 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.4.5 (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached nvidia_nvshmem_cu12-3.4.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.6.0 (from torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached triton-3.6.0-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting cuda-pathfinder~=1.1 (from cuda-bindings==12.9.4->torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached cuda_pathfinder-1.3.3-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/sagemaker-recovery-mode/lib/python3.14/site-packages (from jinja2->torch>=1.9.0->sagemaker-core<3.0.0,>=2.4.0->sagemaker) (3.0.3)\n",
      "Downloading boto3-1.42.35-py3-none-any.whl (140 kB)\n",
      "Downloading botocore-1.42.35-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m222.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached s3transfer-0.16.0-py3-none-any.whl (86 kB)\n",
      "Using cached sagemaker-3.0-py3-none-any.whl (9.5 kB)\n",
      "Using cached sagemaker_core-2.4.0-py3-none-any.whl (1.3 MB)\n",
      "Using cached importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Using cached mock-4.0.3-py3-none-any.whl (28 kB)\n",
      "Using cached protobuf-6.33.4-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp314-cp314-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached awswrangler-3.15.0-py3-none-any.whl (373 kB)\n",
      "Using cached numpy-2.4.1-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
      "Using cached pandas-2.3.3-cp314-cp314-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.3 MB)\n",
      "Using cached pyarrow-22.0.0-cp314-cp314-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "Using cached pyathena-3.25.0-py3-none-any.whl (153 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached cloudpickle-3.1.2-py3-none-any.whl (22 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Using cached paramiko-4.0.0-py3-none-any.whl (223 kB)\n",
      "Using cached bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
      "Using cached cryptography-46.0.3-cp311-abi3-manylinux_2_34_x86_64.whl (4.5 MB)\n",
      "Using cached invoke-2.2.1-py3-none-any.whl (160 kB)\n",
      "Using cached pynacl-1.6.2-cp38-abi3-manylinux_2_34_x86_64.whl (1.4 MB)\n",
      "Using cached schema-0.7.8-py2.py3-none-any.whl (19 kB)\n",
      "Using cached scipy-1.17.0-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (35.0 MB)\n",
      "Using cached smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Using cached tblib-3.2.2-py3-none-any.whl (12 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached torch-2.10.0-cp314-cp314-manylinux_2_28_x86_64.whl (915.7 MB)\n",
      "Using cached cuda_bindings-12.9.4-cp314-cp314-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (11.9 MB)\n",
      "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "Using cached nvidia_nvshmem_cu12-3.4.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (139.1 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.6.0-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (188.3 MB)\n",
      "Using cached cuda_pathfinder-1.3.3-py3-none-any.whl (27 kB)\n",
      "Using cached fsspec-2026.1.0-py3-none-any.whl (201 kB)\n",
      "Using cached networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached filelock-3.20.3-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: schema, nvidia-cusparselt-cu12, mpmath, antlr4-python3-runtime, typing-inspection, triton, tenacity, tblib, sympy, smdebug_rulesconfig, pydantic-core, pyarrow, protobuf, omegaconf, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, mock, mdurl, invoke, importlib-metadata, fsspec, filelock, cuda-pathfinder, cloudpickle, bcrypt, annotated-types, scipy, pynacl, pydantic, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, markdown-it-py, cuda-bindings, cryptography, botocore, s3transfer, rich, paramiko, nvidia-cusolver-cu12, torch, boto3, sagemaker-core, pyathena, awswrangler, sagemaker\n",
      "\u001b[2K  Attempting uninstall: importlib-metadata[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28/57\u001b[0m [invoke]x]blas-cu12]u12]2]\n",
      "\u001b[2K    Found existing installation: importlib_metadata 8.7.0━━━━━\u001b[0m \u001b[32m28/57\u001b[0m [invoke]\n",
      "\u001b[2K    Uninstalling importlib_metadata-8.7.0:━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28/57\u001b[0m [invoke]\n",
      "\u001b[2K      Successfully uninstalled importlib_metadata-8.7.0━━━━━━━\u001b[0m \u001b[32m28/57\u001b[0m [invoke]\n",
      "\u001b[2K  Attempting uninstall: botocore━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m45/57\u001b[0m [cryptography]]]12]12]\n",
      "\u001b[2K    Found existing installation: botocore 1.40.70\u001b[90m━━━━━━━━\u001b[0m \u001b[32m45/57\u001b[0m [cryptography]\n",
      "\u001b[2K    Uninstalling botocore-1.40.70:━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m46/57\u001b[0m [botocore]\n",
      "\u001b[2K      Successfully uninstalled botocore-1.40.70[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m46/57\u001b[0m [botocore]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57/57\u001b[0m [sagemaker]57\u001b[0m [awswrangler]re]-cu12]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.25.2 requires botocore<1.40.71,>=1.40.46, but you have botocore 1.42.35 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.7.0 antlr4-python3-runtime-4.9.3 awswrangler-3.15.0 bcrypt-5.0.0 boto3-1.42.35 botocore-1.42.35 cloudpickle-3.1.2 cryptography-46.0.3 cuda-bindings-12.9.4 cuda-pathfinder-1.3.3 filelock-3.20.3 fsspec-2026.1.0 importlib-metadata-6.11.0 invoke-2.2.1 markdown-it-py-4.0.0 mdurl-0.1.2 mock-4.0.3 mpmath-1.3.0 networkx-3.6.1 numpy-2.4.1 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.4.5 nvidia-nvtx-cu12-12.8.90 omegaconf-2.3.0 pandas-2.3.3 paramiko-4.0.0 protobuf-6.33.4 pyarrow-22.0.0 pyathena-3.25.0 pydantic-2.12.5 pydantic-core-2.41.5 pynacl-1.6.2 rich-13.9.4 s3transfer-0.16.0 sagemaker-3.0 sagemaker-core-2.4.0 schema-0.7.8 scipy-1.17.0 smdebug_rulesconfig-1.0.1 sympy-1.14.0 tblib-3.2.2 tenacity-9.1.2 torch-2.10.0 triton-3.6.0 typing-inspection-0.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required AWS packages\n",
    "%pip install boto3 sagemaker awswrangler pyathena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a7a2337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from io import BytesIO, StringIO\n",
    "import awswrangler as wr\n",
    "from pyathena import connect\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c36822",
   "metadata": {},
   "source": [
    "## 2. AWS Configuration & S3 Bucket Creation\n",
    "\n",
    "Configure AWS session and create S3 bucket for the datalake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b497665a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS Configuration:\n",
      "  Region: us-east-1\n",
      "  Account ID: 864106638709\n",
      "  S3 Bucket: cmsopenpaymentsystems\n",
      "  Role: arn:aws:iam::864106638709:role/LabRole\n",
      "Bucket already exists: cmsopenpaymentsystems\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize AWS session\n",
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "\n",
    "# Get account information\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity().get('Account')\n",
    "\n",
    "# Initialize AWS clients\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "# Define bucket name\n",
    "bucket = f\"cmsopenpaymentsystems\"\n",
    "\n",
    "# Get IAM role (if needed)\n",
    "iam = boto3.client('iam')\n",
    "try:\n",
    "    role = iam.get_role(RoleName='LabRole')['Role']['Arn']\n",
    "except:\n",
    "    role = \"Role not found\"\n",
    "\n",
    "print(f\"AWS Configuration:\")\n",
    "print(f\"  Region: {region}\")\n",
    "print(f\"  Account ID: {account_id}\")\n",
    "print(f\"  S3 Bucket: {bucket}\")\n",
    "print(f\"  Role: {role}\")\n",
    "\n",
    "# Check if S3 bucket exists\n",
    "def check_bucket_exists(bucket_name):\n",
    "    \"\"\"Check if bucket exists and is accessible\"\"\"\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def ensure_bucket_exists(bucket_name, region):\n",
    "    \"\"\"Create bucket only if it doesn't exist\"\"\"\n",
    "    try:\n",
    "        if check_bucket_exists(bucket_name):\n",
    "            print(f\"Bucket already exists: {bucket_name}\")\n",
    "            return True\n",
    "        \n",
    "        # Create bucket if it doesn't exist\n",
    "        if region == 'us-east-1':\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "        print(f\"Created new bucket: {bucket_name}\")\n",
    "        return True\n",
    "    except s3_client.exceptions.BucketAlreadyOwnedByYou:\n",
    "        print(f\"Bucket already exists: {bucket_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error with bucket {bucket_name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Ensure bucket exists\n",
    "bucket_exists = ensure_bucket_exists(bucket, region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aacede33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Data Paths:\n",
      "  Raw Data: s3://cmsopenpaymentsystems/cms-open-payments/raw\n",
      "  Processed Data: s3://cmsopenpaymentsystems/cms-open-payments/processed\n",
      "  Parquet Data: s3://cmsopenpaymentsystems/cms-open-payments/parquet\n",
      "\n",
      "Data Status:\n",
      "  Raw Data in S3: EXISTS\n",
      "  Parquet Data in S3: EXISTS\n",
      "\n",
      "Data already ingested. You can skip to Step 5 (Create Athena Database)\n",
      "Stored 'bucket' (str)\n",
      "Stored 'region' (str)\n",
      "Stored 's3_raw_path' (str)\n",
      "Stored 's3_processed_path' (str)\n",
      "Stored 's3_parquet_path' (str)\n",
      "Stored 'raw_data_exists' (bool)\n",
      "Stored 'parquet_data_exists' (bool)\n",
      "Stored 'skip_ingestion' (bool)\n"
     ]
    }
   ],
   "source": [
    "# Define S3 paths for CMS data\n",
    "cms_data_prefix = \"cms-open-payments\"\n",
    "raw_data_prefix = f\"{cms_data_prefix}/raw\"\n",
    "processed_data_prefix = f\"{cms_data_prefix}/processed\"\n",
    "parquet_data_prefix = f\"{cms_data_prefix}/parquet\"\n",
    "\n",
    "s3_raw_path = f\"s3://{bucket}/{raw_data_prefix}\"\n",
    "s3_processed_path = f\"s3://{bucket}/{processed_data_prefix}\"\n",
    "s3_parquet_path = f\"s3://{bucket}/{parquet_data_prefix}\"\n",
    "\n",
    "print(f\"S3 Data Paths:\")\n",
    "print(f\"  Raw Data: {s3_raw_path}\")\n",
    "print(f\"  Processed Data: {s3_processed_path}\")\n",
    "print(f\"  Parquet Data: {s3_parquet_path}\")\n",
    "\n",
    "# Check if data already exists in S3\n",
    "def check_s3_data_exists(bucket_name, prefix):\n",
    "    \"\"\"Check if data exists in S3 path\"\"\"\n",
    "    try:\n",
    "        response = s3_client.list_objects_v2(\n",
    "            Bucket=bucket_name,\n",
    "            Prefix=prefix,\n",
    "            MaxKeys=1\n",
    "        )\n",
    "        return 'Contents' in response and len(response['Contents']) > 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking S3 path {prefix}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Check for existing data\n",
    "raw_data_exists = check_s3_data_exists(bucket, raw_data_prefix)\n",
    "parquet_data_exists = check_s3_data_exists(bucket, parquet_data_prefix)\n",
    "\n",
    "print(f\"\\nData Status:\")\n",
    "print(f\"  Raw Data in S3: {'EXISTS' if raw_data_exists else 'NOT FOUND'}\")\n",
    "print(f\"  Parquet Data in S3: {'EXISTS' if parquet_data_exists else 'NOT FOUND'}\")\n",
    "\n",
    "if raw_data_exists and parquet_data_exists:\n",
    "    print(f\"\\nData already ingested. You can skip to Step 5 (Create Athena Database)\")\n",
    "    skip_ingestion = True\n",
    "elif raw_data_exists:\n",
    "    print(f\"\\nRaw data exists. You may skip to Step 7 (Convert to Parquet)\")\n",
    "    skip_ingestion = True\n",
    "else:\n",
    "    print(f\"\\nData ingestion required. Continue with Steps 3-4.\")\n",
    "    skip_ingestion = False\n",
    "\n",
    "# Store paths and flags for use in other notebooks\n",
    "%store bucket\n",
    "%store region\n",
    "%store s3_raw_path\n",
    "%store s3_processed_path\n",
    "%store s3_parquet_path\n",
    "%store raw_data_exists\n",
    "%store parquet_data_exists\n",
    "%store skip_ingestion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c3ff86",
   "metadata": {},
   "source": [
    "## 3. Download CMS Open Payments Data\n",
    "\n",
    "Download the CMS Open Payments Program Year 2024 General Payments dataset.\n",
    "\n",
    "**Data Source:** CMS Open Payments  \n",
    "**Dataset:** Program Year 2024 General Payments  \n",
    "**Published:** June 30, 2025  \n",
    "**Coverage:** January 1, 2024 - December 31, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0baf9258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping download - data already exists in S3\n",
      "  S3 Path: s3://cmsopenpaymentsystems/cms-open-payments/raw\n"
     ]
    }
   ],
   "source": [
    "# CMS Open Payments data URL - Direct CSV download\n",
    "cms_data_url = \"https://download.cms.gov/openpayments/PGYR2024_P06302025_06162025/OP_DTL_GNRL_PGYR2024_P06302025_06162025.csv\"\n",
    "\n",
    "# Alternative: If the above URL doesn't work, use this approach:\n",
    "# 1. Go to https://openpaymentsdata.cms.gov/datasets\n",
    "# 2. Select \"Program Year 2024\" and \"General Payments\"\n",
    "# 3. Download the CSV file manually and place it in ../data/ directory\n",
    "\n",
    "if skip_ingestion and raw_data_exists:\n",
    "    print(f\"Skipping download - data already exists in S3\")\n",
    "    print(f\"  S3 Path: {s3_raw_path}\")\n",
    "else:\n",
    "    print(f\"CMS Data URL: {cms_data_url}\")\n",
    "    print(f\"\\nNote: This dataset is approximately 3-4 GB.\")\n",
    "    print(f\"Download may take several minutes depending on your connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92c50632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping download - data already exists in S3\n",
      "  S3 Path: s3://cmsopenpaymentsystems/cms-open-payments/raw\n"
     ]
    }
   ],
   "source": [
    "if skip_ingestion and raw_data_exists:\n",
    "    print(f\"Skipping download - data already exists in S3\")\n",
    "    print(f\"  S3 Path: {s3_raw_path}\")\n",
    "else:\n",
    "    # Create local data directory if it doesn't exist\n",
    "    local_data_dir = Path(\"../data\")\n",
    "    local_data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Local CSV file path\n",
    "    local_csv_file = local_data_dir / \"OP_DTL_GNRL_PGYR2024_P06302025_06162025.csv\"\n",
    "\n",
    "    print(f\"Local data directory: {local_data_dir.absolute()}\")\n",
    "    print(f\"Target CSV file: {local_csv_file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b07991ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping download - data already exists in S3\n",
      "  S3 Path: s3://cmsopenpaymentsystems/cms-open-payments/raw\n",
      "  Local file also present: ../data/OP_DTL_GNRL_PGYR2024_P06302025_06162025.csv\n",
      "  File size: 8.22 GB\n"
     ]
    }
   ],
   "source": [
    "# Download CMS data if not already present\n",
    "if skip_ingestion and raw_data_exists:\n",
    "    print(f\"Skipping download - data already exists in S3\")\n",
    "    print(f\"  S3 Path: {s3_raw_path}\")\n",
    "    \n",
    "    # Check local file for reference\n",
    "    if local_csv_file.exists():\n",
    "        print(f\"  Local file also present: {local_csv_file}\")\n",
    "        print(f\"  File size: {local_csv_file.stat().st_size / (1024**3):.2f} GB\")\n",
    "    else:\n",
    "        print(f\"  Note: Local file not present, but S3 data is available\")\n",
    "\n",
    "elif local_csv_file.exists():\n",
    "    print(f\"CSV file already exists locally: {local_csv_file}\")\n",
    "    print(f\"  File size: {local_csv_file.stat().st_size / (1024**3):.2f} GB\")\n",
    "    print(f\"  Skipping download\")\n",
    "else:\n",
    "    print(f\"Downloading CMS Open Payments data...\")\n",
    "    print(f\"  This may take 10-20 minutes depending on your connection.\")\n",
    "    \n",
    "    try:\n",
    "        # Download CSV file with progress indication\n",
    "        response = requests.get(cms_data_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        print(f\"  Total download size: {total_size / (1024**3):.2f} GB\")\n",
    "        \n",
    "        # Save CSV file directly\n",
    "        with open(local_csv_file, 'wb') as f:\n",
    "            downloaded = 0\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    downloaded += len(chunk)\n",
    "                    if total_size > 0:\n",
    "                        percent = (downloaded / total_size) * 100\n",
    "                        print(f\"\\rProgress: {percent:.1f}%\", end=\"\")\n",
    "        \n",
    "        print(f\"\\nDownload complete: {local_csv_file}\")\n",
    "        print(f\"  File size: {local_csv_file.stat().st_size / (1024**3):.2f} GB\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError downloading data: {e}\")\n",
    "        print(f\"\\nAlternative approach:\")\n",
    "        print(f\"1. Visit: https://openpaymentsdata.cms.gov/datasets\")\n",
    "        print(f\"2. Select 'Program Year 2024' and 'General Payments'\")\n",
    "        print(f\"3. Download CSV and save to: {local_data_dir.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c67f2b8",
   "metadata": {},
   "source": [
    "## 4. Upload Data to S3 upload\n",
    "\n",
    "Upload the downloaded CMS data to S3 for datalake storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "905d1a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping preview - data already in S3\n",
      "  Run queries in Step 6 to view data\n"
     ]
    }
   ],
   "source": [
    "# Preview the data before upload\n",
    "if skip_ingestion and raw_data_exists:\n",
    "    print(\"Skipping preview - data already in S3\")\n",
    "    print(f\"  Run queries in Step 6 to view data\")\n",
    "elif local_csv_file.exists():\n",
    "    print(\"Loading sample of data for preview...\")\n",
    "    df_sample = pd.read_csv(local_csv_file, nrows=5)\n",
    "\n",
    "    print(f\"\\nDataset Preview:\")\n",
    "    print(f\"  Columns: {len(df_sample.columns)}\")\n",
    "    print(f\"  Sample rows:\")\n",
    "    display(df_sample.head())\n",
    "\n",
    "    print(f\"\\nColumn names:\")\n",
    "    for i, col in enumerate(df_sample.columns, 1):\n",
    "        print(f\"  {i}. {col}\")\n",
    "else:\n",
    "    print(\"Local CSV file not found. Cannot preview data.\")\n",
    "    print(f\"  Expected location: {local_csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "753fa4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping upload - data already exists in S3\n",
      "  S3 URI: s3://cmsopenpaymentsystems/cms-open-payments/raw/OP_DTL_GNRL_PGYR2024_P06302025_06162025.csv\n",
      "Stored 's3_raw_file_path' (str)\n"
     ]
    }
   ],
   "source": [
    "# Upload raw CSV to S3\n",
    "s3_raw_file_path = f\"{s3_raw_path}/{local_csv_file.name}\"\n",
    "\n",
    "if skip_ingestion and raw_data_exists:\n",
    "    print(f\"Skipping upload - data already exists in S3\")\n",
    "    print(f\"  S3 URI: {s3_raw_file_path}\")\n",
    "    \n",
    "    # Store the S3 file path\n",
    "    %store s3_raw_file_path\n",
    "    \n",
    "elif not local_csv_file.exists():\n",
    "    print(f\"Cannot upload - local CSV file not found\")\n",
    "    print(f\"  Expected location: {local_csv_file}\")\n",
    "    print(f\"  Please download the file first (Step 3) or data already exists in S3\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Uploading data to S3...\")\n",
    "    print(f\"  Source: {local_csv_file}\")\n",
    "    print(f\"  Destination: {s3_raw_path}/\")\n",
    "\n",
    "    try:\n",
    "        # Upload file with progress callback\n",
    "        file_size = local_csv_file.stat().st_size\n",
    "        \n",
    "        def upload_progress(bytes_uploaded):\n",
    "            percent = (bytes_uploaded / file_size) * 100\n",
    "            print(f\"\\rUpload progress: {percent:.1f}%\", end=\"\")\n",
    "        \n",
    "        s3_client.upload_file(\n",
    "            str(local_csv_file),\n",
    "            bucket,\n",
    "            f\"{raw_data_prefix}/{local_csv_file.name}\",\n",
    "            Callback=upload_progress\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nUpload complete\")\n",
    "        print(f\"  S3 URI: {s3_raw_file_path}\")\n",
    "        \n",
    "        # Store the S3 file path\n",
    "        %store s3_raw_file_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError uploading to S3: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "998fbe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying S3 upload...\n",
      "\n",
      "Files in S3 bucket:\n",
      "  cms-open-payments/raw/OP_DTL_GNRL_PGYR2024_P06302025_06162025.csv (8.22 GB)\n"
     ]
    }
   ],
   "source": [
    "# Verify upload\n",
    "print(\"Verifying S3 upload...\")\n",
    "\n",
    "response = s3_client.list_objects_v2(\n",
    "    Bucket=bucket,\n",
    "    Prefix=raw_data_prefix\n",
    ")\n",
    "\n",
    "if 'Contents' in response:\n",
    "    print(f\"\\nFiles in S3 bucket:\")\n",
    "    for obj in response['Contents']:\n",
    "        size_gb = obj['Size'] / (1024**3)\n",
    "        print(f\"  {obj['Key']} ({size_gb:.2f} GB)\")\n",
    "else:\n",
    "    print(f\"\\nNo files found in S3 bucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667e90c0",
   "metadata": {},
   "source": [
    "## 5. Create Athena Database\n",
    "\n",
    "Create an Amazon Athena database for querying CMS data using SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9701a647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Athena Configuration:\n",
      "  Database: cms_open_payments\n",
      "  Staging Directory: s3://cmsopenpaymentsystems/athena/staging\n",
      "Stored 'database_name' (str)\n",
      "Stored 's3_athena_staging' (str)\n"
     ]
    }
   ],
   "source": [
    "# Define Athena database name\n",
    "database_name = \"cms_open_payments\"\n",
    "\n",
    "# Set S3 staging directory for Athena queries\n",
    "s3_athena_staging = f\"s3://{bucket}/athena/staging\"\n",
    "\n",
    "print(f\"Athena Configuration:\")\n",
    "print(f\"  Database: {database_name}\")\n",
    "print(f\"  Staging Directory: {s3_athena_staging}\")\n",
    "\n",
    "# Store for use in other notebooks\n",
    "%store database_name\n",
    "%store s3_athena_staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf19b172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Athena connection established\n"
     ]
    }
   ],
   "source": [
    "# Create Athena connection\n",
    "athena_conn = connect(\n",
    "    region_name=region,\n",
    "    s3_staging_dir=s3_athena_staging\n",
    ")\n",
    "\n",
    "print(\"Athena connection established\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f70fd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Athena database...\n",
      "  Query: CREATE DATABASE IF NOT EXISTS cms_open_payments\n",
      "Database created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create database\n",
    "create_db_query = f\"CREATE DATABASE IF NOT EXISTS {database_name}\"\n",
    "\n",
    "print(f\"Creating Athena database...\")\n",
    "print(f\"  Query: {create_db_query}\")\n",
    "\n",
    "try:\n",
    "    result = pd.read_sql(create_db_query, athena_conn)\n",
    "    print(f\"Database created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating database: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "703a47ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying database creation...\n",
      "\n",
      " Available Databases:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cms_open_payments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dsoaws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sagemaker_featurestore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            database_name\n",
       "0       cms_open_payments\n",
       "1                 default\n",
       "2                  dsoaws\n",
       "3  sagemaker_featurestore"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Database 'cms_open_payments' exists\n"
     ]
    }
   ],
   "source": [
    "# Verify database creation\n",
    "show_db_query = \"SHOW DATABASES\"\n",
    "\n",
    "print(\"Verifying database creation...\")\n",
    "databases = pd.read_sql(show_db_query, athena_conn)\n",
    "\n",
    "print(f\"\\n Available Databases:\")\n",
    "display(databases)\n",
    "\n",
    "if database_name in databases.values:\n",
    "    print(f\"\\nDatabase '{database_name}' exists\")\n",
    "else:\n",
    "    print(f\"\\nDatabase '{database_name}' not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c0345",
   "metadata": {},
   "source": [
    "## 6. Register Data with Athena\n",
    "\n",
    "Create an external table in Athena to query the CSV data stored in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "552735e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Configuration:\n",
      "  Database: cms_open_payments\n",
      "  Table: general_payments_csv\n",
      "  Location: s3://cmsopenpaymentsystems/cms-open-payments/raw/\n",
      "Stored 'table_name_csv' (str)\n"
     ]
    }
   ],
   "source": [
    "# Define table name\n",
    "table_name_csv = \"general_payments_csv\"\n",
    "\n",
    "print(f\"Table Configuration:\")\n",
    "print(f\"  Database: {database_name}\")\n",
    "print(f\"  Table: {table_name_csv}\")\n",
    "print(f\"  Location: {s3_raw_path}/\")\n",
    "\n",
    "%store table_name_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac3e2638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema preview (first 10 columns):\n",
      "  1. `Change_Type` STRING\n",
      "  2. `Covered_Recipient_Type` STRING\n",
      "  3. `Teaching_Hospital_CCN` BIGINT\n",
      "  4. `Teaching_Hospital_ID` BIGINT\n",
      "  5. `Teaching_Hospital_Name` STRING\n",
      "  6. `Covered_Recipient_Profile_ID` DOUBLE\n",
      "  7. `Covered_Recipient_NPI` DOUBLE\n",
      "  8. `Covered_Recipient_First_Name` DOUBLE\n",
      "  9. `Covered_Recipient_Middle_Name` DOUBLE\n",
      "  10. `Covered_Recipient_Last_Name` DOUBLE\n",
      "  ... (91 columns total)\n"
     ]
    }
   ],
   "source": [
    "# Get actual column names from the CSV\n",
    "df_schema = pd.read_csv(local_csv_file, nrows=1)\n",
    "\n",
    "# Create column definitions for Athena\n",
    "# Map pandas dtypes to Athena types\n",
    "def get_athena_type(dtype):\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return 'BIGINT'\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return 'DOUBLE'\n",
    "    elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "        return 'TIMESTAMP'\n",
    "    else:\n",
    "        return 'STRING'\n",
    "\n",
    "# Create column definitions\n",
    "columns_def = []\n",
    "for col in df_schema.columns:\n",
    "    # Clean column name for Athena (replace spaces and special chars)\n",
    "    clean_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_')\n",
    "    athena_type = get_athena_type(df_schema[col].dtype)\n",
    "    columns_def.append(f\"`{col}` {athena_type}\")\n",
    "\n",
    "columns_str = ',\\n    '.join(columns_def)\n",
    "\n",
    "print(f\"Schema preview (first 10 columns):\")\n",
    "for i, col_def in enumerate(columns_def[:10], 1):\n",
    "    print(f\"  {i}. {col_def}\")\n",
    "print(f\"  ... ({len(columns_def)} columns total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dd87b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating external table...\n",
      "\n",
      "Query preview:\n",
      "\n",
      "CREATE EXTERNAL TABLE IF NOT EXISTS cms_open_payments.general_payments_csv (\n",
      "    `Change_Type` STRING,\n",
      "    `Covered_Recipient_Type` STRING,\n",
      "    `Teaching_Hospital_CCN` BIGINT,\n",
      "    `Teaching_Hospital_ID` BIGINT,\n",
      "    `Teaching_Hospital_Name` STRING,\n",
      "    `Covered_Recipient_Profile_ID` DOUBLE,\n",
      "    `Covered_Recipient_NPI` DOUBLE,\n",
      "    `Covered_Recipient_First_Name` DOUBLE,\n",
      "    `Covered_Recipient_Middle_Name` DOUBLE,\n",
      "    `Covered_Recipient_Last_Name` DOUBLE,\n",
      "    `Covered_Recipient_Name_Suffix` DOUBLE,...\n",
      "\n",
      "Table 'general_payments_csv' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create external table for CSV data\n",
    "create_table_query = f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {database_name}.{table_name_csv} (\n",
    "    {columns_str}\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\\\n'\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '{s3_raw_path}/'\n",
    "TBLPROPERTIES (\n",
    "    'skip.header.line.count'='1',\n",
    "    'serialization.null.format'=''\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Creating external table...\")\n",
    "print(f\"\\nQuery preview:\")\n",
    "print(create_table_query[:500] + \"...\")\n",
    "\n",
    "try:\n",
    "    result = pd.read_sql(create_table_query, athena_conn)\n",
    "    print(f\"\\nTable '{table_name_csv}' created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError creating table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3435350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying table creation...\n",
      "\n",
      "Tables in database 'cms_open_payments':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tab_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>general_payments_csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>general_payments_parquet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   tab_name\n",
       "0      general_payments_csv\n",
       "1  general_payments_parquet"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table 'general_payments_csv' exists\n"
     ]
    }
   ],
   "source": [
    "# Verify table creation\n",
    "show_tables_query = f\"SHOW TABLES IN {database_name}\"\n",
    "\n",
    "print(\"Verifying table creation...\")\n",
    "tables = pd.read_sql(show_tables_query, athena_conn)\n",
    "\n",
    "print(f\"\\nTables in database '{database_name}':\")\n",
    "display(tables)\n",
    "\n",
    "if table_name_csv in tables.values:\n",
    "    print(f\"\\nTable '{table_name_csv}' exists\")\n",
    "else:\n",
    "    print(f\"\\nTable '{table_name_csv}' not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf0c229f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing table access...\n",
      "Query: \n",
      "SELECT COUNT(*) as row_count\n",
      "FROM cms_open_payments.general_payments_csv\n",
      "\n",
      "\n",
      "Query successful\n",
      "  Total rows: 15,397,627\n"
     ]
    }
   ],
   "source": [
    "# Test query - count rows\n",
    "count_query = f\"\"\"\n",
    "SELECT COUNT(*) as row_count\n",
    "FROM {database_name}.{table_name_csv}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Testing table access...\")\n",
    "print(f\"Query: {count_query}\")\n",
    "\n",
    "try:\n",
    "    result = pd.read_sql(count_query, athena_conn)\n",
    "    print(f\"\\nQuery successful\")\n",
    "    print(f\"  Total rows: {result['row_count'][0]:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError querying table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e153c8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching sample data...\n",
      "\n",
      "Sample data retrieved\n",
      "  Shape: (5, 91)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>change_type</th>\n",
       "      <th>covered_recipient_type</th>\n",
       "      <th>teaching_hospital_ccn</th>\n",
       "      <th>teaching_hospital_id</th>\n",
       "      <th>teaching_hospital_name</th>\n",
       "      <th>covered_recipient_profile_id</th>\n",
       "      <th>covered_recipient_npi</th>\n",
       "      <th>covered_recipient_first_name</th>\n",
       "      <th>covered_recipient_middle_name</th>\n",
       "      <th>covered_recipient_last_name</th>\n",
       "      <th>covered_recipient_name_suffix</th>\n",
       "      <th>recipient_primary_business_street_address_line1</th>\n",
       "      <th>recipient_primary_business_street_address_line2</th>\n",
       "      <th>recipient_city</th>\n",
       "      <th>recipient_state</th>\n",
       "      <th>recipient_zip_code</th>\n",
       "      <th>recipient_country</th>\n",
       "      <th>recipient_province</th>\n",
       "      <th>recipient_postal_code</th>\n",
       "      <th>covered_recipient_primary_type_1</th>\n",
       "      <th>covered_recipient_primary_type_2</th>\n",
       "      <th>covered_recipient_primary_type_3</th>\n",
       "      <th>covered_recipient_primary_type_4</th>\n",
       "      <th>covered_recipient_primary_type_5</th>\n",
       "      <th>covered_recipient_primary_type_6</th>\n",
       "      <th>...</th>\n",
       "      <th>indicate_drug_or_biological_or_device_or_medical_supply_2</th>\n",
       "      <th>product_category_or_therapeutic_area_2</th>\n",
       "      <th>name_of_drug_or_biological_or_device_or_medical_supply_2</th>\n",
       "      <th>associated_drug_or_biological_ndc_2</th>\n",
       "      <th>associated_device_or_medical_supply_pdi_2</th>\n",
       "      <th>covered_or_noncovered_indicator_3</th>\n",
       "      <th>indicate_drug_or_biological_or_device_or_medical_supply_3</th>\n",
       "      <th>product_category_or_therapeutic_area_3</th>\n",
       "      <th>name_of_drug_or_biological_or_device_or_medical_supply_3</th>\n",
       "      <th>associated_drug_or_biological_ndc_3</th>\n",
       "      <th>associated_device_or_medical_supply_pdi_3</th>\n",
       "      <th>covered_or_noncovered_indicator_4</th>\n",
       "      <th>indicate_drug_or_biological_or_device_or_medical_supply_4</th>\n",
       "      <th>product_category_or_therapeutic_area_4</th>\n",
       "      <th>name_of_drug_or_biological_or_device_or_medical_supply_4</th>\n",
       "      <th>associated_drug_or_biological_ndc_4</th>\n",
       "      <th>associated_device_or_medical_supply_pdi_4</th>\n",
       "      <th>covered_or_noncovered_indicator_5</th>\n",
       "      <th>indicate_drug_or_biological_or_device_or_medical_supply_5</th>\n",
       "      <th>product_category_or_therapeutic_area_5</th>\n",
       "      <th>name_of_drug_or_biological_or_device_or_medical_supply_5</th>\n",
       "      <th>associated_drug_or_biological_ndc_5</th>\n",
       "      <th>associated_device_or_medical_supply_pdi_5</th>\n",
       "      <th>program_year</th>\n",
       "      <th>payment_publication_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEW</td>\n",
       "      <td>Covered Recipient Physician</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>168864.0</td>\n",
       "      <td>1.730134e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1650 SKYLYN DR STE 220</td>\n",
       "      <td>None</td>\n",
       "      <td>SPARTANBURG</td>\n",
       "      <td>SC</td>\n",
       "      <td>None</td>\n",
       "      <td>United States</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2024</td>\n",
       "      <td>06/30/2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NEW</td>\n",
       "      <td>Covered Recipient Physician</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>168864.0</td>\n",
       "      <td>1.730134e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1650 SKYLYN DR STE 220</td>\n",
       "      <td>None</td>\n",
       "      <td>SPARTANBURG</td>\n",
       "      <td>SC</td>\n",
       "      <td>None</td>\n",
       "      <td>United States</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2024</td>\n",
       "      <td>06/30/2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEW</td>\n",
       "      <td>Covered Recipient Physician</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>168864.0</td>\n",
       "      <td>1.730134e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1650 SKYLYN DR STE 220</td>\n",
       "      <td>None</td>\n",
       "      <td>SPARTANBURG</td>\n",
       "      <td>SC</td>\n",
       "      <td>None</td>\n",
       "      <td>United States</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2024</td>\n",
       "      <td>06/30/2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEW</td>\n",
       "      <td>Covered Recipient Physician</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>8805309.0</td>\n",
       "      <td>1.235592e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>8390 LYNDON B JOHNSON FWY STE 1000</td>\n",
       "      <td>None</td>\n",
       "      <td>DALLAS</td>\n",
       "      <td>TX</td>\n",
       "      <td>None</td>\n",
       "      <td>United States</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2024</td>\n",
       "      <td>06/30/2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEW</td>\n",
       "      <td>Covered Recipient Physician</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>8805309.0</td>\n",
       "      <td>1.235592e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>8390 LYNDON B JOHNSON FWY STE 1000</td>\n",
       "      <td>None</td>\n",
       "      <td>DALLAS</td>\n",
       "      <td>TX</td>\n",
       "      <td>None</td>\n",
       "      <td>United States</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2024</td>\n",
       "      <td>06/30/2025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  change_type       covered_recipient_type teaching_hospital_ccn  \\\n",
       "0         NEW  Covered Recipient Physician                  None   \n",
       "1         NEW  Covered Recipient Physician                  None   \n",
       "2         NEW  Covered Recipient Physician                  None   \n",
       "3         NEW  Covered Recipient Physician                  None   \n",
       "4         NEW  Covered Recipient Physician                  None   \n",
       "\n",
       "  teaching_hospital_id teaching_hospital_name  covered_recipient_profile_id  \\\n",
       "0                 None                   None                      168864.0   \n",
       "1                 None                   None                      168864.0   \n",
       "2                 None                   None                      168864.0   \n",
       "3                 None                   None                     8805309.0   \n",
       "4                 None                   None                     8805309.0   \n",
       "\n",
       "   covered_recipient_npi covered_recipient_first_name  \\\n",
       "0           1.730134e+09                         None   \n",
       "1           1.730134e+09                         None   \n",
       "2           1.730134e+09                         None   \n",
       "3           1.235592e+09                         None   \n",
       "4           1.235592e+09                         None   \n",
       "\n",
       "  covered_recipient_middle_name covered_recipient_last_name  \\\n",
       "0                          None                        None   \n",
       "1                          None                        None   \n",
       "2                          None                        None   \n",
       "3                          None                        None   \n",
       "4                          None                        None   \n",
       "\n",
       "  covered_recipient_name_suffix  \\\n",
       "0                          None   \n",
       "1                          None   \n",
       "2                          None   \n",
       "3                          None   \n",
       "4                          None   \n",
       "\n",
       "  recipient_primary_business_street_address_line1  \\\n",
       "0                          1650 SKYLYN DR STE 220   \n",
       "1                          1650 SKYLYN DR STE 220   \n",
       "2                          1650 SKYLYN DR STE 220   \n",
       "3              8390 LYNDON B JOHNSON FWY STE 1000   \n",
       "4              8390 LYNDON B JOHNSON FWY STE 1000   \n",
       "\n",
       "  recipient_primary_business_street_address_line2 recipient_city  \\\n",
       "0                                            None    SPARTANBURG   \n",
       "1                                            None    SPARTANBURG   \n",
       "2                                            None    SPARTANBURG   \n",
       "3                                            None         DALLAS   \n",
       "4                                            None         DALLAS   \n",
       "\n",
       "  recipient_state recipient_zip_code recipient_country recipient_province  \\\n",
       "0              SC               None     United States               None   \n",
       "1              SC               None     United States               None   \n",
       "2              SC               None     United States               None   \n",
       "3              TX               None     United States               None   \n",
       "4              TX               None     United States               None   \n",
       "\n",
       "  recipient_postal_code covered_recipient_primary_type_1  \\\n",
       "0                  None                             None   \n",
       "1                  None                             None   \n",
       "2                  None                             None   \n",
       "3                  None                             None   \n",
       "4                  None                             None   \n",
       "\n",
       "  covered_recipient_primary_type_2 covered_recipient_primary_type_3  \\\n",
       "0                             None                             None   \n",
       "1                             None                             None   \n",
       "2                             None                             None   \n",
       "3                             None                             None   \n",
       "4                             None                             None   \n",
       "\n",
       "  covered_recipient_primary_type_4 covered_recipient_primary_type_5  \\\n",
       "0                             None                             None   \n",
       "1                             None                             None   \n",
       "2                             None                             None   \n",
       "3                             None                             None   \n",
       "4                             None                             None   \n",
       "\n",
       "  covered_recipient_primary_type_6  ...  \\\n",
       "0                             None  ...   \n",
       "1                             None  ...   \n",
       "2                             None  ...   \n",
       "3                             None  ...   \n",
       "4                             None  ...   \n",
       "\n",
       "  indicate_drug_or_biological_or_device_or_medical_supply_2  \\\n",
       "0                                               None          \n",
       "1                                               None          \n",
       "2                                               None          \n",
       "3                                               None          \n",
       "4                                               None          \n",
       "\n",
       "  product_category_or_therapeutic_area_2  \\\n",
       "0                                   None   \n",
       "1                                   None   \n",
       "2                                   None   \n",
       "3                                   None   \n",
       "4                                   None   \n",
       "\n",
       "  name_of_drug_or_biological_or_device_or_medical_supply_2  \\\n",
       "0                                               None         \n",
       "1                                               None         \n",
       "2                                               None         \n",
       "3                                               None         \n",
       "4                                               None         \n",
       "\n",
       "  associated_drug_or_biological_ndc_2  \\\n",
       "0                                None   \n",
       "1                                None   \n",
       "2                                None   \n",
       "3                                None   \n",
       "4                                None   \n",
       "\n",
       "  associated_device_or_medical_supply_pdi_2 covered_or_noncovered_indicator_3  \\\n",
       "0                                      None                              None   \n",
       "1                                      None                              None   \n",
       "2                                      None                              None   \n",
       "3                                      None                              None   \n",
       "4                                      None                              None   \n",
       "\n",
       "  indicate_drug_or_biological_or_device_or_medical_supply_3  \\\n",
       "0                                               None          \n",
       "1                                               None          \n",
       "2                                               None          \n",
       "3                                               None          \n",
       "4                                               None          \n",
       "\n",
       "  product_category_or_therapeutic_area_3  \\\n",
       "0                                   None   \n",
       "1                                   None   \n",
       "2                                   None   \n",
       "3                                   None   \n",
       "4                                   None   \n",
       "\n",
       "  name_of_drug_or_biological_or_device_or_medical_supply_3  \\\n",
       "0                                               None         \n",
       "1                                               None         \n",
       "2                                               None         \n",
       "3                                               None         \n",
       "4                                               None         \n",
       "\n",
       "  associated_drug_or_biological_ndc_3  \\\n",
       "0                                None   \n",
       "1                                None   \n",
       "2                                None   \n",
       "3                                None   \n",
       "4                                None   \n",
       "\n",
       "  associated_device_or_medical_supply_pdi_3 covered_or_noncovered_indicator_4  \\\n",
       "0                                      None                              None   \n",
       "1                                      None                              None   \n",
       "2                                      None                              None   \n",
       "3                                      None                              None   \n",
       "4                                      None                              None   \n",
       "\n",
       "   indicate_drug_or_biological_or_device_or_medical_supply_4  \\\n",
       "0                                               None           \n",
       "1                                               None           \n",
       "2                                               None           \n",
       "3                                               None           \n",
       "4                                               None           \n",
       "\n",
       "  product_category_or_therapeutic_area_4  \\\n",
       "0                                   None   \n",
       "1                                   None   \n",
       "2                                   None   \n",
       "3                                   None   \n",
       "4                                   None   \n",
       "\n",
       "  name_of_drug_or_biological_or_device_or_medical_supply_4  \\\n",
       "0                                               None         \n",
       "1                                               None         \n",
       "2                                               None         \n",
       "3                                               None         \n",
       "4                                               None         \n",
       "\n",
       "  associated_drug_or_biological_ndc_4  \\\n",
       "0                                None   \n",
       "1                                None   \n",
       "2                                None   \n",
       "3                                None   \n",
       "4                                None   \n",
       "\n",
       "   associated_device_or_medical_supply_pdi_4  \\\n",
       "0                                       None   \n",
       "1                                       None   \n",
       "2                                       None   \n",
       "3                                       None   \n",
       "4                                       None   \n",
       "\n",
       "  covered_or_noncovered_indicator_5  \\\n",
       "0                              None   \n",
       "1                              None   \n",
       "2                              None   \n",
       "3                              None   \n",
       "4                              None   \n",
       "\n",
       "   indicate_drug_or_biological_or_device_or_medical_supply_5  \\\n",
       "0                                               None           \n",
       "1                                               None           \n",
       "2                                               None           \n",
       "3                                               None           \n",
       "4                                               None           \n",
       "\n",
       "  product_category_or_therapeutic_area_5  \\\n",
       "0                                   None   \n",
       "1                                   None   \n",
       "2                                   None   \n",
       "3                                   None   \n",
       "4                                   None   \n",
       "\n",
       "  name_of_drug_or_biological_or_device_or_medical_supply_5  \\\n",
       "0                                               None         \n",
       "1                                               None         \n",
       "2                                               None         \n",
       "3                                               None         \n",
       "4                                               None         \n",
       "\n",
       "  associated_drug_or_biological_ndc_5  \\\n",
       "0                                None   \n",
       "1                                None   \n",
       "2                                None   \n",
       "3                                None   \n",
       "4                                None   \n",
       "\n",
       "  associated_device_or_medical_supply_pdi_5 program_year  \\\n",
       "0                                      None         2024   \n",
       "1                                      None         2024   \n",
       "2                                      None         2024   \n",
       "3                                      None         2024   \n",
       "4                                      None         2024   \n",
       "\n",
       "  payment_publication_date  \n",
       "0               06/30/2025  \n",
       "1               06/30/2025  \n",
       "2               06/30/2025  \n",
       "3               06/30/2025  \n",
       "4               06/30/2025  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample query - preview data\n",
    "sample_query = f\"\"\"\n",
    "SELECT *\n",
    "FROM {database_name}.{table_name_csv}\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "print(\"Fetching sample data...\")\n",
    "\n",
    "try:\n",
    "    sample_data = pd.read_sql(sample_query, athena_conn)\n",
    "    print(f\"\\nSample data retrieved\")\n",
    "    print(f\"  Shape: {sample_data.shape}\")\n",
    "    display(sample_data.head())\n",
    "except Exception as e:\n",
    "    print(f\"\\nError fetching sample data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d3faaa",
   "metadata": {},
   "source": [
    "## 7. Convert CSV to Parquet\n",
    "\n",
    "Convert the CSV data to Parquet format for better performance and compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e730f1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet Conversion Configuration:\n",
      "  Source Table: cms_open_payments.general_payments_csv\n",
      "  Target Table: cms_open_payments.general_payments_parquet\n",
      "  Target Location: s3://cmsopenpaymentsystems/cms-open-payments/parquet/\n",
      "Stored 'table_name_parquet' (str)\n"
     ]
    }
   ],
   "source": [
    "# Define Parquet table name\n",
    "table_name_parquet = \"general_payments_parquet\"\n",
    "\n",
    "print(f\"Parquet Conversion Configuration:\")\n",
    "print(f\"  Source Table: {database_name}.{table_name_csv}\")\n",
    "print(f\"  Target Table: {database_name}.{table_name_parquet}\")\n",
    "print(f\"  Target Location: {s3_parquet_path}/\")\n",
    "\n",
    "%store table_name_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54c8b8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting CSV to Parquet format...\n",
      "Note: This operation may take 15-30 minutes for large datasets\n",
      "\n",
      "Query:\n",
      "\n",
      "    CREATE TABLE cms_open_payments.general_payments_parquet\n",
      "    WITH (\n",
      "        format = 'PARQUET',\n",
      "        parquet_compression = 'SNAPPY',\n",
      "        external_location = 's3://cmsopenpaymentsystems/cms-open-payments/parquet/',\n",
      "        partitioned_by = ARRAY['program_year']\n",
      "    )\n",
      "    AS\n",
      "    SELECT \n",
      "        change_type,\n",
      "        covered_recipient_type,\n",
      "        teaching_hospital_ccn,\n",
      "        teaching_hospital_id,\n",
      "        teaching_hospital_name,\n",
      "        covered_recipient_profile_id,\n",
      "        covered_recipient_npi,\n",
      "        covered_recipient_first_name,\n",
      "        covered_recipient_middle_name,\n",
      "        covered_recipient_last_name,\n",
      "        covered_recipient_name_suffix,\n",
      "        recipient_primary_business_street_address_line1,\n",
      "        recipient_primary_business_street_address_line2,\n",
      "        recipient_city,\n",
      "        recipient_state,\n",
      "        recipient_zip_code,\n",
      "        recipient_country,\n",
      "        recipient_province,\n",
      "        recipient_postal_code,\n",
      "        covered_recipient_primary_type_1,\n",
      "        covered_recipient_primary_type_2,\n",
      "        covered_recipient_primary_type_3,\n",
      "        covered_recipient_primary_type_4,\n",
      "        covered_recipient_primary_type_5,\n",
      "        covered_recipient_primary_type_6,\n",
      "        covered_recipient_specialty_1,\n",
      "        covered_recipient_specialty_2,\n",
      "        covered_recipient_specialty_3,\n",
      "        covered_recipient_specialty_4,\n",
      "        covered_recipient_specialty_5,\n",
      "        covered_recipient_specialty_6,\n",
      "        covered_recipient_license_state_code1,\n",
      "        covered_recipient_license_state_code2,\n",
      "        covered_recipient_license_state_code3,\n",
      "        covered_recipient_license_state_code4,\n",
      "        covered_recipient_license_state_code5,\n",
      "        submitting_applicable_manufacturer_or_applicable_gpo_name,\n",
      "        applicable_manufacturer_or_applicable_gpo_making_payment_id,\n",
      "        applicable_manufacturer_or_applicable_gpo_making_payment_name,\n",
      "        applicable_manufacturer_or_applicable_gpo_making_payment_state,\n",
      "        applicable_manufacturer_or_applicable_gpo_making_payment_country,\n",
      "        total_amount_of_payment_usdollars,\n",
      "        date_of_payment,\n",
      "        number_of_payments_included_in_total_amount,\n",
      "        form_of_payment_or_transfer_of_value,\n",
      "        nature_of_payment_or_transfer_of_value,\n",
      "        city_of_travel,\n",
      "        state_of_travel,\n",
      "        country_of_travel,\n",
      "        physician_ownership_indicator,\n",
      "        third_party_payment_recipient_indicator,\n",
      "        name_of_third_party_entity_receiving_payment_or_transfer_of_value,\n",
      "        charity_indicator,\n",
      "        third_party_equals_covered_recipient_indicator,\n",
      "        contextual_information,\n",
      "        delay_in_publication_indicator,\n",
      "        record_id,\n",
      "        dispute_status_for_publication,\n",
      "        related_product_indicator,\n",
      "        covered_or_noncovered_indicator_1,\n",
      "        indicate_drug_or_biological_or_device_or_medical_supply_1,\n",
      "        product_category_or_therapeutic_area_1,\n",
      "        name_of_drug_or_biological_or_device_or_medical_supply_1,\n",
      "        associated_drug_or_biological_ndc_1,\n",
      "        associated_device_or_medical_supply_pdi_1,\n",
      "        covered_or_noncovered_indicator_2,\n",
      "        indicate_drug_or_biological_or_device_or_medical_supply_2,\n",
      "        product_category_or_therapeutic_area_2,\n",
      "        name_of_drug_or_biological_or_device_or_medical_supply_2,\n",
      "        associated_drug_or_biological_ndc_2,\n",
      "        associated_device_or_medical_supply_pdi_2,\n",
      "        covered_or_noncovered_indicator_3,\n",
      "        indicate_drug_or_biological_or_device_or_medical_supply_3,\n",
      "        product_category_or_therapeutic_area_3,\n",
      "        name_of_drug_or_biological_or_device_or_medical_supply_3,\n",
      "        associated_drug_or_biological_ndc_3,\n",
      "        associated_device_or_medical_supply_pdi_3,\n",
      "        covered_or_noncovered_indicator_4,\n",
      "        indicate_drug_or_biological_or_device_or_medical_supply_4,\n",
      "        product_category_or_therapeutic_area_4,\n",
      "        name_of_drug_or_biological_or_device_or_medical_supply_4,\n",
      "        associated_drug_or_biological_ndc_4,\n",
      "        associated_device_or_medical_supply_pdi_4,\n",
      "        covered_or_noncovered_indicator_5,\n",
      "        indicate_drug_or_biological_or_device_or_medical_supply_5,\n",
      "        product_category_or_therapeutic_area_5,\n",
      "        name_of_drug_or_biological_or_device_or_medical_supply_5,\n",
      "        associated_drug_or_biological_ndc_5,\n",
      "        associated_device_or_medical_supply_pdi_5,\n",
      "        payment_publication_date,\n",
      "        '2024' as program_year\n",
      "    FROM cms_open_payments.general_payments_csv\n",
      "    \n",
      "\n",
      "Error during conversion: Execution failed on sql: \n",
      "    CREATE TABLE cms_open_payments.general_payments_parquet\n",
      "    WITH (\n",
      "        format = 'PARQUET',\n",
      "        parquet_compression = 'SNAPPY',\n",
      "        external_location = 's3://cmsopenpaymentsystems/cms-open-payments/parquet/',\n",
      "        partitioned_by = ARRAY['program_year']\n",
      "    )\n",
      "    AS\n",
      "    SELECT \n",
      "        change_type,\n",
      "        covered_recipient_type,\n",
      "        teaching_hospital_ccn,\n",
      "        teaching_hospital_id,\n",
      "        teaching_hospital_name,\n",
      "        covered_recipient_profile_id,\n",
      "        covered_recipient_npi,\n",
      "        covered_recipient_first_name,\n",
      "        covered_recipient_middle_name,\n",
      "        covered_recipient_last_name,\n",
      "        covered_recipient_name_suffix,\n",
      "        recipient_primary_business_street_address_line1,\n",
      "        recipient_primary_business_street_address_line2,\n",
      "        recipient_city,\n",
      "        recipient_state,\n",
      "        recipient_zip_code,\n",
      "        recipient_country,\n",
      "        recipient_province,\n",
      "        recipient_postal_code,\n",
      "        covered_recipient_primary_type_1,\n",
      "        covered_recipient_primary_type_2,\n",
      "        covered_recipient_primary_type_3,\n",
      "        covered_recipient_primary_type_4,\n",
      "        covered_recipient_primary_type_5,\n",
      "        covered_recipient_primary_type_6,\n",
      "        covered_recipient_specialty_1,\n",
      "        covered_recipient_specialty_2,\n",
      "        covered_recipient_specialty_3,\n",
      "        covered_recipient_specialty_4,\n",
      "        covered_recipient_specialty_5,\n",
      "        covered_recipient_specialty_6,\n",
      "        covered_recipient_license_state_code1,\n",
      "        covered_recipient_license_state_code2,\n",
      "        covered_recipient_license_state_code3,\n",
      "        covered_recipient_license_state_code4,\n",
      "        covered_recipient_license_state_code5,\n",
      "        submitting_applicable_manufacturer_or_applicable_gpo_name,\n",
      "        applicable_manufacturer_or_applicable_gpo_making_payment_id,\n",
      "        applicable_manufacturer_or_applicable_gpo_making_payment_name,\n",
      "        applicable_manufacturer_or_applicable_gpo_making_payment_state,\n",
      "        applicable_manufacturer_or_applicable_gpo_making_payment_country,\n",
      "        total_amount_of_payment_usdollars,\n",
      "        date_of_payment,\n",
      "        number_of_payments_included_in_total_amount,\n",
      "        form_of_payment_or_transfer_of_value,\n",
      "        nature_of_payment_or_transfer_of_value,\n",
      "        city_of_travel,\n",
      "        state_of_travel,\n",
      "        country_of_travel,\n",
      "        physician_ownership_indicator,\n",
      "        third_party_payment_recipient_indicator,\n",
      "        name_of_third_party_entity_receiving_payment_or_transfer_of_value,\n",
      "        charity_indicator,\n",
      "        third_party_equals_covered_recipient_indicator,\n",
      "        contextual_information,\n",
      "        delay_in_publication_indicator,\n",
      "        record_id,\n",
      "        dispute_status_for_publication,\n",
      "        related_product_indicator,\n",
      "        covered_or_noncovered_indicator_1,\n",
      "        indicate_drug_or_biological_or_device_or_medical_supply_1,\n",
      "        product_category_or_therapeutic_area_1,\n",
      "        name_of_drug_or_biological_or_device_or_medical_supply_1,\n",
      "        associated_drug_or_biological_ndc_1,\n",
      "        associated_device_or_medical_supply_pdi_1,\n",
      "        covered_or_noncovered_indicator_2,\n",
      "        indicate_drug_or_biological_or_device_or_medical_supply_2,\n",
      "        product_category_or_therapeutic_area_2,\n",
      "        name_of_drug_or_biological_or_device_or_medical_supply_2,\n",
      "        associated_drug_or_biological_ndc_2,\n",
      "        associated_device_or_medical_supply_pdi_2,\n",
      "        covered_or_noncovered_indicator_3,\n",
      "        indicate_drug_or_biological_or_device_or_medical_supply_3,\n",
      "        product_category_or_therapeutic_area_3,\n",
      "        name_of_drug_or_biological_or_device_or_medical_supply_3,\n",
      "        associated_drug_or_biological_ndc_3,\n",
      "        associated_device_or_medical_supply_pdi_3,\n",
      "        covered_or_noncovered_indicator_4,\n",
      "        indicate_drug_or_biological_or_device_or_medical_supply_4,\n",
      "        product_category_or_therapeutic_area_4,\n",
      "        name_of_drug_or_biological_or_device_or_medical_supply_4,\n",
      "        associated_drug_or_biological_ndc_4,\n",
      "        associated_device_or_medical_supply_pdi_4,\n",
      "        covered_or_noncovered_indicator_5,\n",
      "        indicate_drug_or_biological_or_device_or_medical_supply_5,\n",
      "        product_category_or_therapeutic_area_5,\n",
      "        name_of_drug_or_biological_or_device_or_medical_supply_5,\n",
      "        associated_drug_or_biological_ndc_5,\n",
      "        associated_device_or_medical_supply_pdi_5,\n",
      "        payment_publication_date,\n",
      "        '2024' as program_year\n",
      "    FROM cms_open_payments.general_payments_csv\n",
      "    \n",
      "TABLE_ALREADY_EXISTS: line 1:1: Destination table 'awsdatacatalog.cms_open_payments.general_payments_parquet' already exists. You may need to manually clean the data at location 's3://cmsopenpaymentsystems/athena/staging/tables/0db84019-181b-41d7-aeaf-5a552891539e' before retrying. Athena will not delete data in your account.\n",
      "unable to rollback\n",
      "\n",
      "Note: If table already exists, drop it first:\n",
      "  DROP TABLE IF EXISTS cms_open_payments.general_payments_parquet\n"
     ]
    }
   ],
   "source": [
    "# Get column names and create explicit column list\n",
    "columns_query = f\"\"\"\n",
    "SELECT * \n",
    "FROM {database_name}.{table_name_csv} \n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Get all columns from source table\n",
    "    columns_df = pd.read_sql(columns_query, athena_conn)\n",
    "    columns = columns_df.columns.tolist()\n",
    "    \n",
    "    # Remove program_year if it exists\n",
    "    if 'program_year' in columns:\n",
    "        columns.remove('program_year')\n",
    "    \n",
    "    # Create column selection string, ensuring program_year is last\n",
    "    columns_str = ',\\n        '.join(columns)\n",
    "    \n",
    "    create_parquet_query = f\"\"\"\n",
    "    CREATE TABLE {database_name}.{table_name_parquet}\n",
    "    WITH (\n",
    "        format = 'PARQUET',\n",
    "        parquet_compression = 'SNAPPY',\n",
    "        external_location = '{s3_parquet_path}/',\n",
    "        partitioned_by = ARRAY['program_year']\n",
    "    )\n",
    "    AS\n",
    "    SELECT \n",
    "        {columns_str},\n",
    "        '2024' as program_year\n",
    "    FROM {database_name}.{table_name_csv}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Converting CSV to Parquet format...\")\n",
    "    print(\"Note: This operation may take 15-30 minutes for large datasets\")\n",
    "    print(f\"\\nQuery:\")\n",
    "    print(create_parquet_query)\n",
    "\n",
    "    # Execute conversion\n",
    "    result = pd.read_sql(create_parquet_query, athena_conn)\n",
    "    print(f\"\\nConversion complete\")\n",
    "    print(f\"  Parquet table '{table_name_parquet}' created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during conversion: {e}\")\n",
    "    print(f\"\\nNote: If table already exists, drop it first:\")\n",
    "    print(f\"  DROP TABLE IF EXISTS {database_name}.{table_name_parquet}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "177c7afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying Parquet table...\n",
      "\n",
      "Parquet table verified\n",
      "  Total rows: 15,397,627\n"
     ]
    }
   ],
   "source": [
    "# Verify Parquet table\n",
    "count_parquet_query = f\"\"\"\n",
    "SELECT COUNT(*) as row_count\n",
    "FROM {database_name}.{table_name_parquet}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Verifying Parquet table...\")\n",
    "\n",
    "try:\n",
    "    result = pd.read_sql(count_parquet_query, athena_conn)\n",
    "    print(f\"\\nParquet table verified\")\n",
    "    print(f\"  Total rows: {result['row_count'][0]:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError verifying Parquet table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89eaa880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing CSV vs Parquet storage:\n",
      "\n",
      "Storage Comparison:\n",
      "  CSV Size: 8.22 GB\n",
      "  Parquet Size: 0.51 GB\n",
      "  Compression: 93.8% reduction\n",
      "  Space Saved: 7.70 GB\n"
     ]
    }
   ],
   "source": [
    "# Compare file sizes\n",
    "print(\"Comparing CSV vs Parquet storage:\")\n",
    "\n",
    "# Get CSV size\n",
    "csv_objects = s3_client.list_objects_v2(\n",
    "    Bucket=bucket,\n",
    "    Prefix=raw_data_prefix\n",
    ")\n",
    "\n",
    "csv_size = sum(obj['Size'] for obj in csv_objects.get('Contents', []))\n",
    "\n",
    "# Get Parquet size\n",
    "parquet_objects = s3_client.list_objects_v2(\n",
    "    Bucket=bucket,\n",
    "    Prefix=parquet_data_prefix\n",
    ")\n",
    "\n",
    "parquet_size = sum(obj['Size'] for obj in parquet_objects.get('Contents', []))\n",
    "\n",
    "print(f\"\\nStorage Comparison:\")\n",
    "print(f\"  CSV Size: {csv_size / (1024**3):.2f} GB\")\n",
    "print(f\"  Parquet Size: {parquet_size / (1024**3):.2f} GB\")\n",
    "if parquet_size > 0:\n",
    "    compression_ratio = (1 - parquet_size/csv_size) * 100\n",
    "    print(f\"  Compression: {compression_ratio:.1f}% reduction\")\n",
    "    print(f\"  Space Saved: {(csv_size - parquet_size) / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6925b16",
   "metadata": {},
   "source": [
    "## 8. Query Data with AWS Data Wrangler\n",
    "\n",
    "Use AWS Data Wrangler for more efficient data querying and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56e7d541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying payment statistics with AWS Data Wrangler...\n",
      "\n",
      "Query: \n",
      "SELECT \n",
      "    COUNT(*) as total_payments,\n",
      "    SUM(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as total_amount,\n",
      "    AVG(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as avg_amount,\n",
      "    MIN(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as min_amount,\n",
      "    MAX(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as max_amount\n",
      "FROM cms_open_payments.general_payments_parquet\n",
      "\n",
      "\n",
      "Query successful\n",
      "\n",
      "Payment Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_payments</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>avg_amount</th>\n",
       "      <th>min_amount</th>\n",
       "      <th>max_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15397627</td>\n",
       "      <td>6.450168e+13</td>\n",
       "      <td>7.630776e+06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.000012e+11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_payments  total_amount    avg_amount  min_amount    max_amount\n",
       "0        15397627  6.450168e+13  7.630776e+06        0.01  1.000012e+11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query using AWS Data Wrangler\n",
    "sample_query_wr = f\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_payments,\n",
    "    SUM(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as total_amount,\n",
    "    AVG(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as avg_amount,\n",
    "    MIN(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as min_amount,\n",
    "    MAX(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as max_amount\n",
    "FROM {database_name}.{table_name_parquet}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Querying payment statistics with AWS Data Wrangler...\")\n",
    "print(f\"\\nQuery: {sample_query_wr}\")\n",
    "\n",
    "try:\n",
    "    df_stats = wr.athena.read_sql_query(\n",
    "        sql=sample_query_wr,\n",
    "        database=database_name,\n",
    "        ctas_approach=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nQuery successful\")\n",
    "    print(f\"\\nPayment Statistics:\")\n",
    "    display(df_stats)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError querying data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2163587f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing payments by recipient type...\n",
      "\n",
      "Query successful\n",
      "\n",
      "Payments by Recipient Type:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Covered_Recipient_Type</th>\n",
       "      <th>payment_count</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Covered Recipient Physician</td>\n",
       "      <td>9894393</td>\n",
       "      <td>4.290113e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Covered Recipient Non-Physician Practitioner</td>\n",
       "      <td>5468860</td>\n",
       "      <td>2.160014e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Covered Recipient Teaching Hospital</td>\n",
       "      <td>34374</td>\n",
       "      <td>4.145168e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Covered_Recipient_Type  payment_count  total_amount\n",
       "0                   Covered Recipient Physician        9894393  4.290113e+13\n",
       "1  Covered Recipient Non-Physician Practitioner        5468860  2.160014e+13\n",
       "2           Covered Recipient Teaching Hospital          34374  4.145168e+08"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample data by recipient type\n",
    "recipient_query = f\"\"\"\n",
    "SELECT \n",
    "    Covered_Recipient_Type,\n",
    "    COUNT(*) as payment_count,\n",
    "    SUM(CAST(Total_Amount_of_Payment_USDollars AS DOUBLE)) as total_amount\n",
    "FROM {database_name}.{table_name_parquet}\n",
    "GROUP BY Covered_Recipient_Type\n",
    "ORDER BY total_amount DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"Analyzing payments by recipient type...\")\n",
    "\n",
    "try:\n",
    "    df_recipients = wr.athena.read_sql_query(\n",
    "        sql=recipient_query,\n",
    "        database=database_name,\n",
    "        ctas_approach=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nQuery successful\")\n",
    "    print(f\"\\nPayments by Recipient Type:\")\n",
    "    display(df_recipients)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError querying data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf9fb6",
   "metadata": {},
   "source": [
    "## 9. Validation & Verification\n",
    "\n",
    "Perform final validation checks on the datalake setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae064097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATALAKE SETUP VALIDATION\n",
      "======================================================================\n",
      "\n",
      "1. S3 Storage:\n",
      "  cms-open-payments/raw/\n",
      "  cms-open-payments/parquet/\n",
      "\n",
      "2. Athena Database:\n",
      "Database 'cms_open_payments' exists\n",
      "\n",
      "3. Athena Tables:\n",
      "Table 'general_payments_csv' exists\n",
      "Table 'general_payments_parquet' exists\n",
      "\n",
      "4. Data Accessibility:\n",
      "Query successful (15,397,627 rows)\n",
      "\n",
      "======================================================================\n",
      "ALL VALIDATION CHECKS PASSED\n",
      "Datalake setup complete and operational\n",
      "======================================================================\n",
      "Stored 'setup_datalake_passed' (bool)\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive validation\n",
    "print(\"=\" * 70)\n",
    "print(\"DATALAKE SETUP VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "validation_passed = True\n",
    "\n",
    "# Check 1: S3 Buckets\n",
    "print(\"\\n1. S3 Storage:\")\n",
    "try:\n",
    "    for prefix in [raw_data_prefix, parquet_data_prefix]:\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=1)\n",
    "        if 'Contents' in response:\n",
    "            print(f\"  {prefix}/\")\n",
    "        else:\n",
    "            print(f\"  [FAIL]{prefix}/ (empty or missing)\")\n",
    "            validation_passed = False\n",
    "except Exception as e:\n",
    "    print(f\"Error checking S3: {e}\")\n",
    "    validation_passed = False\n",
    "\n",
    "# Check 2: Athena Database\n",
    "print(\"\\n2. Athena Database:\")\n",
    "try:\n",
    "    databases = pd.read_sql(\"SHOW DATABASES\", athena_conn)\n",
    "    if database_name in databases.values:\n",
    "        print(f\"Database '{database_name}' exists\")\n",
    "    else:\n",
    "        print(f\"Database '{database_name}' not found\")\n",
    "        validation_passed = False\n",
    "except Exception as e:\n",
    "    print(f\"Error checking database: {e}\")\n",
    "    validation_passed = False\n",
    "\n",
    "# Check 3: Tables\n",
    "print(\"\\n3. Athena Tables:\")\n",
    "try:\n",
    "    tables = pd.read_sql(f\"SHOW TABLES IN {database_name}\", athena_conn)\n",
    "    for table in [table_name_csv, table_name_parquet]:\n",
    "        if table in tables.values:\n",
    "            print(f\"Table '{table}' exists\")\n",
    "        else:\n",
    "            print(f\"Table '{table}' not found\")\n",
    "            validation_passed = False\n",
    "except Exception as e:\n",
    "    print(f\"Error checking tables: {e}\")\n",
    "    validation_passed = False\n",
    "\n",
    "# Check 4: Data Accessibility\n",
    "print(\"\\n4. Data Accessibility:\")\n",
    "try:\n",
    "    count_result = pd.read_sql(\n",
    "        f\"SELECT COUNT(*) as cnt FROM {database_name}.{table_name_parquet}\",\n",
    "        athena_conn\n",
    "    )\n",
    "    row_count = count_result['cnt'][0]\n",
    "    print(f\"Query successful ({row_count:,} rows)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error querying data: {e}\")\n",
    "    validation_passed = False\n",
    "\n",
    "# Final result\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "if validation_passed:\n",
    "    print(\"ALL VALIDATION CHECKS PASSED\")\n",
    "    print(\"Datalake setup complete and operational\")\n",
    "    setup_datalake_passed = True\n",
    "else:\n",
    "    print(\"SOME VALIDATION CHECKS FAILED\")\n",
    "    print(\"Please review the errors above and re-run failed steps\")\n",
    "    setup_datalake_passed = False\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Store validation result\n",
    "%store setup_datalake_passed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker-recovery-mode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
