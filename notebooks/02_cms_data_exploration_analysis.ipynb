{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebc95b1c",
   "metadata": {},
   "source": [
    "# CMS Open Payments Data Exploration & Analysis\n",
    "\n",
    "**Project:** AAI-540 Machine Learning Operations - Final Team Project  \n",
    "**Dataset:** CMS Open Payments Program Year 2024 General Payments  \n",
    "**Purpose:** Exploratory Data Analysis for Payment Patterns and Statistical Insights\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Environment Setup & Configuration](#setup)\n",
    "2. [Data Loading from Datalake](#loading)\n",
    "3. [Data Quality Assessment](#quality)\n",
    "4. [Univariate Analysis](#univariate)\n",
    "5. [Bivariate & Multivariate Analysis](#multivariate)\n",
    "6. [Temporal Analysis](#temporal)\n",
    "7. [Geographic Analysis](#geographic)\n",
    "8. [Feature Engineering](#features)\n",
    "9. [Outlier Detection](#outliers)\n",
    "10. [Advanced Visualizations](#advanced)\n",
    "11. [Key Findings & Insights](#findings)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386ba112",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Configuration\n",
    "\n",
    "Setting up the environment with necessary libraries and AWS integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb5f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -r ../requirements.txt --quiet\n",
    "!pip install boto3 sagemaker awswrangler pyathena --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e213c954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from scipy import stats\n",
    "\n",
    "# AWS libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "import awswrangler as wr\n",
    "from pyathena import connect\n",
    "\n",
    "# Add parent directory to path for custom modules\n",
    "parent_dir = Path.cwd().parent\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "# Visualization settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab2e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom utilities (if available)\n",
    "try:\n",
    "    from config import CONFIG\n",
    "    from utils import CMSDataLoader, PaymentVisualizer, FeatureEngineer\n",
    "    print(\"Custom utilities imported\")\n",
    "    use_custom_utils = True\n",
    "except ImportError:\n",
    "    print(\"Custom utilities not found - using standard libraries only\")\n",
    "    use_custom_utils = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd72de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore AWS configuration from datalake setup notebook\n",
    "%store -r bucket\n",
    "%store -r region\n",
    "%store -r database_name\n",
    "%store -r table_name_parquet\n",
    "%store -r s3_parquet_path\n",
    "%store -r s3_athena_staging\n",
    "\n",
    "# If variables not restored, set defaults matching datalake setup\n",
    "try:\n",
    "    # Test if variables exist\n",
    "    test_vars = [bucket, region, database_name, table_name_parquet]\n",
    "    \n",
    "    print(f\"AWS Configuration:\")\n",
    "    print(f\"  Region: {region}\")\n",
    "    print(f\"  S3 Bucket: {bucket}\")\n",
    "    print(f\"  Database: {database_name}\")\n",
    "    print(f\"  Table: {table_name_parquet}\")\n",
    "    print(f\"  Parquet Path: {s3_parquet_path}\")\n",
    "    print(f\"  Athena Staging: {s3_athena_staging}\")\n",
    "    \n",
    "except NameError as e:\n",
    "    print(f\"Variables not found in store. Setting up from AWS configuration...\")\n",
    "    \n",
    "    # Initialize AWS session\n",
    "    boto_session = boto3.Session()\n",
    "    region = boto_session.region_name\n",
    "    \n",
    "    # Get account information\n",
    "    sts_client = boto3.client('sts')\n",
    "    account_id = sts_client.get_caller_identity().get('Account')\n",
    "    \n",
    "    # Set configuration matching datalake setup\n",
    "    bucket = f\"cmsopenpaymentsystems{account_id}\"\n",
    "    database_name = \"cms_open_payments\"\n",
    "    table_name_parquet = \"general_payments_parquet\"\n",
    "    \n",
    "    # Define S3 paths\n",
    "    cms_data_prefix = \"cms-open-payments\"\n",
    "    parquet_data_prefix = f\"{cms_data_prefix}/parquet\"\n",
    "    s3_parquet_path = f\"s3://{bucket}/{parquet_data_prefix}\"\n",
    "    s3_athena_staging = f\"s3://{bucket}/athena/staging\"\n",
    "    \n",
    "    print(f\"\\nAWS Configuration (manual setup):\")\n",
    "    print(f\"  Region: {region}\")\n",
    "    print(f\"  Account ID: {account_id}\")\n",
    "    print(f\"  S3 Bucket: {bucket}\")\n",
    "    print(f\"  Database: {database_name}\")\n",
    "    print(f\"  Table: {table_name_parquet}\")\n",
    "    print(f\"  Parquet Path: {s3_parquet_path}\")\n",
    "    print(f\"  Athena Staging: {s3_athena_staging}\")\n",
    "    print(f\"\\nNote: Please run the datalake setup notebook (01_setup_cms_datalake.ipynb) first for full setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381c31ff",
   "metadata": {},
   "source": [
    "## 2. Data Loading from Datalake\n",
    "\n",
    "Load CMS Open Payments data from AWS Athena using optimized Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f841c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Athena connection\n",
    "athena_conn = connect(\n",
    "    region_name=region,\n",
    "    s3_staging_dir=s3_athena_staging\n",
    ")\n",
    "\n",
    "print(\"Athena connection established\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb303736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load full dataset from Parquet (recommended for complete analysis)\n",
    "# Note: This may take several minutes and require substantial memory\n",
    "\n",
    "load_full_dataset = False  # Set to True to load full dataset\n",
    "\n",
    "if load_full_dataset:\n",
    "    print(\"Loading full dataset from Parquet...\")\n",
    "    print(\"Note: This may take several minutes\")\n",
    "    \n",
    "    df = wr.athena.read_sql_query(\n",
    "        sql=f\"SELECT * FROM {database_name}.{table_name_parquet}\",\n",
    "        database=database_name,\n",
    "        ctas_approach=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Full dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "else:\n",
    "    print(\"Skipping full dataset load - will use sample queries instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f7ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Load sample dataset for faster EDA\n",
    "sample_size = 100000  # Adjust based on your needs\n",
    "\n",
    "print(f\"Loading sample dataset ({sample_size:,} rows)...\")\n",
    "\n",
    "sample_query = f\"\"\"\n",
    "SELECT *\n",
    "FROM {database_name}.{table_name_parquet}\n",
    "LIMIT {sample_size}\n",
    "\"\"\"\n",
    "\n",
    "df = wr.athena.read_sql_query(\n",
    "    sql=sample_query,\n",
    "    database=database_name,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "print(f\"Sample dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "print(f\"  Memory Usage: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd6c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "print(\"Dataset Preview:\")\n",
    "display(df.head(3))\n",
    "\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Columns: {df.shape[1]}\")\n",
    "print(f\"\\nColumn Names:\")\n",
    "for i, col in enumerate(df.columns[:20], 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "if len(df.columns) > 20:\n",
    "    print(f\"  ... ({len(df.columns) - 20} more columns)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa411aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "print(\"Preprocessing data...\")\n",
    "\n",
    "# Identify and convert date columns\n",
    "date_columns = [col for col in df.columns if 'date' in col.lower() or 'Date' in col]\n",
    "for col in date_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "# Identify numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "print(f\"  Date columns: {len(date_columns)}\")\n",
    "print(f\"  Numeric columns: {len(numeric_cols)}\")\n",
    "\n",
    "# Create temporal features if date column exists\n",
    "payment_date_col = 'Date_of_Payment'\n",
    "if payment_date_col in df.columns:\n",
    "    df['Payment_Year'] = df[payment_date_col].dt.year\n",
    "    df['Payment_Month'] = df[payment_date_col].dt.month\n",
    "    df['Payment_Quarter'] = df[payment_date_col].dt.quarter\n",
    "    df['Payment_DayOfWeek'] = df[payment_date_col].dt.dayofweek\n",
    "    df['Payment_Week'] = df[payment_date_col].dt.isocalendar().week\n",
    "    print(f\"Temporal features created\")\n",
    "\n",
    "print(f\"Preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95e1db0",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment\n",
    "\n",
    "Comprehensive assessment of data quality including completeness, validity, and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cefa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"=\" * 70)\n",
    "print(\"DATASET BASIC STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nDimensions:\")\n",
    "print(f\"  Total Rows: {df.shape[0]:,}\")\n",
    "print(f\"  Total Columns: {df.shape[1]}\")\n",
    "\n",
    "print(f\"\\nMemory Usage:\")\n",
    "memory_mb = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "print(f\"  Total: {memory_mb:.2f} MB\")\n",
    "print(f\"  Per Row: {memory_mb / df.shape[0] * 1024:.2f} KB\")\n",
    "\n",
    "print(f\"\\nColumn Types:\")\n",
    "print(f\"  Numeric: {len(df.select_dtypes(include=['number']).columns)}\")\n",
    "print(f\"  Object/String: {len(df.select_dtypes(include=['object']).columns)}\")\n",
    "print(f\"  DateTime: {len(df.select_dtypes(include=['datetime']).columns)}\")\n",
    "\n",
    "print(f\"\\nMissing Values:\")\n",
    "total_cells = df.shape[0] * df.shape[1]\n",
    "missing_cells = df.isnull().sum().sum()\n",
    "print(f\"  Total Cells: {total_cells:,}\")\n",
    "print(f\"  Missing Cells: {missing_cells:,}\")\n",
    "print(f\"  Missing Percentage: {(missing_cells/total_cells)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nDuplicate Rows:\")\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"  Count: {duplicates:,}\")\n",
    "print(f\"  Percentage: {(duplicates/df.shape[0])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786923bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "print(\"Missing Values by Column (Top 20):\")\n",
    "\n",
    "missing_stats = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum().values,\n",
    "    'Missing_Percent': (df.isnull().sum().values / len(df) * 100)\n",
    "})\n",
    "\n",
    "missing_stats = missing_stats[missing_stats['Missing_Count'] > 0].sort_values(\n",
    "    'Missing_Percent', ascending=False\n",
    ")\n",
    "\n",
    "display(missing_stats.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a83f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "if len(missing_stats) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    top_missing = missing_stats.head(20)\n",
    "    \n",
    "    colors = ['#e74c3c' if x > 50 else '#f39c12' if x > 20 else '#3498db' \n",
    "              for x in top_missing['Missing_Percent']]\n",
    "    \n",
    "    ax.barh(range(len(top_missing)), top_missing['Missing_Percent'], \n",
    "            color=colors, edgecolor='black', alpha=0.7)\n",
    "    ax.set_yticks(range(len(top_missing)))\n",
    "    ax.set_yticklabels(top_missing['Column'])\n",
    "    ax.set_xlabel('Missing Values (%)', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Top 20 Columns by Missing Values', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, (idx, row) in enumerate(top_missing.iterrows()):\n",
    "        ax.text(row['Missing_Percent'] + 1, i, f\"{row['Missing_Percent']:.1f}%\", \n",
    "                va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No missing values detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd42b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types summary\n",
    "print(\"\\nData Types Summary:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(\"\\n Sample of each data type:\")\n",
    "for dtype in df.dtypes.unique():\n",
    "    cols = df.select_dtypes(include=[dtype]).columns[:3]\n",
    "    print(f\"  {dtype}: {', '.join(cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e599c779",
   "metadata": {},
   "source": [
    "## 4. Univariate Analysis\n",
    "\n",
    "Analyzing individual variables to understand distributions, central tendencies, and variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7002d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payment amount analysis\n",
    "payment_col = 'Total_Amount_of_Payment_USDollars'\n",
    "\n",
    "if payment_col in df.columns:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PAYMENT AMOUNT STATISTICS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    payment_stats = df[payment_col].describe()\n",
    "    \n",
    "    print(f\"\\nBasic Statistics:\")\n",
    "    print(f\"  Count: {payment_stats['count']:,.0f}\")\n",
    "    print(f\"  Mean: ${payment_stats['mean']:,.2f}\")\n",
    "    print(f\"  Median: ${payment_stats['50%']:,.2f}\")\n",
    "    print(f\"  Std Dev: ${payment_stats['std']:,.2f}\")\n",
    "    print(f\"  Min: ${payment_stats['min']:,.2f}\")\n",
    "    print(f\"  Max: ${payment_stats['max']:,.2f}\")\n",
    "    \n",
    "    print(f\"\\nQuartiles:\")\n",
    "    print(f\"  25th percentile: ${payment_stats['25%']:,.2f}\")\n",
    "    print(f\"  50th percentile: ${payment_stats['50%']:,.2f}\")\n",
    "    print(f\"  75th percentile: ${payment_stats['75%']:,.2f}\")\n",
    "    \n",
    "    print(f\"\\nAdditional Percentiles:\")\n",
    "    for p in [90, 95, 99, 99.9]:\n",
    "        val = df[payment_col].quantile(p/100)\n",
    "        print(f\"  {p}th percentile: ${val:,.2f}\")\n",
    "    \n",
    "    # Skewness and Kurtosis\n",
    "    skewness = df[payment_col].skew()\n",
    "    kurtosis = df[payment_col].kurtosis()\n",
    "    print(f\"\\nDistribution Shape:\")\n",
    "    print(f\"  Skewness: {skewness:.2f}\")\n",
    "    print(f\"  Kurtosis: {kurtosis:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f212fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize payment distribution\n",
    "if payment_col in df.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0, 0].hist(df[payment_col].dropna(), bins=50, color='steelblue', \n",
    "                    edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('Payment Amount ($)', fontsize=11, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "    axes[0, 0].set_title('Payment Amount Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Log-scale histogram\n",
    "    log_payments = np.log10(df[payment_col][df[payment_col] > 0])\n",
    "    axes[0, 1].hist(log_payments, bins=50, color='coral', \n",
    "                    edgecolor='black', alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Log10(Payment Amount)', fontsize=11, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "    axes[0, 1].set_title('Payment Amount Distribution (Log Scale)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # Box plot\n",
    "    axes[1, 0].boxplot(df[payment_col].dropna(), vert=True, patch_artist=True,\n",
    "                       boxprops=dict(facecolor='lightgreen', alpha=0.7),\n",
    "                       medianprops=dict(color='red', linewidth=2))\n",
    "    axes[1, 0].set_ylabel('Payment Amount ($)', fontsize=11, fontweight='bold')\n",
    "    axes[1, 0].set_title('Payment Amount Box Plot', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Violin plot\n",
    "    parts = axes[1, 1].violinplot([df[payment_col].dropna()], vert=True, \n",
    "                                   showmeans=True, showmedians=True)\n",
    "    for pc in parts['bodies']:\n",
    "        pc.set_facecolor('plum')\n",
    "        pc.set_alpha(0.7)\n",
    "    axes[1, 1].set_ylabel('Payment Amount ($)', fontsize=11, fontweight='bold')\n",
    "    axes[1, 1].set_title('Payment Amount Violin Plot', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b864d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables analysis\n",
    "print(\"=\" * 70)\n",
    "print(\"CATEGORICAL VARIABLES ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "categorical_cols = [\n",
    "    'Change_Type',\n",
    "    'Covered_Recipient_Type',\n",
    "    'Form_of_Payment_or_Transfer_of_Value',\n",
    "    'Nature_of_Payment_or_Transfer_of_Value'\n",
    "]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        value_counts = df[col].value_counts().head(10)\n",
    "        print(value_counts)\n",
    "        print(f\"  Unique values: {df[col].nunique()}\")\n",
    "        print(f\"  Missing values: {df[col].isnull().sum()} ({df[col].isnull().sum()/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7614bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize categorical distributions\n",
    "categorical_cols_viz = [\n",
    "    'Covered_Recipient_Type',\n",
    "    'Form_of_Payment_or_Transfer_of_Value',\n",
    "    'Nature_of_Payment_or_Transfer_of_Value'\n",
    "]\n",
    "\n",
    "available_cols = [col for col in categorical_cols_viz if col in df.columns]\n",
    "\n",
    "if len(available_cols) > 0:\n",
    "    n_cols = min(len(available_cols), 2)\n",
    "    n_rows = (len(available_cols) + 1) // 2\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 6*n_rows))\n",
    "    if n_rows * n_cols == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for idx, col in enumerate(available_cols):\n",
    "        value_counts = df[col].value_counts().head(15)\n",
    "        \n",
    "        axes[idx].barh(range(len(value_counts)), value_counts.values,\n",
    "                       color=sns.color_palette('viridis', len(value_counts)),\n",
    "                       edgecolor='black', alpha=0.7)\n",
    "        axes[idx].set_yticks(range(len(value_counts)))\n",
    "        axes[idx].set_yticklabels(value_counts.index, fontsize=9)\n",
    "        axes[idx].set_xlabel('Count', fontsize=11, fontweight='bold')\n",
    "        axes[idx].set_title(f'{col}\\n(Top 15)', fontsize=12, fontweight='bold')\n",
    "        axes[idx].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for idx in range(len(available_cols), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e6b9e3",
   "metadata": {},
   "source": [
    "## 5. Bivariate & Multivariate Analysis\n",
    "\n",
    "Explore relationships between multiple variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3619d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payment amount by recipient type\n",
    "recipient_type_col = 'Covered_Recipient_Type'\n",
    "\n",
    "if recipient_type_col in df.columns and payment_col in df.columns:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PAYMENT STATISTICS BY RECIPIENT TYPE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    type_stats = df.groupby(recipient_type_col)[payment_col].agg([\n",
    "        'count', 'sum', 'mean', 'median', 'std', 'min', 'max'\n",
    "    ]).round(2)\n",
    "    \n",
    "    type_stats.columns = ['Count', 'Total ($)', 'Mean ($)', 'Median ($)', 'Std Dev ($)', 'Min ($)', 'Max ($)']\n",
    "    type_stats = type_stats.sort_values('Total ($)', ascending=False)\n",
    "    \n",
    "    display(type_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize payment by recipient type\n",
    "if recipient_type_col in df.columns and payment_col in df.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Box plot\n",
    "    df.boxplot(column=payment_col, by=recipient_type_col, ax=axes[0],\n",
    "               patch_artist=True, grid=True)\n",
    "    axes[0].set_xlabel('Recipient Type', fontsize=11, fontweight='bold')\n",
    "    axes[0].set_ylabel('Payment Amount ($)', fontsize=11, fontweight='bold')\n",
    "    axes[0].set_title('Payment Distribution by Recipient Type', fontsize=12, fontweight='bold')\n",
    "    plt.sca(axes[0])\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Bar plot - total amounts\n",
    "    total_by_type = df.groupby(recipient_type_col)[payment_col].sum().sort_values(ascending=False)\n",
    "    colors = sns.color_palette('rocket', len(total_by_type))\n",
    "    axes[1].bar(range(len(total_by_type)), total_by_type.values,\n",
    "                color=colors, edgecolor='black', alpha=0.7)\n",
    "    axes[1].set_xticks(range(len(total_by_type)))\n",
    "    axes[1].set_xticklabels(total_by_type.index, rotation=45, ha='right')\n",
    "    axes[1].set_xlabel('Recipient Type', fontsize=11, fontweight='bold')\n",
    "    axes[1].set_ylabel('Total Payment Amount ($)', fontsize=11, fontweight='bold')\n",
    "    axes[1].set_title('Total Payments by Recipient Type', fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b43833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payment by Nature of Payment\n",
    "nature_col = 'Nature_of_Payment_or_Transfer_of_Value'\n",
    "\n",
    "if nature_col in df.columns and payment_col in df.columns:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TOP 15 PAYMENT NATURES BY TOTAL AMOUNT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    nature_stats = df.groupby(nature_col)[payment_col].agg([\n",
    "        'count', 'sum', 'mean', 'median'\n",
    "    ]).round(2)\n",
    "    \n",
    "    nature_stats.columns = ['Count', 'Total ($)', 'Mean ($)', 'Median ($)']\n",
    "    nature_stats = nature_stats.sort_values('Total ($)', ascending=False).head(15)\n",
    "    \n",
    "    display(nature_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb19d11",
   "metadata": {},
   "source": [
    "## 6. Temporal Analysis\n",
    "\n",
    "Analyze payment patterns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly payment trends\n",
    "if 'Payment_Month' in df.columns and payment_col in df.columns:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"MONTHLY PAYMENT STATISTICS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    monthly_stats = df.groupby('Payment_Month')[payment_col].agg([\n",
    "        'count', 'sum', 'mean', 'median'\n",
    "    ]).round(2)\n",
    "    \n",
    "    monthly_stats.columns = ['Count', 'Total ($)', 'Mean ($)', 'Median ($)']\n",
    "    monthly_stats.index.name = 'Month'\n",
    "    \n",
    "    display(monthly_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c937b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize monthly trends\n",
    "if 'Payment_Month' in df.columns and payment_col in df.columns:\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Monthly payment count\n",
    "    monthly_counts = df.groupby('Payment_Month').size()\n",
    "    axes[0].plot(monthly_counts.index, monthly_counts.values, \n",
    "                 marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "    axes[0].set_xlabel('Month', fontsize=11, fontweight='bold')\n",
    "    axes[0].set_ylabel('Number of Payments', fontsize=11, fontweight='bold')\n",
    "    axes[0].set_title('Monthly Payment Count', fontsize=12, fontweight='bold')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    axes[0].set_xticks(range(1, 13))\n",
    "    \n",
    "    # Monthly payment total\n",
    "    monthly_totals = df.groupby('Payment_Month')[payment_col].sum()\n",
    "    axes[1].bar(monthly_totals.index, monthly_totals.values,\n",
    "                color=sns.color_palette('viridis', 12), edgecolor='black', alpha=0.7)\n",
    "    axes[1].set_xlabel('Month', fontsize=11, fontweight='bold')\n",
    "    axes[1].set_ylabel('Total Payment Amount ($)', fontsize=11, fontweight='bold')\n",
    "    axes[1].set_title('Monthly Total Payment Amount', fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    axes[1].set_xticks(range(1, 13))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06345402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quarterly analysis\n",
    "if 'Payment_Quarter' in df.columns and payment_col in df.columns:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"QUARTERLY PAYMENT STATISTICS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    quarterly_stats = df.groupby('Payment_Quarter')[payment_col].agg([\n",
    "        'count', 'sum', 'mean', 'median'\n",
    "    ]).round(2)\n",
    "    \n",
    "    quarterly_stats.columns = ['Count', 'Total ($)', 'Mean ($)', 'Median ($)']\n",
    "    quarterly_stats.index.name = 'Quarter'\n",
    "    \n",
    "    display(quarterly_stats)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    quarterly_stats['Total ($)'].plot(kind='bar', ax=ax, \n",
    "                                       color=['#3498db', '#2ecc71', '#f39c12', '#e74c3c'],\n",
    "                                       edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel('Quarter', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Total Payment Amount ($)', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Quarterly Total Payment Amount', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e94d54",
   "metadata": {},
   "source": [
    "## 7. Geographic Analysis {#geographic}\n",
    "\n",
    "Analyzing payment distributions across geographic regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da986ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State-level statistics\n",
    "state_col = 'Recipient_State'\n",
    "\n",
    "if state_col in df.columns and payment_col in df.columns:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"TOP 20 STATES BY PAYMENT METRICS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    state_stats = df.groupby(state_col)[payment_col].agg([\n",
    "        'count', 'sum', 'mean', 'median'\n",
    "    ]).round(2)\n",
    "    \n",
    "    state_stats.columns = ['Count', 'Total ($)', 'Mean ($)', 'Median ($)']\n",
    "    state_stats = state_stats.sort_values('Total ($)', ascending=False).head(20)\n",
    "    state_stats.index.name = 'State'\n",
    "    \n",
    "    display(state_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d70f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize geographic distribution\n",
    "if state_col in df.columns and payment_col in df.columns:\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 12))\n",
    "    \n",
    "    # Top 20 states by count\n",
    "    top_states_count = df[state_col].value_counts().head(20)\n",
    "    axes[0].barh(range(len(top_states_count)), top_states_count.values,\n",
    "                 color=sns.color_palette('rocket', len(top_states_count)),\n",
    "                 edgecolor='black', alpha=0.7)\n",
    "    axes[0].set_yticks(range(len(top_states_count)))\n",
    "    axes[0].set_yticklabels(top_states_count.index)\n",
    "    axes[0].set_xlabel('Number of Payments', fontsize=11, fontweight='bold')\n",
    "    axes[0].set_title('Top 20 States by Payment Count', fontsize=12, fontweight='bold')\n",
    "    axes[0].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Top 20 states by total amount\n",
    "    top_states_amount = df.groupby(state_col)[payment_col].sum().sort_values(ascending=False).head(20)\n",
    "    axes[1].barh(range(len(top_states_amount)), top_states_amount.values,\n",
    "                 color=sns.color_palette('mako', len(top_states_amount)),\n",
    "                 edgecolor='black', alpha=0.7)\n",
    "    axes[1].set_yticks(range(len(top_states_amount)))\n",
    "    axes[1].set_yticklabels(top_states_amount.index)\n",
    "    axes[1].set_xlabel('Total Payment Amount ($)', fontsize=11, fontweight='bold')\n",
    "    axes[1].set_title('Top 20 States by Total Payment Amount', fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d2c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive choropleth map\n",
    "if state_col in df.columns and payment_col in df.columns:\n",
    "    # Aggregate by state\n",
    "    state_summary = df.groupby(state_col).agg({\n",
    "        payment_col: ['count', 'sum', 'mean']\n",
    "    }).round(2)\n",
    "    \n",
    "    state_summary.columns = ['Payment_Count', 'Total_Amount', 'Avg_Amount']\n",
    "    state_summary = state_summary.reset_index()\n",
    "    \n",
    "    # Create choropleth\n",
    "    fig = px.choropleth(\n",
    "        state_summary,\n",
    "        locations=state_col,\n",
    "        locationmode='USA-states',\n",
    "        color='Total_Amount',\n",
    "        hover_name=state_col,\n",
    "        hover_data={'Payment_Count': ':,', 'Total_Amount': ':$,.0f', 'Avg_Amount': ':$,.2f'},\n",
    "        color_continuous_scale='Viridis',\n",
    "        scope='usa',\n",
    "        title='Total Payment Amount by State'\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        geo=dict(bgcolor='rgba(0,0,0,0)'),\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24b888e",
   "metadata": {},
   "source": [
    "## 7. Geographic Analysis\n",
    "\n",
    "Explore payment patterns across different geographic regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2409ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aggregated features by recipient\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURE ENGINEERING: AGGREGATED RECIPIENT METRICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Identify recipient ID column\n",
    "recipient_id_cols = [col for col in df.columns if 'recipient' in col.lower() and 'id' in col.lower()]\n",
    "print(f\"\\nAvailable recipient ID columns: {recipient_id_cols}\")\n",
    "\n",
    "if len(recipient_id_cols) > 0:\n",
    "    recipient_id_col = recipient_id_cols[0]  # Use first available ID column\n",
    "    print(f\"Using: {recipient_id_col}\")\n",
    "    \n",
    "    # Create aggregated features\n",
    "    agg_features = df.groupby(recipient_id_col).agg({\n",
    "        payment_col: ['count', 'sum', 'mean', 'median', 'std', 'min', 'max']\n",
    "    }).round(2)\n",
    "    \n",
    "    agg_features.columns = ['_'.join(col).strip() for col in agg_features.columns.values]\n",
    "    agg_features = agg_features.reset_index()\n",
    "    \n",
    "    print(f\"\\nAggregated features created: {agg_features.shape}\")\n",
    "    display(agg_features.head(10))\n",
    "else:\n",
    "    print(\"⚠ No recipient ID column found for aggregation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87091e9",
   "metadata": {},
   "source": [
    "## 10. Advanced Visualizations\n",
    "\n",
    "Create sophisticated visualizations for deeper insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f4ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection using IQR method\n",
    "if payment_col in df.columns:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"OUTLIER DETECTION ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    amounts = df[payment_col].dropna()\n",
    "    \n",
    "    # Method 1: IQR (Interquartile Range)\n",
    "    Q1 = amounts.quantile(0.25)\n",
    "    Q3 = amounts.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    iqr_outliers = df[(df[payment_col] < lower_bound) | (df[payment_col] > upper_bound)]\n",
    "    \n",
    "    print(f\"\\nIQR Method:\")\n",
    "    print(f\"  Q1 (25th percentile): ${Q1:,.2f}\")\n",
    "    print(f\"  Q3 (75th percentile): ${Q3:,.2f}\")\n",
    "    print(f\"  IQR: ${IQR:,.2f}\")\n",
    "    print(f\"  Lower Bound: ${lower_bound:,.2f}\")\n",
    "    print(f\"  Upper Bound: ${upper_bound:,.2f}\")\n",
    "    print(f\"  Outliers Detected: {len(iqr_outliers):,} ({len(iqr_outliers)/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Method 2: Z-Score\n",
    "    z_scores = np.abs(stats.zscore(amounts))\n",
    "    z_threshold = 3\n",
    "    z_outliers = df[np.abs(stats.zscore(df[payment_col].fillna(0))) > z_threshold]\n",
    "    \n",
    "    print(f\"\\nZ-Score Method (threshold={z_threshold}):\")\n",
    "    print(f\"  Outliers Detected: {len(z_outliers):,} ({len(z_outliers)/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Method 3: Percentile-based\n",
    "    percentile_99 = amounts.quantile(0.99)\n",
    "    percentile_outliers = df[df[payment_col] > percentile_99]\n",
    "    \n",
    "    print(f\"\\nPercentile Method (99th percentile):\")\n",
    "    print(f\"  Threshold: ${percentile_99:,.2f}\")\n",
    "    print(f\"  Outliers Detected: {len(percentile_outliers):,} ({len(percentile_outliers)/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd760da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze outlier characteristics\n",
    "if payment_col in df.columns and len(iqr_outliers) > 0:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"OUTLIER CHARACTERISTICS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nPayment Amount Statistics for Outliers:\")\n",
    "    print(f\"  Count: {len(iqr_outliers):,}\")\n",
    "    print(f\"  Mean: ${iqr_outliers[payment_col].mean():,.2f}\")\n",
    "    print(f\"  Median: ${iqr_outliers[payment_col].median():,.2f}\")\n",
    "    print(f\"  Min: ${iqr_outliers[payment_col].min():,.2f}\")\n",
    "    print(f\"  Max: ${iqr_outliers[payment_col].max():,.2f}\")\n",
    "    \n",
    "    # Top outliers\n",
    "    print(f\"\\nTop 10 Outliers by Payment Amount:\")\n",
    "    top_outliers = iqr_outliers.nlargest(10, payment_col)[\n",
    "        [col for col in [payment_col, recipient_type_col, state_col, nature_col] \n",
    "         if col in iqr_outliers.columns]\n",
    "    ]\n",
    "    display(top_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc30da8",
   "metadata": {},
   "source": [
    "## 8. Feature Engineering\n",
    "\n",
    "Create new features for predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca9bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive scatter plot - Payment amount vs Count\n",
    "if payment_col in df.columns and recipient_type_col in df.columns:\n",
    "    # Aggregate by recipient\n",
    "    if len(recipient_id_cols) > 0:\n",
    "        scatter_data = df.groupby([recipient_id_cols[0], recipient_type_col]).agg({\n",
    "            payment_col: ['count', 'sum']\n",
    "        }).reset_index()\n",
    "        \n",
    "        scatter_data.columns = ['Recipient_ID', 'Recipient_Type', 'Payment_Count', 'Total_Amount']\n",
    "        \n",
    "        fig = px.scatter(\n",
    "            scatter_data.head(1000),  # Limit for performance\n",
    "            x='Payment_Count',\n",
    "            y='Total_Amount',\n",
    "            color='Recipient_Type',\n",
    "            size='Total_Amount',\n",
    "            hover_data=['Recipient_ID'],\n",
    "            title='Payment Frequency vs Total Amount by Recipient Type',\n",
    "            labels={'Payment_Count': 'Number of Payments', 'Total_Amount': 'Total Payment Amount ($)'},\n",
    "            height=600\n",
    "        )\n",
    "        \n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e87ed",
   "metadata": {},
   "source": [
    "## 9. Outlier Detection\n",
    "\n",
    "Identify and analyze outlier payments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd6c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "print(\"=\" * 70)\n",
    "print(\"KEY FINDINGS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. DATASET OVERVIEW\")\n",
    "print(f\"   - Total Records: {df.shape[0]:,}\")\n",
    "print(f\"   - Total Columns: {df.shape[1]}\")\n",
    "print(f\"   - Data Completeness: {(1 - df.isnull().sum().sum() / (df.shape[0] * df.shape[1]))*100:.1f}%\")\n",
    "\n",
    "if payment_col in df.columns:\n",
    "    print(\"\\n2. PAYMENT STATISTICS\")\n",
    "    print(f\"   - Total Payment Amount: ${df[payment_col].sum():,.2f}\")\n",
    "    print(f\"   - Average Payment: ${df[payment_col].mean():,.2f}\")\n",
    "    print(f\"   - Median Payment: ${df[payment_col].median():,.2f}\")\n",
    "    print(f\"   - Payment Range: ${df[payment_col].min():,.2f} - ${df[payment_col].max():,.2f}\")\n",
    "\n",
    "if recipient_type_col in df.columns:\n",
    "    print(\"\\n3. RECIPIENT DISTRIBUTION\")\n",
    "    type_counts = df[recipient_type_col].value_counts()\n",
    "    for rtype, count in type_counts.items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f\"   - {rtype}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "if 'Payment_Month' in df.columns:\n",
    "    print(\"\\n4. TEMPORAL PATTERNS\")\n",
    "    month_with_most = df.groupby('Payment_Month')[payment_col].sum().idxmax()\n",
    "    month_with_least = df.groupby('Payment_Month')[payment_col].sum().idxmin()\n",
    "    print(f\"   - Highest payment month: {month_with_most}\")\n",
    "    print(f\"   - Lowest payment month: {month_with_least}\")\n",
    "\n",
    "if state_col in df.columns:\n",
    "    print(\"\\n5. GEOGRAPHIC DISTRIBUTION\")\n",
    "    print(f\"   - Number of states: {df[state_col].nunique()}\")\n",
    "    top_state = df.groupby(state_col)[payment_col].sum().idxmax()\n",
    "    top_state_amount = df.groupby(state_col)[payment_col].sum().max()\n",
    "    print(f\"   - Top state by total amount: {top_state} (${top_state_amount:,.2f})\")\n",
    "\n",
    "print(\"\\n6. DATA QUALITY OBSERVATIONS\")\n",
    "high_missing = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False).head(3)\n",
    "print(f\"   - Columns with highest missingness:\")\n",
    "for col, pct in high_missing.items():\n",
    "    if pct > 0:\n",
    "        print(f\"     • {col}: {pct:.1f}%\")\n",
    "\n",
    "print(\"\\n7. OUTLIER ANALYSIS\")\n",
    "if payment_col in df.columns:\n",
    "    print(f\"   - IQR outliers: {len(iqr_outliers):,} ({len(iqr_outliers)/len(df)*100:.2f}%)\")\n",
    "    print(f\"   - 99th percentile threshold: ${percentile_99:,.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
