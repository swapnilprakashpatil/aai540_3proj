{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebc95b1c",
   "metadata": {},
   "source": [
    "# CMS Open Payments Data Exploration & Analysis\n",
    "\n",
    "**Project:** AAI-540 Machine Learning Operations - Final Team Project  \n",
    "**Dataset:** CMS Open Payments Program Year 2024 General Payments  \n",
    "**Purpose:** Exploratory Data Analysis for Anomaly Detection and Risk Scoring\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Environment Setup & Configuration](#setup)\n",
    "2. [Data Loading from Datalake](#loading)\n",
    "3. [Data Cleaning & Preprocessing](#cleaning)\n",
    "4. [Data Quality Assessment](#quality)\n",
    "5. [Exploratory Data Analysis](#eda)\n",
    "6. [Correlation Analysis](#correlation)\n",
    "7. [Geographic Analysis](#geographic)\n",
    "\n",
    "---\n",
    "\n",
    "## Project Objectives\n",
    "\n",
    "- Explore and understand CMS Open Payments data patterns\n",
    "- Identify unusual payment patterns and outliers through visualization\n",
    "- Assess data quality and completeness\n",
    "- Analyze payment distributions across recipients, geography, and time\n",
    "- Prepare insights for feature engineering and model development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386ba112",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Configuration\n",
    "\n",
    "Setting up the environment with necessary libraries and AWS integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb5f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -r ../requirements.txt --quiet\n",
    "!pip install boto3 sagemaker awswrangler pyathena --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e213c954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from scipy import stats\n",
    "import boto3\n",
    "import sagemaker\n",
    "import awswrangler as wr\n",
    "from pyathena import connect\n",
    "from sagemaker.session import Session\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "parent_dir = Path.cwd().parent\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab2e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from utils.visualizations import PaymentVisualizer\n",
    "    visualizer = PaymentVisualizer()\n",
    "    use_visualizer = True\n",
    "except ImportError:\n",
    "    use_visualizer = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd72de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r bucket\n",
    "%store -r region\n",
    "%store -r database_name\n",
    "%store -r table_name_parquet\n",
    "%store -r s3_parquet_path\n",
    "%store -r s3_athena_staging\n",
    "\n",
    "try:\n",
    "    test_vars = [bucket, region, database_name, table_name_parquet]\n",
    "    print(f\"Region: {region} | Bucket: {bucket} | Database: {database_name}\")\n",
    "except NameError:\n",
    "    boto_session = boto3.Session()\n",
    "    region = boto_session.region_name\n",
    "    sts_client = boto3.client('sts')\n",
    "    account_id = sts_client.get_caller_identity().get('Account')\n",
    "    \n",
    "    bucket = \"cmsopenpaymentsystemslight\"\n",
    "    database_name = \"cms_open_payments_light\"\n",
    "    table_name_parquet = \"general_payments_parquet\"\n",
    "    cms_data_prefix = \"cms-open-payments_light\"\n",
    "    parquet_data_prefix = f\"{cms_data_prefix}/parquet\"\n",
    "    s3_parquet_path = f\"s3://{bucket}/{parquet_data_prefix}\"\n",
    "    s3_athena_staging = f\"s3://{bucket}/athena/staging\"\n",
    "    \n",
    "    print(f\"Region: {region} | Bucket: {bucket} | Database: {database_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ae993",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_from_nb01 = {\n",
    "    'bucket': 'S3 bucket name',\n",
    "    'region': 'AWS region',\n",
    "    'database_name': 'Athena database name',\n",
    "    'table_name_parquet': 'Parquet table name',\n",
    "    's3_parquet_path': 'S3 parquet path',\n",
    "    's3_athena_staging': 'Athena staging path'\n",
    "}\n",
    "\n",
    "missing_vars = []\n",
    "for var_name, description in required_from_nb01.items():\n",
    "    try:\n",
    "        eval(var_name)\n",
    "    except NameError:\n",
    "        missing_vars.append((var_name, description))\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"WARNING: {len(missing_vars)} prerequisites missing. Run notebook 01 first.\")\n",
    "else:\n",
    "    print(\"Prerequisites validated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381c31ff",
   "metadata": {},
   "source": [
    "## 2. Data Loading from Datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f841c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_conn = connect(\n",
    "    region_name=region,\n",
    "    s3_staging_dir=s3_athena_staging\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb303736",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_full_dataset = True\n",
    "\n",
    "if load_full_dataset:\n",
    "    df = wr.athena.read_sql_query(\n",
    "        sql=f\"SELECT * FROM {database_name}.{table_name_parquet}\",\n",
    "        database=database_name,\n",
    "        ctas_approach=False\n",
    "    )\n",
    "    print(f\"Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f7ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1_000_000 if load_full_dataset else 100_000\n",
    "\n",
    "sample_query = f\"\"\"\n",
    "SELECT *\n",
    "FROM {database_name}.{table_name_parquet}\n",
    "LIMIT {sample_size}\n",
    "\"\"\"\n",
    "\n",
    "df = wr.athena.read_sql_query(\n",
    "    sql=sample_query,\n",
    "    database=database_name,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "print(f\"Sample loaded: {df.shape[0]:,} rows, {df.shape[1]} columns | Memory: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd6c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head(3))\n",
    "print(f\"Shape: {df.shape} | Columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913ec6c2",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning & Preprocessing\n",
    "\n",
    "Prepare data for anomaly detection models by cleaning, standardizing, and selecting relevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1bb8b3",
   "metadata": {},
   "source": [
    "### 3.1 Initial Data Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e31087",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_shape = df.shape\n",
    "initial_columns = df.shape[1]\n",
    "\n",
    "print(f\"Shape: {initial_shape} | Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"\\nData Types:\\n{df.dtypes.value_counts()}\")\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percent': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "missing_summary = missing_summary[missing_summary['Missing_Count'] > 0].sort_values(\n",
    "    'Missing_Percent', ascending=False\n",
    ")\n",
    "\n",
    "if len(missing_summary) > 0:\n",
    "    print(f\"\\nMissing Values: {len(missing_summary)}/{len(df.columns)} columns\")\n",
    "    display(missing_summary.head(10))\n",
    "\n",
    "print(f\"\\nDuplicates: {df.duplicated().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8764b30",
   "metadata": {},
   "source": [
    "### 3.2 Feature Selection for Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_PAYMENT_FEATURES = [\n",
    "    'total_amount_of_payment_usdollars',\n",
    "    'number_of_payments_included_in_total_amount',\n",
    "    'date_of_payment'\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'covered_recipient_type',\n",
    "    'nature_of_payment_or_transfer_of_value',\n",
    "    'form_of_payment_or_transfer_of_value',\n",
    "    'physician_specialty',\n",
    "    'recipient_state'\n",
    "]\n",
    "\n",
    "IDENTIFIER_FEATURES = [\n",
    "    'covered_recipient_profile_id',\n",
    "    'covered_recipient_npi',\n",
    "    'applicable_manufacturer_or_applicable_gpo_making_payment_name'\n",
    "]\n",
    "\n",
    "RISK_INDICATOR_FEATURES = [\n",
    "    'physician_ownership_indicator',\n",
    "    'third_party_payment_recipient_indicator',\n",
    "    'product_indicator'\n",
    "]\n",
    "\n",
    "all_selected_features = (CORE_PAYMENT_FEATURES + CATEGORICAL_FEATURES + \n",
    "                         IDENTIFIER_FEATURES + RISK_INDICATOR_FEATURES)\n",
    "\n",
    "available_features = [f for f in all_selected_features if f in df.columns]\n",
    "missing_features = [f for f in all_selected_features if f not in df.columns]\n",
    "\n",
    "print(f\"Features: {len(available_features)}/{len(all_selected_features)} available\")\n",
    "if missing_features:\n",
    "    print(f\"Missing: {missing_features}\")\n",
    "\n",
    "df_selected = df[available_features].copy()\n",
    "print(f\"Selected: {df_selected.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cadf093",
   "metadata": {},
   "source": [
    "### 3.3 Data Type Conversion and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2ef998",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'total_amount_of_payment_usdollars' in df_selected.columns:\n",
    "    df_selected['total_amount_of_payment_usdollars'] = pd.to_numeric(\n",
    "        df_selected['total_amount_of_payment_usdollars'], errors='coerce'\n",
    "    )\n",
    "    negative_count = (df_selected['total_amount_of_payment_usdollars'] < 0).sum()\n",
    "    if negative_count > 0:\n",
    "        df_selected['total_amount_of_payment_usdollars'] = df_selected['total_amount_of_payment_usdollars'].abs()\n",
    "\n",
    "if 'date_of_payment' in df_selected.columns:\n",
    "    df_selected['date_of_payment'] = pd.to_datetime(df_selected['date_of_payment'], errors='coerce')\n",
    "\n",
    "if 'number_of_payments_included_in_total_amount' in df_selected.columns:\n",
    "    df_selected['number_of_payments_included_in_total_amount'] = pd.to_numeric(\n",
    "        df_selected['number_of_payments_included_in_total_amount'], errors='coerce'\n",
    "    ).fillna(1).astype('int64')\n",
    "\n",
    "indicator_mapping = {'Yes': 1, 'Y': 1, 'No': 0, 'N': 0, 'Unknown': 0}\n",
    "for col in RISK_INDICATOR_FEATURES:\n",
    "    if col in df_selected.columns and df_selected[col].dtype == 'object':\n",
    "        df_selected[col] = df_selected[col].map(indicator_mapping).fillna(0).astype('int64')\n",
    "\n",
    "if 'recipient_state' in df_selected.columns:\n",
    "    df_selected['recipient_state'] = df_selected['recipient_state'].str.upper().str.strip()\n",
    "\n",
    "print(\"Data types converted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5738f554",
   "metadata": {},
   "source": [
    "### 3.4 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c172706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_before = df_selected.isnull().sum().sum()\n",
    "\n",
    "if 'total_amount_of_payment_usdollars' in df_selected.columns:\n",
    "    payment_median = df_selected['total_amount_of_payment_usdollars'].median()\n",
    "    missing_amt = df_selected['total_amount_of_payment_usdollars'].isnull().sum()\n",
    "    if missing_amt > 0:\n",
    "        df_selected['total_amount_of_payment_usdollars'].fillna(payment_median, inplace=True)\n",
    "\n",
    "if 'date_of_payment' in df_selected.columns:\n",
    "    missing_dates = df_selected['date_of_payment'].isnull().sum()\n",
    "    if missing_dates > 0:\n",
    "        df_selected['date_of_payment'].fillna(method='ffill', inplace=True)\n",
    "        df_selected['date_of_payment'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "for col in df_selected.select_dtypes(include=['object']).columns:\n",
    "    missing_count = df_selected[col].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        if col in ['covered_recipient_type', 'nature_of_payment_or_transfer_of_value']:\n",
    "            mode_val = df_selected[col].mode()[0] if len(df_selected[col].mode()) > 0 else \"Unknown\"\n",
    "            df_selected[col].fillna(mode_val, inplace=True)\n",
    "        else:\n",
    "            df_selected[col].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "for col in df_selected.select_dtypes(include=[np.number]).columns:\n",
    "    missing_count = df_selected[col].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        if 'count' in col.lower() or 'number' in col.lower():\n",
    "            df_selected[col].fillna(0, inplace=True)\n",
    "        else:\n",
    "            df_selected[col].fillna(df_selected[col].median(), inplace=True)\n",
    "\n",
    "missing_after = df_selected.isnull().sum().sum()\n",
    "print(f\"Missing values resolved: {missing_before - missing_after:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c2754a",
   "metadata": {},
   "source": [
    "### 3.5 Remove Invalid Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f742f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_before = len(df_selected)\n",
    "\n",
    "duplicates = df_selected.duplicated().sum()\n",
    "if duplicates > 0:\n",
    "    df_selected = df_selected.drop_duplicates()\n",
    "\n",
    "if 'total_amount_of_payment_usdollars' in df_selected.columns:\n",
    "    zero_payments = (df_selected['total_amount_of_payment_usdollars'] == 0).sum()\n",
    "    if zero_payments > 0:\n",
    "        df_selected = df_selected[df_selected['total_amount_of_payment_usdollars'] > 0]\n",
    "\n",
    "if 'date_of_payment' in df_selected.columns:\n",
    "    invalid_dates = df_selected['date_of_payment'].isnull().sum()\n",
    "    if invalid_dates > 0:\n",
    "        df_selected = df_selected[df_selected['date_of_payment'].notnull()]\n",
    "\n",
    "if 'covered_recipient_profile_id' in df_selected.columns:\n",
    "    missing_id = df_selected['covered_recipient_profile_id'].isnull().sum()\n",
    "    if missing_id > 0:\n",
    "        df_selected = df_selected[df_selected['covered_recipient_profile_id'].notnull()]\n",
    "\n",
    "df_selected = df_selected.reset_index(drop=True)\n",
    "records_after = len(df_selected)\n",
    "records_removed = records_before - records_after\n",
    "\n",
    "print(f\"Records: {records_after:,} | Removed: {records_removed:,} ({records_removed/records_before*100:.2f}%) | Retention: {records_after/records_before*100:.2f}%\")\n",
    "\n",
    "df = df_selected.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1334c65",
   "metadata": {},
   "source": [
    "### 3.6 Data Cleaning Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a28011",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Initial Records',\n",
    "        'Final Records',\n",
    "        'Records Removed',\n",
    "        'Initial Columns',\n",
    "        'Final Columns',\n",
    "        'Missing Values',\n",
    "        'Duplicates',\n",
    "        'Data Completeness (%)'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{initial_shape[0]:,}\",\n",
    "        f\"{df.shape[0]:,}\",\n",
    "        f\"{initial_shape[0] - df.shape[0]:,}\",\n",
    "        initial_columns,\n",
    "        df.shape[1],\n",
    "        f\"{df.isnull().sum().sum():,}\",\n",
    "        f\"{df.duplicated().sum():,}\",\n",
    "        f\"{(1 - df.isnull().sum().sum()/(df.shape[0]*df.shape[1]))*100:.2f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "display(pd.DataFrame(summary_data))\n",
    "\n",
    "feature_data = {\n",
    "    'Category': ['Core Payment', 'Categorical', 'Risk Indicators', 'Identifiers'],\n",
    "    'Count': [\n",
    "        len([f for f in CORE_PAYMENT_FEATURES if f in df.columns]),\n",
    "        len([f for f in CATEGORICAL_FEATURES if f in df.columns]),\n",
    "        len([f for f in RISK_INDICATOR_FEATURES if f in df.columns]),\n",
    "        len([f for f in IDENTIFIER_FEATURES if f in df.columns])\n",
    "    ]\n",
    "}\n",
    "\n",
    "display(pd.DataFrame(feature_data))\n",
    "\n",
    "if 'total_amount_of_payment_usdollars' in df.columns:\n",
    "    payment_stats = df['total_amount_of_payment_usdollars']\n",
    "    payment_data = {\n",
    "        'Statistic': ['Total', 'Mean', 'Median', 'Min', 'Max'],\n",
    "        'Amount ($)': [\n",
    "            f\"{payment_stats.sum():,.2f}\",\n",
    "            f\"{payment_stats.mean():,.2f}\",\n",
    "            f\"{payment_stats.median():,.2f}\",\n",
    "            f\"{payment_stats.min():,.2f}\",\n",
    "            f\"{payment_stats.max():,.2f}\"\n",
    "        ]\n",
    "    }\n",
    "    display(pd.DataFrame(payment_data))\n",
    "\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa411aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_date_col = 'date_of_payment'\n",
    "if payment_date_col in df.columns:\n",
    "    df['payment_year'] = df[payment_date_col].dt.year\n",
    "    df['payment_month'] = df[payment_date_col].dt.month\n",
    "    df['payment_quarter'] = df[payment_date_col].dt.quarter\n",
    "    df['payment_dayofweek'] = df[payment_date_col].dt.dayofweek\n",
    "    df['is_weekend'] = (df[payment_date_col].dt.dayofweek >= 5).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae527e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_col_options = ['total_amount_of_payment_usdollars', 'total_amount', 'amount', 'payment_amount']\n",
    "payment_col = None\n",
    "for col in payment_col_options:\n",
    "    if col in df.columns:\n",
    "        payment_col = col\n",
    "        break\n",
    "\n",
    "if payment_col is None:\n",
    "    amount_cols = [col for col in df.columns if 'amount' in col.lower()]\n",
    "    if len(amount_cols) > 0:\n",
    "        payment_col = amount_cols[0]\n",
    "\n",
    "recipient_type_col_options = ['covered_recipient_type', 'recipient_type', 'recipienttype']\n",
    "recipient_type_col = None\n",
    "for col in recipient_type_col_options:\n",
    "    if col in df.columns:\n",
    "        recipient_type_col = col\n",
    "        break\n",
    "\n",
    "if recipient_type_col is None:\n",
    "    type_cols = [col for col in df.columns if 'recipient' in col.lower() and 'type' in col.lower()]\n",
    "    if len(type_cols) > 0:\n",
    "        recipient_type_col = type_cols[0]\n",
    "\n",
    "recipient_id_cols = [col for col in df.columns if 'recipient' in col.lower() and 'id' in col.lower()]\n",
    "\n",
    "state_col_options = ['recipient_state', 'state']\n",
    "state_col = None\n",
    "for col in state_col_options:\n",
    "    if col in df.columns:\n",
    "        state_col = col\n",
    "        break\n",
    "\n",
    "if state_col is None:\n",
    "    state_cols = [col for col in df.columns if 'state' in col.lower() and 'recipient' in col.lower()]\n",
    "    if len(state_cols) > 0:\n",
    "        state_col = state_cols[0]\n",
    "\n",
    "city_col_options = ['recipient_City', 'city']\n",
    "city_col = None\n",
    "for col in city_col_options:\n",
    "    if col in df.columns:\n",
    "        city_col = col\n",
    "        break\n",
    "\n",
    "if city_col is None:\n",
    "    city_cols = [col for col in df.columns if 'city' in col.lower() and 'recipient' in col.lower()]\n",
    "    if len(city_cols) > 0:\n",
    "        city_col = city_cols[0]\n",
    "\n",
    "print(f\"Payment: {payment_col} | Recipient Type: {recipient_type_col} | State: {state_col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95e1db0",
   "metadata": {},
   "source": [
    "## 4. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cefa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cells = df.shape[0] * df.shape[1]\n",
    "missing_cells = df.isnull().sum().sum()\n",
    "duplicates = df.duplicated().sum()\n",
    "\n",
    "print(f\"Dimensions: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "print(f\"Data Types: Numeric={len(df.select_dtypes(include=['number']).columns)} | Object={len(df.select_dtypes(include=['object']).columns)} | DateTime={len(df.select_dtypes(include=['datetime']).columns)}\")\n",
    "print(f\"Missing: {missing_cells:,}/{total_cells:,} ({(missing_cells/total_cells)*100:.2f}%)\")\n",
    "print(f\"Duplicates: {duplicates:,} ({(duplicates/df.shape[0])*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786923bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_stats = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum().values,\n",
    "    'Missing_Percent': (df.isnull().sum().values / len(df) * 100)\n",
    "})\n",
    "\n",
    "missing_stats = missing_stats[missing_stats['Missing_Count'] > 0].sort_values(\n",
    "    'Missing_Percent', ascending=False\n",
    ")\n",
    "\n",
    "display(missing_stats.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a83f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(missing_stats) > 0 and use_visualizer:\n",
    "    fig = visualizer.plot_missing_values(df, top_n=20)\n",
    "    if fig:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd42b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e599c779",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis\n",
    "\n",
    "Analyze distributions and patterns in cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7002d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "if payment_col and payment_col in df.columns:\n",
    "    payment_stats = df[payment_col].describe()\n",
    "    \n",
    "    print(f\"Count: {payment_stats['count']:,.0f}\")\n",
    "    print(f\"Mean: ${payment_stats['mean']:,.2f} | Median: ${payment_stats['50%']:,.2f} | Std: ${payment_stats['std']:,.2f}\")\n",
    "    print(f\"Min: ${payment_stats['min']:,.2f} | Max: ${payment_stats['max']:,.2f}\")\n",
    "    print(f\"Quartiles: 25%=${payment_stats['25%']:,.2f} | 50%=${payment_stats['50%']:,.2f} | 75%=${payment_stats['75%']:,.2f}\")\n",
    "    \n",
    "    for p in [90, 95, 99]:\n",
    "        print(f\"{p}th percentile: ${df[payment_col].quantile(p/100):,.2f}\", end=' | ')\n",
    "    print()\n",
    "    \n",
    "    print(f\"Skewness: {df[payment_col].skew():.2f} | Kurtosis: {df[payment_col].kurtosis():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f212fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if payment_col and payment_col in df.columns and use_visualizer:\n",
    "    visualizer.plot_payment_distribution_detailed(df, payment_col=payment_col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b864d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    'change_type',\n",
    "    'covered_recipient_type',\n",
    "    'form_of_payment_or_transfer_of_value',\n",
    "    'nature_of_payment_or_transfer_of_value'\n",
    "]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        value_counts = df[col].value_counts().head(10)\n",
    "        print(f\"\\n{col}: {df[col].nunique()} unique | Missing: {df[col].isnull().sum()} ({df[col].isnull().sum()/len(df)*100:.1f}%)\")\n",
    "        print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7614bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols_viz = [\n",
    "    'covered_recipient_type',\n",
    "    'form_of_payment_or_transfer_of_value',\n",
    "    'nature_of_payment_or_transfer_of_value'\n",
    "]\n",
    "\n",
    "for col in categorical_cols_viz:\n",
    "    if col in df.columns and use_visualizer:\n",
    "        visualizer.plot_category_distribution(df, col, top_n=15)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e6b9e3",
   "metadata": {},
   "source": [
    "### 5.1 Bivariate Analysis\n",
    "\n",
    "Explore relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3619d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payment amount by recipient type\n",
    "recipient_type_col = 'covered_recipient_type'\n",
    "\n",
    "if recipient_type_col in df.columns and payment_col in df.columns:\n",
    "    \n",
    "    type_stats = df.groupby(recipient_type_col)[payment_col].agg([\n",
    "        'count', 'sum', 'mean', 'median', 'std', 'min', 'max'\n",
    "    ]).round(2)\n",
    "    \n",
    "    type_stats.columns = ['Count', 'Total ($)', 'Mean ($)', 'Median ($)', 'Std Dev ($)', 'Min ($)', 'Max ($)']\n",
    "    type_stats = type_stats.sort_values('Total ($)', ascending=False)\n",
    "    \n",
    "    display(type_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if recipient_type_col and recipient_type_col in df.columns and payment_col and payment_col in df.columns and use_visualizer:\n",
    "    visualizer.plot_bivariate_comparison(df, group_col=recipient_type_col, amount_col=payment_col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb192315",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_visualizer:\n",
    "    try:\n",
    "        visualizer.plot_payment_nature_by_total(\n",
    "            df,\n",
    "            nature_col='nature_of_payment_or_transfer_of_value',\n",
    "            amount_col='total_amount_of_payment_usdollars',\n",
    "            top_n=15\n",
    "        )\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb19d11",
   "metadata": {},
   "source": [
    "### 5.2 Temporal Analysis\n",
    "\n",
    "Analyze payment patterns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3419eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if payment_col and payment_col in df.columns:\n",
    "    numeric_features = [payment_col]\n",
    "    potential_numeric = ['number_of_payments_included_in_total_amount', \n",
    "                        'payment_month', 'payment_quarter', 'payment_dayofweek']\n",
    "    \n",
    "    for col in potential_numeric:\n",
    "        if col in df.columns:\n",
    "            numeric_features.append(col)\n",
    "    \n",
    "    if len(numeric_features) > 2 and use_visualizer:\n",
    "        visualizer.plot_correlation_heatmap(df, numeric_features)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b1d52",
   "metadata": {},
   "source": [
    "### Correlation Analysis\n",
    "\n",
    "Explore correlations between numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'payment_month' in df.columns and payment_col in df.columns:\n",
    "    monthly_stats = df.groupby('payment_month')[payment_col].agg([\n",
    "        'count', 'sum', 'mean', 'median'\n",
    "    ]).round(2)\n",
    "    monthly_stats.columns = ['Count', 'Total ($)', 'Mean ($)', 'Median ($)']\n",
    "    monthly_stats.index.name = 'Month'\n",
    "    display(monthly_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a6d40",
   "metadata": {},
   "source": [
    "### Monthly Payment Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77eb4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'date_of_payment' in df.columns and payment_col and payment_col in df.columns and use_visualizer:\n",
    "    visualizer.plot_temporal_trends(df, date_col='date_of_payment', amount_col=payment_col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb90451c",
   "metadata": {},
   "source": [
    "### Temporal Trends Overview\n",
    "\n",
    "Visualize overall temporal patterns in payments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c937b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'payment_month' in df.columns and payment_col and payment_col in df.columns and use_visualizer:\n",
    "    visualizer.plot_monthly_trends(df, payment_col=payment_col, month_col='payment_month')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a6f4a5",
   "metadata": {},
   "source": [
    "### Quarterly Payment Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06345402",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'payment_quarter' in df.columns and payment_col and payment_col in df.columns:\n",
    "    quarterly_stats = df.groupby('payment_quarter')[payment_col].agg([\n",
    "        'count', 'sum', 'mean', 'median'\n",
    "    ]).round(2)\n",
    "    quarterly_stats.columns = ['Count', 'Total ($)', 'Mean ($)', 'Median ($)']\n",
    "    quarterly_stats.index.name = 'Quarter'\n",
    "    display(quarterly_stats)\n",
    "    \n",
    "    if use_visualizer:\n",
    "        visualizer.plot_quarterly_trends(df, quarter_col='payment_quarter', amount_col=payment_col)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e94d54",
   "metadata": {},
   "source": [
    "## 7. Geographic Analysis\n",
    "\n",
    "Analyzing payment distributions across geographic regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da986ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_col = 'recipient_state'\n",
    "\n",
    "if state_col in df.columns and payment_col in df.columns:\n",
    "    state_stats = df.groupby(state_col)[payment_col].agg([\n",
    "        'count', 'sum', 'mean', 'median'\n",
    "    ]).round(2)\n",
    "    state_stats.columns = ['Count', 'Total ($)', 'Mean ($)', 'Median ($)']\n",
    "    state_stats = state_stats.sort_values('Total ($)', ascending=False).head(20)\n",
    "    state_stats.index.name = 'State'\n",
    "    display(state_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e3f10e",
   "metadata": {},
   "source": [
    "### State-Level Payment Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb962c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "if state_col and state_col in df.columns and use_visualizer:\n",
    "    visualizer.plot_geographic_distribution(df, state_col=state_col)\n",
    "    plt.show()\n",
    "    \n",
    "    if payment_col and payment_col in df.columns:\n",
    "        state_summary = df.groupby(state_col)[payment_col].agg(['count', 'sum', 'mean', 'median']).round(2)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        top_states = state_summary['sum'].nlargest(10)\n",
    "        sns.barplot(x=top_states.values, y=top_states.index)\n",
    "        plt.title('Top 10 States by Total Payment Amount')\n",
    "        plt.xlabel('Total Payment Amount ($)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928411b5",
   "metadata": {},
   "source": [
    "### Geographic Distribution Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d70f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if state_col and state_col in df.columns and payment_col and payment_col in df.columns and use_visualizer:\n",
    "    visualizer.plot_state_comparison(df, state_col=state_col, payment_col=payment_col, top_n=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07884e47",
   "metadata": {},
   "source": [
    "### Interactive Geographic Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2409ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipient_id_cols = [col for col in df.columns if 'recipient' in col.lower() and 'id' in col.lower()]\n",
    "\n",
    "if len(recipient_id_cols) > 0:\n",
    "    recipient_id_col = recipient_id_cols[0]\n",
    "    \n",
    "    agg_features = df.groupby(recipient_id_col).agg({\n",
    "        payment_col: ['count', 'sum', 'mean', 'median', 'std', 'min', 'max']\n",
    "    }).round(2)\n",
    "    \n",
    "    agg_features.columns = ['_'.join(col).strip() for col in agg_features.columns.values]\n",
    "    agg_features = agg_features.reset_index()\n",
    "    \n",
    "    print(f\"Aggregated features: {agg_features.shape}\")\n",
    "    display(agg_features.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
