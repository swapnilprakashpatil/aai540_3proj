{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebc95b1c",
   "metadata": {},
   "source": [
    "# CMS Open Payments Feature Engineering & Model Preparation\n",
    "\n",
    "**Project:** AAI-540 Machine Learning Operations - Final Team Project  \n",
    "**Dataset:** CMS Open Payments Program Year 2024 General Payments  \n",
    "**Purpose:** Feature Engineering and Data Preparation for Anomaly Detection Models\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Environment Setup & Configuration](#setup)\n",
    "2. [Data Loading from Datalake](#loading)\n",
    "3. [Data Cleaning & Preprocessing](#cleaning)\n",
    "4. [Data Partitioning (40/10/10/40)](#partitioning)\n",
    "5. [Feature Engineering for ML Models](#ml-features)\n",
    "6. [Outlier Detection](#outliers)\n",
    "7. [Feature Store & Feature Group Setup](#feature-store)\n",
    "\n",
    "---\n",
    "\n",
    "## Project Objectives\n",
    "\n",
    "- Prepare cleaned CMS Open Payments data for machine learning models\n",
    "- Engineer features for Isolation Forest, XGBoost, and Autoencoder models\n",
    "- Calculate risk scores and anomaly indicators for payment transactions\n",
    "- Partition data according to project requirements (40% train, 10% test, 10% validation, 40% production)\n",
    "- Store engineered features in Amazon SageMaker Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386ba112",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Configuration\n",
    "\n",
    "Setting up the environment with necessary libraries and AWS integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb5f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -r ../requirements.txt --quiet\n",
    "!pip install boto3 sagemaker awswrangler pyathena --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e213c954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from scipy import stats\n",
    "\n",
    "# AWS libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "import awswrangler as wr\n",
    "from pyathena import connect\n",
    "\n",
    "from sagemaker.session import Session\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Add parent directory to path for custom modules\n",
    "parent_dir = Path.cwd().parent\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "# Visualization settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab2e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom utilities\n",
    "try:\n",
    "    from utils.visualizations import PaymentVisualizer\n",
    "    visualizer = PaymentVisualizer()\n",
    "    print(\"Visualization utilities imported successfully\")\n",
    "    use_visualizer = True\n",
    "except ImportError as e:\n",
    "    print(f\"Could not import visualization utilities: {e}\")\n",
    "    print(\"Using standard plotting libraries only\")\n",
    "    use_visualizer = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd72de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore AWS configuration from datalake setup notebook\n",
    "%store -r bucket\n",
    "%store -r region\n",
    "%store -r database_name\n",
    "%store -r table_name_parquet\n",
    "%store -r s3_parquet_path\n",
    "%store -r s3_athena_staging\n",
    "\n",
    "# If variables not restored, set defaults matching datalake setup\n",
    "try:\n",
    "    # Test if variables exist\n",
    "    test_vars = [bucket, region, database_name, table_name_parquet]\n",
    "    \n",
    "    print(f\"AWS Configuration:\")\n",
    "    print(f\"  Region: {region}\")\n",
    "    print(f\"  S3 Bucket: {bucket}\")\n",
    "    print(f\"  Database: {database_name}\")\n",
    "    print(f\"  Table: {table_name_parquet}\")\n",
    "    print(f\"  Parquet Path: {s3_parquet_path}\")\n",
    "    print(f\"  Athena Staging: {s3_athena_staging}\")\n",
    "    \n",
    "except NameError as e:\n",
    "    print(f\"Variables not found in store. Setting up from AWS configuration...\")\n",
    "    \n",
    "    # Initialize AWS session\n",
    "    boto_session = boto3.Session()\n",
    "    region = boto_session.region_name\n",
    "    \n",
    "    # Get account information\n",
    "    sts_client = boto3.client('sts')\n",
    "    account_id = sts_client.get_caller_identity().get('Account')\n",
    "    \n",
    "    # Set configuration matching datalake setup\n",
    "    #bucket = f\"cmsopenpaymentsystems{account_id}\"\n",
    "    bucket = \"cmsopenpaymentsystemslight\" # changed to this to match Notebook 01 (JN)\n",
    "    database_name = \"cms_open_payments_light\"\n",
    "    table_name_parquet = \"general_payments_parquet\"\n",
    "    \n",
    "    # Define S3 paths\n",
    "    cms_data_prefix = \"cms-open-payments_light\"\n",
    "    parquet_data_prefix = f\"{cms_data_prefix}/parquet\"\n",
    "    s3_parquet_path = f\"s3://{bucket}/{parquet_data_prefix}\"\n",
    "    s3_athena_staging = f\"s3://{bucket}/athena/staging\"\n",
    "    \n",
    "    print(f\"\\nAWS Configuration (manual setup):\")\n",
    "    print(f\"  Region: {region}\")\n",
    "    print(f\"  Account ID: {account_id}\")\n",
    "    print(f\"  S3 Bucket: {bucket}\")\n",
    "    print(f\"  Database: {database_name}\")\n",
    "    print(f\"  Table: {table_name_parquet}\")\n",
    "    print(f\"  Parquet Path: {s3_parquet_path}\")\n",
    "    print(f\"  Athena Staging: {s3_athena_staging}\")\n",
    "    print(f\"\\nNote: Please run the datalake setup notebook (01_setup_cms_datalake.ipynb) first for full setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ae993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate prerequisites from notebook 01\n",
    "\n",
    "print(\"PREREQUISITE CHECK: Validating variables from notebook 01\")\n",
    "\n",
    "\n",
    "required_from_nb01 = {\n",
    "    'bucket': 'S3 bucket name',\n",
    "    'region': 'AWS region',\n",
    "    'database_name': 'Athena database name',\n",
    "    'table_name_parquet': 'Parquet table name',\n",
    "    's3_parquet_path': 'S3 parquet path',\n",
    "    's3_athena_staging': 'Athena staging path'\n",
    "}\n",
    "\n",
    "missing_vars = []\n",
    "for var_name, description in required_from_nb01.items():\n",
    "    try:\n",
    "        eval(var_name)\n",
    "        print(f\"{var_name}: {description}\")\n",
    "    except NameError:\n",
    "        missing_vars.append((var_name, description))\n",
    "        print(f\"{var_name}: {description} - MISSING\")\n",
    "\n",
    "if missing_vars:\n",
    "    \n",
    "    print(\"  WARNING: Missing prerequisites from notebook 01\")\n",
    "    \n",
    "    print(f\"\\nMissing {len(missing_vars)} required variables:\")\n",
    "    for var, desc in missing_vars:\n",
    "        print(f\"  - {var}: {desc}\")\n",
    "    print(\"\\nThese will be set to defaults, but you should run notebook 01 first:\")\n",
    "    print(\"  01_setup_cms_datalake.ipynb\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nAll prerequisites satisfied\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381c31ff",
   "metadata": {},
   "source": [
    "## 2. Data Loading from Datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f841c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Athena connection\n",
    "athena_conn = connect(\n",
    "    region_name=region,\n",
    "    s3_staging_dir=s3_athena_staging\n",
    ")\n",
    "\n",
    "print(\"Athena connection established\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb303736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load full dataset from Parquet\n",
    "load_full_dataset = True  # Set to True to load full dataset\n",
    "\n",
    "if load_full_dataset:\n",
    "    print(\"Loading full dataset from Parquet...\")\n",
    "    print(\"Note: This may take several minutes\")\n",
    "    \n",
    "    df = wr.athena.read_sql_query(\n",
    "        sql=f\"SELECT * FROM {database_name}.{table_name_parquet}\",\n",
    "        database=database_name,\n",
    "        ctas_approach=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Full dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "else:\n",
    "    print(\"Skipping full dataset load - will use sample queries instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f7ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Load sample dataset for faster EDA\n",
    "sample_size = 1_000_000 if load_full_dataset else 100_000\n",
    "\n",
    "print(f\"Loading sample dataset ({sample_size:,} rows)...\")\n",
    "\n",
    "sample_query = f\"\"\"\n",
    "SELECT *\n",
    "FROM {database_name}.{table_name_parquet}\n",
    "LIMIT {sample_size}\n",
    "\"\"\"\n",
    "\n",
    "df = wr.athena.read_sql_query(\n",
    "    sql=sample_query,\n",
    "    database=database_name,\n",
    "    ctas_approach=False\n",
    ")\n",
    "\n",
    "print(f\"Sample dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd6c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "print(\"Dataset Preview:\")\n",
    "display(df.head(3))\n",
    "\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Columns: {df.shape[1]}\")\n",
    "print(f\"\\nColumn Names:\")\n",
    "for i, col in enumerate(df.columns[:20], 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "if len(df.columns) > 20:\n",
    "    print(f\"  ... ({len(df.columns) - 20} more columns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913ec6c2",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning & Preprocessing\n",
    "\n",
    "Prepare data for anomaly detection models by cleaning, standardizing, and selecting relevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1bb8b3",
   "metadata": {},
   "source": [
    "### 3.1 Initial Data Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e31087",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_shape = df.shape\n",
    "initial_columns = df.shape[1]\n",
    "\n",
    "print(f\"\\nDataset Shape: {initial_shape}\")\n",
    "print(f\"Total Records: {initial_shape[0]:,}\")\n",
    "print(f\"Total Columns: {initial_columns}\")\n",
    "\n",
    "print(f\"\\nMemory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percent': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "missing_summary = missing_summary[missing_summary['Missing_Count'] > 0].sort_values(\n",
    "    'Missing_Percent', ascending=False\n",
    ")\n",
    "\n",
    "print(f\"Columns with missing values: {len(missing_summary)}/{len(df.columns)}\")\n",
    "if len(missing_summary) > 0:\n",
    "    print(\"\\nTop 10 columns with highest missingness:\")\n",
    "    display(missing_summary.head(10))\n",
    "\n",
    "print(f\"\\nDuplicate Records: {df.duplicated().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8764b30",
   "metadata": {},
   "source": [
    "### 3.2 Feature Selection for Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core features required for anomaly detection and risk scoring\n",
    "CORE_PAYMENT_FEATURES = [\n",
    "    'total_amount_of_payment_usdollars',\n",
    "    'number_of_payments_included_in_total_amount',\n",
    "    'date_of_payment'\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'covered_recipient_type',\n",
    "    'nature_of_payment_or_transfer_of_value',\n",
    "    'form_of_payment_or_transfer_of_value',\n",
    "    'physician_specialty',\n",
    "    'recipient_state'\n",
    "]\n",
    "\n",
    "IDENTIFIER_FEATURES = [\n",
    "    'covered_recipient_profile_id',\n",
    "    'covered_recipient_npi',\n",
    "    'applicable_manufacturer_or_applicable_gpo_making_payment_name'\n",
    "]\n",
    "\n",
    "RISK_INDICATOR_FEATURES = [\n",
    "    'physician_ownership_indicator',\n",
    "    'third_party_payment_recipient_indicator',\n",
    "    'product_indicator'\n",
    "]\n",
    "\n",
    "# Verify feature availability\n",
    "all_selected_features = (CORE_PAYMENT_FEATURES + CATEGORICAL_FEATURES + \n",
    "                         IDENTIFIER_FEATURES + RISK_INDICATOR_FEATURES)\n",
    "\n",
    "available_features = [f for f in all_selected_features if f in df.columns]\n",
    "missing_features = [f for f in all_selected_features if f not in df.columns]\n",
    "\n",
    "print(f\"\\nFeature Availability:\")\n",
    "print(f\"Selected: {len(all_selected_features)}\")\n",
    "print(f\"Available: {len(available_features)}\")\n",
    "print(f\"Missing: {len(missing_features)}\")\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"\\nMissing features: {missing_features}\")\n",
    "\n",
    "# Keep only selected features\n",
    "df_selected = df[available_features].copy()\n",
    "\n",
    "print(f\"\\nSelected dataset shape: {df_selected.shape}\")\n",
    "print(f\"Columns retained: {df_selected.shape[1]}\")\n",
    "print(f\"Records: {df_selected.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cadf093",
   "metadata": {},
   "source": [
    "### 3.3 Data Type Conversion and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2ef998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert payment amount to numeric\n",
    "if 'total_amount_of_payment_usdollars' in df_selected.columns:\n",
    "    df_selected['total_amount_of_payment_usdollars'] = pd.to_numeric(\n",
    "        df_selected['total_amount_of_payment_usdollars'], errors='coerce'\n",
    "    )\n",
    "    # Handle negative amounts\n",
    "    negative_count = (df_selected['total_amount_of_payment_usdollars'] < 0).sum()\n",
    "    if negative_count > 0:\n",
    "        print(f\"Converting {negative_count:,} negative amounts to absolute values\")\n",
    "        df_selected['total_amount_of_payment_usdollars'] = df_selected['total_amount_of_payment_usdollars'].abs()\n",
    "\n",
    "# Convert date to datetime\n",
    "if 'date_of_payment' in df_selected.columns:\n",
    "    df_selected['date_of_payment'] = pd.to_datetime(df_selected['date_of_payment'], errors='coerce')\n",
    "    print(f\"Converted date_of_payment to datetime\")\n",
    "\n",
    "# Convert payment count to integer\n",
    "if 'number_of_payments_included_in_total_amount' in df_selected.columns:\n",
    "    df_selected['number_of_payments_included_in_total_amount'] = pd.to_numeric(\n",
    "        df_selected['number_of_payments_included_in_total_amount'], errors='coerce'\n",
    "    ).fillna(1).astype('int64')\n",
    "\n",
    "# Standardize indicator fields to binary\n",
    "indicator_mapping = {'Yes': 1, 'Y': 1, 'No': 0, 'N': 0, 'Unknown': 0}\n",
    "\n",
    "for col in RISK_INDICATOR_FEATURES:\n",
    "    if col in df_selected.columns:\n",
    "        if df_selected[col].dtype == 'object':\n",
    "            df_selected[col] = df_selected[col].map(indicator_mapping).fillna(0).astype('int64')\n",
    "            print(f\"Standardized {col} to binary\")\n",
    "\n",
    "# Standardize state codes\n",
    "if 'recipient_state' in df_selected.columns:\n",
    "    df_selected['recipient_state'] = df_selected['recipient_state'].str.upper().str.strip()\n",
    "\n",
    "print(\"\\nData type conversion complete\")\n",
    "print(\"\\nFinal data types:\")\n",
    "print(df_selected.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5738f554",
   "metadata": {},
   "source": [
    "### 3.4 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c172706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_before = df_selected.isnull().sum().sum()\n",
    "\n",
    "# Fill payment amount with median\n",
    "if 'total_amount_of_payment_usdollars' in df_selected.columns:\n",
    "    payment_median = df_selected['total_amount_of_payment_usdollars'].median()\n",
    "    missing_amt = df_selected['total_amount_of_payment_usdollars'].isnull().sum()\n",
    "    if missing_amt > 0:\n",
    "        df_selected['total_amount_of_payment_usdollars'].fillna(payment_median, inplace=True)\n",
    "        print(f\"Filled {missing_amt:,} missing payment amounts with median: ${payment_median:.2f}\")\n",
    "\n",
    "# Fill date with forward/backward fill\n",
    "if 'date_of_payment' in df_selected.columns:\n",
    "    missing_dates = df_selected['date_of_payment'].isnull().sum()\n",
    "    if missing_dates > 0:\n",
    "        df_selected['date_of_payment'].fillna(method='ffill', inplace=True)\n",
    "        df_selected['date_of_payment'].fillna(method='bfill', inplace=True)\n",
    "        print(f\"Filled {missing_dates:,} missing dates\")\n",
    "\n",
    "# Fill categorical fields with mode or Unknown\n",
    "for col in df_selected.select_dtypes(include=['object']).columns:\n",
    "    missing_count = df_selected[col].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        if col in ['covered_recipient_type', 'nature_of_payment_or_transfer_of_value']:\n",
    "            mode_val = df_selected[col].mode()[0] if len(df_selected[col].mode()) > 0 else \"Unknown\"\n",
    "            df_selected[col].fillna(mode_val, inplace=True)\n",
    "        else:\n",
    "            df_selected[col].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# Fill numeric fields\n",
    "for col in df_selected.select_dtypes(include=[np.number]).columns:\n",
    "    missing_count = df_selected[col].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        if 'count' in col.lower() or 'number' in col.lower():\n",
    "            df_selected[col].fillna(0, inplace=True)\n",
    "        else:\n",
    "            df_selected[col].fillna(df_selected[col].median(), inplace=True)\n",
    "\n",
    "missing_after = df_selected.isnull().sum().sum()\n",
    "\n",
    "print(f\"\\nMissing values before: {missing_before:,}\")\n",
    "print(f\"Missing values after: {missing_after:,}\")\n",
    "print(f\"Missing values resolved: {missing_before - missing_after:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c2754a",
   "metadata": {},
   "source": [
    "### 3.5 Remove Invalid Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f742f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_before = len(df_selected)\n",
    "\n",
    "# Remove duplicates\n",
    "duplicates = df_selected.duplicated().sum()\n",
    "if duplicates > 0:\n",
    "    df_selected = df_selected.drop_duplicates()\n",
    "    print(f\"Removed {duplicates:,} duplicate records\")\n",
    "\n",
    "# Remove zero payment amounts\n",
    "if 'total_amount_of_payment_usdollars' in df_selected.columns:\n",
    "    zero_payments = (df_selected['total_amount_of_payment_usdollars'] == 0).sum()\n",
    "    if zero_payments > 0:\n",
    "        df_selected = df_selected[df_selected['total_amount_of_payment_usdollars'] > 0]\n",
    "        print(f\"Removed {zero_payments:,} zero payment records\")\n",
    "\n",
    "# Remove invalid dates\n",
    "if 'date_of_payment' in df_selected.columns:\n",
    "    invalid_dates = df_selected['date_of_payment'].isnull().sum()\n",
    "    if invalid_dates > 0:\n",
    "        df_selected = df_selected[df_selected['date_of_payment'].notnull()]\n",
    "        print(f\"Removed {invalid_dates:,} invalid date records\")\n",
    "\n",
    "# Remove records missing critical identifiers\n",
    "if 'covered_recipient_profile_id' in df_selected.columns:\n",
    "    missing_id = df_selected['covered_recipient_profile_id'].isnull().sum()\n",
    "    if missing_id > 0:\n",
    "        df_selected = df_selected[df_selected['covered_recipient_profile_id'].notnull()]\n",
    "        print(f\"Removed {missing_id:,} records with missing recipient ID\")\n",
    "\n",
    "# Reset index\n",
    "df_selected = df_selected.reset_index(drop=True)\n",
    "\n",
    "records_after = len(df_selected)\n",
    "records_removed = records_before - records_after\n",
    "\n",
    "print(f\"\\nRecords before: {records_before:,}\")\n",
    "print(f\"Records after: {records_after:,}\")\n",
    "print(f\"Records removed: {records_removed:,} ({records_removed/records_before*100:.2f}%)\")\n",
    "print(f\"Data retention: {records_after/records_before*100:.2f}%\")\n",
    "\n",
    "# Update df with cleaned data\n",
    "df = df_selected.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1334c65",
   "metadata": {},
   "source": [
    "### 3.6 Data Cleaning Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a28011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics table\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Initial Records',\n",
    "        'Final Records',\n",
    "        'Records Removed',\n",
    "        'Initial Columns',\n",
    "        'Final Columns',\n",
    "        'Columns Removed',\n",
    "        'Missing Values',\n",
    "        'Duplicates',\n",
    "        'Data Completeness (%)'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{initial_shape[0]:,}\",\n",
    "        f\"{df.shape[0]:,}\",\n",
    "        f\"{initial_shape[0] - df.shape[0]:,}\",\n",
    "        initial_columns,\n",
    "        df.shape[1],\n",
    "        initial_columns - df.shape[1],\n",
    "        f\"{df.isnull().sum().sum():,}\",\n",
    "        f\"{df.duplicated().sum():,}\",\n",
    "        f\"{(1 - df.isnull().sum().sum()/(df.shape[0]*df.shape[1]))*100:.2f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df)\n",
    "\n",
    "# Feature categories table\n",
    "feature_data = {\n",
    "    'Category': ['Core Payment', 'Categorical', 'Risk Indicators', 'Identifiers'],\n",
    "    'Count': [\n",
    "        len([f for f in CORE_PAYMENT_FEATURES if f in df.columns]),\n",
    "        len([f for f in CATEGORICAL_FEATURES if f in df.columns]),\n",
    "        len([f for f in RISK_INDICATOR_FEATURES if f in df.columns]),\n",
    "        len([f for f in IDENTIFIER_FEATURES if f in df.columns])\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\nFeature Categories:\")\n",
    "display(pd.DataFrame(feature_data))\n",
    "\n",
    "# Payment statistics table\n",
    "if 'total_amount_of_payment_usdollars' in df.columns:\n",
    "    payment_stats = df['total_amount_of_payment_usdollars']\n",
    "    payment_data = {\n",
    "        'Statistic': ['Total', 'Mean', 'Median', 'Min', 'Max'],\n",
    "        'Amount ($)': [\n",
    "            f\"{payment_stats.sum():,.2f}\",\n",
    "            f\"{payment_stats.mean():,.2f}\",\n",
    "            f\"{payment_stats.median():,.2f}\",\n",
    "            f\"{payment_stats.min():,.2f}\",\n",
    "            f\"{payment_stats.max():,.2f}\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nPayment Statistics:\")\n",
    "    display(pd.DataFrame(payment_data))\n",
    "\n",
    "print(\"\\nData is ready for feature engineering and model training\")\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa411aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal features from date column\n",
    "payment_date_col = 'date_of_payment'\n",
    "if payment_date_col in df.columns:\n",
    "    df['payment_year'] = df[payment_date_col].dt.year\n",
    "    df['payment_month'] = df[payment_date_col].dt.month\n",
    "    df['payment_quarter'] = df[payment_date_col].dt.quarter\n",
    "    df['payment_dayofweek'] = df[payment_date_col].dt.dayofweek\n",
    "    df['is_weekend'] = (df[payment_date_col].dt.dayofweek >= 5).astype('int64')\n",
    "    print(\"Temporal features created successfully\")\n",
    "else:\n",
    "    print(f\"Warning: {payment_date_col} not found in dataframe\")\n",
    "\n",
    "print(\"Feature creation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095e3f61-946a-4b7e-9dbe-7fcbfd1741a9",
   "metadata": {},
   "source": [
    "## 4. Data Partitioning\n",
    "\n",
    "Split the data according to project requirements 40/10/10/40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4805cc5-b6f0-4a8a-916f-7c482e8acb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split (40/10/10/40)\n",
    "print(\"Applying 40/10/10/40 data split...\")\n",
    "\n",
    "# fix random seed \n",
    "np.random.seed(42)\n",
    "\n",
    "# create a random mask to assign each row to a category\n",
    "perms = np.random.rand(len(df))\n",
    "\n",
    "# map the random decimals to the four mandatory groups\n",
    "df['dataset_usage'] = pd.cut(\n",
    "    perms, \n",
    "    bins=[0, 0.4, 0.5, 0.6, 1.0], \n",
    "    labels=['train', 'test', 'validation', 'production']\n",
    ")\n",
    "\n",
    "# display the exact counts to confirm the split matches requirements\n",
    "split_summary = df['dataset_usage'].value_counts().sort_index()\n",
    "split_pct = df['dataset_usage'].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "print(\"\\nFinal Partition Summary:\")\n",
    "for label in split_summary.index:\n",
    "    print(f\"  {label.capitalize()}: {split_summary[label]:,} rows ({split_pct[label]:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5a78a",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering for ML Models\n",
    "\n",
    "Create new features for predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122b2aa0-7951-4f38-a28f-4f9557daa7f4",
   "metadata": {},
   "source": [
    "### 5.1 Baseline Engineering\n",
    "Data leak prevention. This isolates the training set to establish what \"normal\" behavior looks for each doctor/hospital before merging them back into the main dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53909e8-a91e-47be-83d1-caaf9e41b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop existing features if they exist to prevent error\n",
    "cols_to_drop = ['hist_pay_count', 'hist_pay_total', 'hist_pay_avg', \n",
    "                'hist_pay_std', 'hist_pay_max', 'amt_to_avg_ratio', 'amt_to_max_ratio']\n",
    "\n",
    "# only drop columns that are actually in the dataframe\n",
    "df = df.drop(columns=[c for c in cols_to_drop if c in df.columns])\n",
    "\n",
    "# setup column names\n",
    "target_id = 'covered_recipient_profile_id' \n",
    "payment_col = 'total_amount_of_payment_usdollars'\n",
    "nature_col = 'nature_of_payment_or_transfer_of_value'\n",
    "recipient_type_col = 'covered_recipient_type'\n",
    "state_col = 'recipient_state'\n",
    "\n",
    "# isolate training set to calculate historical \"Normal\" behavior\n",
    "df_train = df[df['dataset_usage'] == 'train']\n",
    "global_median = df_train[payment_col].median()\n",
    "\n",
    "print(f\"Calculating baselines for: {target_id}\")\n",
    "\n",
    "# calculate Recipient-Level historical metrics from training data\n",
    "recipient_features = df_train.groupby(target_id).agg({\n",
    "    payment_col: ['count', 'sum', 'mean', 'std', 'max']\n",
    "}).reset_index()\n",
    "\n",
    "# flatten columns so they are easy to reference\n",
    "recipient_features.columns = [\n",
    "    target_id, 'hist_pay_count', 'hist_pay_total', 'hist_pay_avg', 'hist_pay_std', 'hist_pay_max'\n",
    "]\n",
    "\n",
    "# merge features back to the main dataframe\n",
    "# using left merge to ensure we don't lose the production/validation rows\n",
    "df = df.merge(recipient_features, on=target_id, how='left')\n",
    "\n",
    "# create ratio features \n",
    "df['amt_to_avg_ratio'] = df[payment_col] / df['hist_pay_avg']\n",
    "df['amt_to_max_ratio'] = df[payment_col] / df['hist_pay_max']\n",
    "\n",
    "# Impute NaNs (handling recipients not seen in training)\n",
    "df['is_new_recipient'] = df['hist_pay_avg'].isnull().astype(int)\n",
    "df['hist_pay_avg'] = df['hist_pay_avg'].fillna(global_median)\n",
    "df['amt_to_avg_ratio'] = df['amt_to_avg_ratio'].fillna(df[payment_col] / global_median)\n",
    "\n",
    "# final cleanup for remaining numerical NaNs\n",
    "df[['hist_pay_count', 'hist_pay_total', 'hist_pay_std', 'hist_pay_max', 'amt_to_max_ratio']] = \\\n",
    "    df[['hist_pay_count', 'hist_pay_total', 'hist_pay_std', 'hist_pay_max', 'amt_to_max_ratio']].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea12286-8894-44f9-a578-51b33298c63a",
   "metadata": {},
   "source": [
    "### Statistical Deviation & Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d11d5a6-5af9-45ce-90a2-2c44816166e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporal features, extracting seasonal patterns\n",
    "df['date_of_payment'] = pd.to_datetime(df['date_of_payment'])\n",
    "df['payment_month'] = df['date_of_payment'].dt.month\n",
    "df['is_weekend'] = (df['date_of_payment'].dt.dayofweek >= 5).astype(int)\n",
    "\n",
    "# categorical risk indicators\n",
    "risk_categories = ['Gift', 'Entertainment', 'Travel and Lodging']\n",
    "df['is_high_risk_nature'] = df['nature_of_payment_or_transfer_of_value'].isin(risk_categories).astype(int)\n",
    "\n",
    "print(\"Block 2 Complete: Contextual features created.\")\n",
    "\n",
    "# final verification, view the engineered data across the 40/10/10/40 split\n",
    "cols_to_view = ['dataset_usage', payment_col, 'hist_pay_avg', 'amt_to_avg_ratio', 'is_new_recipient', 'is_weekend']\n",
    "display(df.groupby('dataset_usage')[cols_to_view].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fd6995-f079-4bdb-a53b-45e0ac925e8b",
   "metadata": {},
   "source": [
    "### Multi-Dimensional Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf47cfac-1e31-44a9-8cc7-13106c5c9ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle missing raw payment amounts by filling with the global training median\n",
    "# this ensures that rows with NaN payments don't break the outlier detection models\n",
    "df[payment_col] = df[payment_col].fillna(global_median)\n",
    "\n",
    "# recalculate the ratio for these newly filled rows\n",
    "df['amt_to_avg_ratio'] = df['amt_to_avg_ratio'].fillna(df[payment_col] / global_median)\n",
    "\n",
    "# check for any remaining NaNs across the entire feature set\n",
    "final_nan_check = df[['hist_pay_avg', 'amt_to_avg_ratio', 'is_new_recipient', 'is_weekend']].isnull().sum()\n",
    "print(\"Remaining NaN values per feature:\")\n",
    "print(final_nan_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325c7255-e7f4-406f-84a2-93f86faa1e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the numerical features that need scaling\n",
    "features_to_scale = ['total_amount_of_payment_usdollars', 'hist_pay_avg', \n",
    "                     'hist_pay_std', 'amt_to_avg_ratio']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit only on training data to prevent leakage, then transform everything\n",
    "df.loc[df['dataset_usage'] == 'train', [f\"{c}_scaled\" for c in features_to_scale]] = \\\n",
    "    scaler.fit_transform(df.loc[df['dataset_usage'] == 'train', features_to_scale])\n",
    "\n",
    "# Apply the trained scaler to the rest of the data (val/test/prod)\n",
    "non_train_mask = df['dataset_usage'] != 'train'\n",
    "df.loc[non_train_mask, [f\"{c}_scaled\" for c in features_to_scale]] = \\\n",
    "    scaler.transform(df.loc[non_train_mask, features_to_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87091e9",
   "metadata": {},
   "source": [
    "## 6. Outlier Detection\n",
    "\n",
    "Identify and analyze outlier payments using multiple detection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f4ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection using IQR method\n",
    "if payment_col in df.columns:   \n",
    "    \n",
    "    amounts = df[payment_col].dropna()\n",
    "    \n",
    "    # Method 1: IQR (Interquartile Range)\n",
    "    Q1 = amounts.quantile(0.25)\n",
    "    Q3 = amounts.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    iqr_outliers = df[(df[payment_col] < lower_bound) | (df[payment_col] > upper_bound)]\n",
    "    \n",
    "    print(f\"\\nIQR Method:\")\n",
    "    print(f\"  Q1 (25th percentile): ${Q1:,.2f}\")\n",
    "    print(f\"  Q3 (75th percentile): ${Q3:,.2f}\")\n",
    "    print(f\"  IQR: ${IQR:,.2f}\")\n",
    "    print(f\"  Lower Bound: ${lower_bound:,.2f}\")\n",
    "    print(f\"  Upper Bound: ${upper_bound:,.2f}\")\n",
    "    print(f\"  Outliers Detected: {len(iqr_outliers):,} ({len(iqr_outliers)/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Method 2: Z-Score\n",
    "    z_scores = np.abs(stats.zscore(amounts))\n",
    "    z_threshold = 3\n",
    "    z_outliers = df[np.abs(stats.zscore(df[payment_col].fillna(0))) > z_threshold]\n",
    "    \n",
    "    print(f\"\\nZ-Score Method (threshold={z_threshold}):\")\n",
    "    print(f\"  Outliers Detected: {len(z_outliers):,} ({len(z_outliers)/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Method 3: Percentile-based\n",
    "    percentile_99 = amounts.quantile(0.99)\n",
    "    percentile_outliers = df[df[payment_col] > percentile_99]\n",
    "    \n",
    "    print(f\"\\nPercentile Method (99th percentile):\")\n",
    "    print(f\"  Threshold: ${percentile_99:,.2f}\")\n",
    "    print(f\"  Outliers Detected: {len(percentile_outliers):,} ({len(percentile_outliers)/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8144fa",
   "metadata": {},
   "source": [
    "### Outlier Detection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd760da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze outlier characteristics\n",
    "if payment_col in df.columns and len(iqr_outliers) > 0:\n",
    "    print(f\"\\nPayment Amount Statistics for Outliers:\")\n",
    "    print(f\"  Count: {len(iqr_outliers):,}\")\n",
    "    print(f\"  Mean: ${iqr_outliers[payment_col].mean():,.2f}\")\n",
    "    print(f\"  Median: ${iqr_outliers[payment_col].median():,.2f}\")\n",
    "    print(f\"  Min: ${iqr_outliers[payment_col].min():,.2f}\")\n",
    "    print(f\"  Max: ${iqr_outliers[payment_col].max():,.2f}\")\n",
    "\n",
    "    # Top outliers\n",
    "    print(f\"\\nTop 10 Outliers by Payment Amount:\")\n",
    "    top_outliers = iqr_outliers.nlargest(10, payment_col)[\n",
    "        [col for col in [payment_col, recipient_type_col, state_col, nature_col] \n",
    "         if col in iqr_outliers.columns]\n",
    "    ]\n",
    "    display(top_outliers)\n",
    "    \n",
    "    # Visualize outliers\n",
    "    if use_visualizer and len(recipient_id_cols) > 0:\n",
    "        # Create aggregated data for outlier visualization\n",
    "        df_aggregated = df.groupby(recipient_id_cols[0]).agg({\n",
    "            payment_col: ['count', 'sum', 'mean']\n",
    "        }).reset_index()\n",
    "        df_aggregated.columns = ['recipient_id', 'payment_count', 'total_amount', 'avg_amount']\n",
    "        \n",
    "        fig = visualizer.plot_outliers_analysis(df_aggregated, amounts, \n",
    "                                                lower_bound, upper_bound, \n",
    "                                                z_threshold=3)\n",
    "        if fig:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca9bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive scatter plot - Payment amount vs Count\n",
    "if payment_col and payment_col in df.columns and recipient_type_col and recipient_type_col in df.columns:\n",
    "    if use_visualizer and len(recipient_id_cols) > 0:\n",
    "        # Aggregate by recipient\n",
    "        scatter_data = df.groupby([recipient_id_cols[0], recipient_type_col]).agg({\n",
    "            payment_col: ['count', 'sum']\n",
    "        }).reset_index()\n",
    "        \n",
    "        scatter_data.columns = ['recipient_id', 'recipient_type', 'payment_count', 'total_amount']\n",
    "        \n",
    "        # Use visualizer for interactive scatter\n",
    "        fig = visualizer.create_interactive_scatter(\n",
    "            scatter_data.head(1000),  # Limit for performance\n",
    "            x_col='payment_count',\n",
    "            y_col='total_amount',\n",
    "            color_col='recipient_type',\n",
    "            size_col='total_amount',\n",
    "            hover_data=['recipient_id'],\n",
    "            title='Payment Frequency vs Total Amount by Recipient Type'\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075774d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MODEL_FEATURES for anomaly detection models\n",
    "MODEL_FEATURES = [\n",
    "    'total_amount_of_payment_usdollars',\n",
    "    'hist_pay_avg',\n",
    "    'amt_to_avg_ratio',\n",
    "    'is_new_recipient',\n",
    "    'payment_month',\n",
    "    'is_weekend'\n",
    "]\n",
    "\n",
    "# Add optional risk indicators if they exist in the dataframe\n",
    "if 'is_high_risk_nature' in df.columns:\n",
    "    MODEL_FEATURES.append('is_high_risk_nature')\n",
    "if 'physician_ownership_indicator' in df.columns:\n",
    "    MODEL_FEATURES.append('physician_ownership_indicator')\n",
    "if 'third_party_payment_recipient_indicator' in df.columns:\n",
    "    MODEL_FEATURES.append('third_party_payment_recipient_indicator')\n",
    "\n",
    "# Filter to only available features\n",
    "model_features_available = [f for f in MODEL_FEATURES if f in df.columns]\n",
    "print(f\"\\nModel features defined: {len(MODEL_FEATURES)}\")\n",
    "print(f\"Model features available: {len(model_features_available)}\")\n",
    "AVAILABLE_FEATURES = available_features\n",
    "\n",
    "# Store feature lists\n",
    "%store AVAILABLE_FEATURES\n",
    "%store MODEL_FEATURES\n",
    "%store CORE_PAYMENT_FEATURES\n",
    "%store CATEGORICAL_FEATURES\n",
    "%store RISK_INDICATOR_FEATURES\n",
    "\n",
    "# Store dataframe\n",
    "%store df\n",
    "\n",
    "print(\"\\nVariables stored successfully:\")\n",
    "print(f\"  AVAILABLE_FEATURES: {len(AVAILABLE_FEATURES)} features\")\n",
    "print(f\"  MODEL_FEATURES: {len(MODEL_FEATURES)} features\")\n",
    "print(f\"  df: {df.shape[0]:,} records x {df.shape[1]} columns\")\n",
    "print(\"\\nThese variables can be retrieved in subsequent notebooks using %store -r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2382cd",
   "metadata": {},
   "source": [
    "### 6.1 Data Validation & Cleaning\n",
    "\n",
    "Validate and clean data before Feature Store ingestion to prevent downstream issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab1fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check for and remove infinity values\n",
    "print(\"\\n1. Checking for infinity values...\")\n",
    "numeric_cols_df = df.select_dtypes(include=[np.number]).columns\n",
    "inf_counts = {}\n",
    "for col in numeric_cols_df:\n",
    "    inf_count = np.isinf(df[col]).sum()\n",
    "    if inf_count > 0:\n",
    "        inf_counts[col] = inf_count\n",
    "        \n",
    "if inf_counts:\n",
    "    print(f\"Found infinity values in {len(inf_counts)} columns:\")\n",
    "    for col, count in list(inf_counts.items())[:5]:\n",
    "        print(f\"- {col}: {count:,} infinities\")\n",
    "    if len(inf_counts) > 5:\n",
    "        print(f\"... and {len(inf_counts) - 5} more columns\")\n",
    "    \n",
    "    # Replace infinities with NaN for proper handling\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    print(\"Replaced all infinities with NaN\")\n",
    "else:\n",
    "    print(\"No infinity values found\")\n",
    "\n",
    "# 2. Cap extreme outliers at 99.9th percentile\n",
    "print(\"\\n2. Capping extreme outliers at 99.9th percentile...\")\n",
    "capped_cols = []\n",
    "for col in numeric_cols_df:\n",
    "    if col in df.columns:\n",
    "        q999 = df[col].quantile(0.999)\n",
    "        q001 = df[col].quantile(0.001)\n",
    "        \n",
    "        # Check if any values exceed thresholds\n",
    "        if (df[col] > q999).any() or (df[col] < q001).any():\n",
    "            outlier_count = ((df[col] > q999) | (df[col] < q001)).sum()\n",
    "            df[col] = df[col].clip(lower=q001, upper=q999)\n",
    "            capped_cols.append((col, outlier_count))\n",
    "\n",
    "if capped_cols:\n",
    "    print(f\"Capped outliers in {len(capped_cols)} columns:\")\n",
    "    for col, count in capped_cols[:5]:\n",
    "        print(f\"- {col}: {count:,} values capped\")\n",
    "    if len(capped_cols) > 5:\n",
    "        print(f\"... and {len(capped_cols) - 5} more columns\")\n",
    "else:\n",
    "    print(\"No extreme outliers found\")\n",
    "\n",
    "# 3. Check feature availability\n",
    "print(\"\\n3. Validating engineered features...\")\n",
    "required_features = ['hist_pay_avg', 'amt_to_avg_ratio', 'is_new_recipient', \n",
    "                     'is_high_risk_nature', 'payment_month', 'is_weekend']\n",
    "missing_features = [f for f in required_features if f not in df.columns]\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"WARNING: Missing {len(missing_features)} required features:\")\n",
    "    for f in missing_features:\n",
    "        print(f\"- {f}\")\n",
    "    print(\"These features should be created in section 8.1\")\n",
    "else:\n",
    "    print(f\"All {len(required_features)} required features present\")\n",
    "\n",
    "# 4. Final data quality summary\n",
    "print(\"\\n4. Final Data Quality Summary:\")\n",
    "print(f\"   Total records: {len(df):,}\")\n",
    "print(f\"   Total features: {len(df.columns)}\")\n",
    "print(f\"   Missing value percentage: {(df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100):.2f}%\")\n",
    "print(f\"   Duplicate rows: {df.duplicated().sum():,}\")\n",
    "\n",
    "# Check numeric ranges\n",
    "numeric_summary = df[numeric_cols_df].describe()\n",
    "print(f\"\\nNumeric features range check:\")\n",
    "print(f\"Min value across all numeric cols: {df[numeric_cols_df].min().min():.2f}\")\n",
    "print(f\"Max value across all numeric cols: {df[numeric_cols_df].max().max():.2f}\")\n",
    "print(\"DATA VALIDATION COMPLETE - Ready for Feature Store ingestion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e87ed",
   "metadata": {},
   "source": [
    "## 7. Feature Store & Feature Group Setup\n",
    "\n",
    "Store engineered features in Amazon SageMaker Feature Store for model training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f735ae9f",
   "metadata": {},
   "source": [
    "### 7.1 Setup & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a16e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import time\n",
    "\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "# Initialize Feature Store session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "default_s3_bucket_name = sagemaker_session.default_bucket()\n",
    "prefix = \"cms-anomaly-detection-featurestore\"\n",
    "role_arn = sagemaker.get_execution_role()\n",
    "\n",
    "# Create unique feature group names with timestamps\n",
    "timestamp = strftime(\"%d-%H-%M-%S\", gmtime())\n",
    "cms_payments_feature_group_name = f\"cms-payments-fg-{timestamp}\"\n",
    "payment_aggregates_feature_group_name = f\"cms-payment-agg-fg-{timestamp}\"\n",
    "\n",
    "# Prepare data for ingestion\n",
    "def cast_object_to_string(data_frame):\n",
    "    for label in data_frame.columns:\n",
    "        if data_frame.dtypes[label] == \"object\":\n",
    "            data_frame[label] = data_frame[label].astype(\"str\").astype(\"string\")\n",
    "        elif \"datetime\" in str(data_frame.dtypes[label]):\n",
    "            data_frame[label] = data_frame[label].astype(str)\n",
    "        elif \"category\" in str(data_frame.dtypes[label]):\n",
    "            data_frame[label] = data_frame[label].astype(str)\n",
    "\n",
    "# Define essential features - separate engineered from raw features\n",
    "# REQUIRED engineered features (must exist)\n",
    "required_engineered_features = [\n",
    "    'hist_pay_count', 'hist_pay_total', 'hist_pay_avg', 'hist_pay_std', 'hist_pay_max',\n",
    "    'amt_to_avg_ratio', 'amt_to_max_ratio', 'is_new_recipient',\n",
    "    'payment_year', 'payment_month', 'payment_quarter', 'payment_dayofweek', \n",
    "    'is_weekend', 'is_high_risk_nature', 'dataset_usage'\n",
    "]\n",
    "\n",
    "# OPTIONAL raw features (use if available in dataset)\n",
    "optional_raw_features = [\n",
    "    'covered_recipient_profile_id', 'total_amount_of_payment_usdollars',\n",
    "    'number_of_payments_included_in_total_amount', 'covered_recipient_type',\n",
    "    'nature_of_payment_or_transfer_of_value', 'form_of_payment_or_transfer_of_value',\n",
    "    'physician_specialty', 'recipient_state'\n",
    "]\n",
    "\n",
    "# Validate that REQUIRED engineered features exist\n",
    "missing_engineered = [f for f in required_engineered_features if f not in df.columns]\n",
    "if missing_engineered:\n",
    "    print(f\"\\n  ERROR: Missing {len(missing_engineered)} required ENGINEERED features:\")\n",
    "    for f in missing_engineered:\n",
    "        print(f\"    - {f}\")\n",
    "    print(\"\\nThese features should be created in section 8.1 (Feature Engineering)\")\n",
    "    print(\"Please run feature engineering cells before proceeding.\")\n",
    "    raise ValueError(f\"Missing {len(missing_engineered)} required engineered features for Feature Store\")\n",
    "\n",
    "# Check which optional raw features are available\n",
    "missing_raw = [f for f in optional_raw_features if f not in df.columns]\n",
    "if missing_raw:\n",
    "    print(f\"\\nNote: {len(missing_raw)} optional raw features not found in dataset (will skip):\")\n",
    "    for f in missing_raw:\n",
    "        print(f\"  - {f}\")\n",
    "\n",
    "# Combine all desired features\n",
    "essential_features = required_engineered_features + optional_raw_features\n",
    "\n",
    "# Filter to available columns only\n",
    "selected_cols = [col for col in essential_features if col in df.columns]\n",
    "print(f\"\\nFeature Selection Summary:\")\n",
    "print(f\"  Original dataframe: {df.shape[1]} columns\")\n",
    "print(f\"  Selected features: {len(selected_cols)} columns\")\n",
    "print(f\"  Required engineered: {len([f for f in required_engineered_features if f in selected_cols])}/{len(required_engineered_features)}\")\n",
    "print(f\"  Optional raw: {len([f for f in optional_raw_features if f in selected_cols])}/{len(optional_raw_features)}\")\n",
    "print(f\"  Reduction: {((df.shape[1] - len(selected_cols)) / df.shape[1] * 100):.1f}% fewer columns\")\n",
    "print(f\"  Benefit: Faster serialization, transfer, and storage\")\n",
    "\n",
    "df_payments = df[selected_cols].copy()\n",
    "cast_object_to_string(df_payments)\n",
    "\n",
    "current_time_sec = int(round(time.time()))\n",
    "df_payments['EventTime'] = pd.Series([current_time_sec] * len(df_payments), dtype=\"float64\")\n",
    "record_identifier_feature_name = 'covered_recipient_profile_id'\n",
    "\n",
    "print(f\"\\nData ready for ingestion:\")\n",
    "print(f\"  Records: {len(df_payments):,}\")\n",
    "print(f\"  Features: {len(df_payments.columns)} columns\")\n",
    "print(f\"  Estimated time: ~{len(df_payments)/3000/60:.1f} minutes (at 3k rec/sec)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13978dae",
   "metadata": {},
   "source": [
    "### 7.2 Create Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e0c1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Feature Group instances\n",
    "cms_payments_fg = FeatureGroup(\n",
    "    name=cms_payments_feature_group_name, \n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "payment_aggregates_fg = FeatureGroup(\n",
    "    name=payment_aggregates_feature_group_name, \n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Load feature definitions\n",
    "cms_payments_fg.load_feature_definitions(data_frame=df_payments)\n",
    "\n",
    "def wait_for_feature_group_creation(feature_group, max_wait=3600):\n",
    "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while status == \"Creating\":\n",
    "        if time.time() - start_time > max_wait:\n",
    "            raise TimeoutError(\"Feature Group creation timed out\")\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    \n",
    "    if status != \"Created\":\n",
    "        raise RuntimeError(f\"Feature Group creation failed with status: {status}\")\n",
    "\n",
    "# Create Feature Group\n",
    "cms_payments_fg.create(\n",
    "    s3_uri=f\"s3://{default_s3_bucket_name}/{prefix}/payments\",\n",
    "    record_identifier_name=record_identifier_feature_name,\n",
    "    event_time_feature_name=\"EventTime\",\n",
    "    role_arn=role_arn,\n",
    "    enable_online_store=True,\n",
    "    description=\"CMS Open Payments data with engineered features\"\n",
    ")\n",
    "\n",
    "wait_for_feature_group_creation(cms_payments_fg)\n",
    "print(f\"Feature Group created: {cms_payments_feature_group_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03058f34",
   "metadata": {},
   "source": [
    "### 7.3 Ingest & Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fc9f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Configuration for high-throughput ingestion\n",
    "BATCH_SIZE = 100000  # Process in large batches\n",
    "MAX_WORKERS = 50      # Increased from 10 for higher parallelism\n",
    "MAX_PROCESSES = 8     # Utilize multiple CPU cores\n",
    "\n",
    "# Split dataframe into batches\n",
    "num_batches = int(np.ceil(len(df_payments) / BATCH_SIZE))\n",
    "print(f\"Ingesting {len(df_payments):,} records in {num_batches} batches\")\n",
    "print(f\"Batch size: {BATCH_SIZE:,} | Workers: {MAX_WORKERS} | Processes: {MAX_PROCESSES}\")\n",
    "\n",
    "ingestion_results = []\n",
    "for batch_idx in range(num_batches):\n",
    "    batch_start = batch_idx * BATCH_SIZE\n",
    "    batch_end = min((batch_idx + 1) * BATCH_SIZE, len(df_payments))\n",
    "    df_batch = df_payments.iloc[batch_start:batch_end]\n",
    "    \n",
    "    batch_time = time.time()\n",
    "    result = cms_payments_fg.ingest(\n",
    "        data_frame=df_batch,\n",
    "        max_workers=MAX_WORKERS,\n",
    "        max_processes=MAX_PROCESSES,\n",
    "        wait=True\n",
    "    )\n",
    "    batch_elapsed = time.time() - batch_time\n",
    "    \n",
    "    ingestion_results.append(result)\n",
    "    records_processed = batch_end\n",
    "    throughput = len(df_batch) / batch_elapsed\n",
    "    print(f\"Batch {batch_idx + 1}/{num_batches}: {len(df_batch):,} records in {batch_elapsed:.1f}s \"\n",
    "          f\"({throughput:.0f} rec/sec) | Total: {records_processed:,}/{len(df_payments):,}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "avg_throughput = len(df_payments) / total_time\n",
    "\n",
    "print(f\"Ingestion Complete!\")\n",
    "print(f\"Total time: {total_time:.1f}s ({total_time/60:.2f} min)\")\n",
    "print(f\"Records: {len(df_payments):,}\")\n",
    "print(f\"Throughput: {avg_throughput:.0f} records/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b01602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify ingestion and store Feature Store configuration\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "try:\n",
    "    featurestore_runtime = boto_session.client(\n",
    "        service_name=\"sagemaker-featurestore-runtime\", \n",
    "        region_name=region\n",
    "    )\n",
    "    \n",
    "    sample_id = str(int(df_payments[record_identifier_feature_name].iloc[0]))\n",
    "    response = featurestore_runtime.get_record(\n",
    "        FeatureGroupName=cms_payments_feature_group_name,\n",
    "        RecordIdentifierValueAsString=sample_id\n",
    "    )\n",
    "    print(f\"Verified online store access ({len(response.get('Record', []))} features retrieved)\")\n",
    "except Exception as e:\n",
    "    print(f\"  Online store not yet available: {str(e)[:40]}...\")\n",
    "    featurestore_runtime = None\n",
    "\n",
    "# Get Feature Store metadata\n",
    "fg_desc = cms_payments_fg.describe()\n",
    "offline_store_config = fg_desc.get('OfflineStoreConfig', {})\n",
    "s3_storage_config = offline_store_config.get('S3StorageConfig', {})\n",
    "offline_s3_uri = s3_storage_config.get('ResolvedOutputS3Uri', 'N/A')\n",
    "\n",
    "print(f\"\\nFeature Store Configuration:\")\n",
    "print(f\"  Feature Group: {cms_payments_feature_group_name}\")\n",
    "print(f\"  Offline Store URI: {offline_s3_uri}\")\n",
    "print(f\"  Record Identifier: {record_identifier_feature_name}\")\n",
    "\n",
    "# Store configuration for downstream notebooks\n",
    "%store cms_payments_feature_group_name\n",
    "%store record_identifier_feature_name\n",
    "%store featurestore_runtime\n",
    "%store cms_payments_fg\n",
    "%store region\n",
    "%store bucket\n",
    "%store s3_athena_staging\n",
    "%store database_name\n",
    "%store table_name_parquet\n",
    "%store df\n",
    "\n",
    "print(\"Variables stored for downstream notebooks:\")\n",
    "\n",
    "stored_vars = {\n",
    "    'cms_payments_feature_group_name': cms_payments_feature_group_name,\n",
    "    'record_identifier_feature_name': record_identifier_feature_name,\n",
    "    'featurestore_runtime': 'Client object' if featurestore_runtime else None,\n",
    "    'cms_payments_fg': 'FeatureGroup object',\n",
    "    'region': region,\n",
    "    'bucket': bucket,\n",
    "    'database_name': database_name,\n",
    "    'df': f'{df.shape[0]:,} rows x {df.shape[1]} cols'\n",
    "}\n",
    "\n",
    "for var, val in stored_vars.items():\n",
    "    print(f\"  {var}: {val}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
